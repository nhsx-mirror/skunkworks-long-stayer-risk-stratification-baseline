{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Feature engineering\n",
    "\n",
    "This notebook processes cleaned data into the feature set used for modelling.\n",
    "\n",
    "The decisions around feature engineering are the culmination of a number of explorations of the data, including modelling of the full dataset, which is not included in this repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1653294513618
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1653294517836
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "clean_data_df = pd.read_parquet(\"../../data/clean-data.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add derived fields\n",
    "\n",
    "These were removed during cleaning due to missing data, and can be recalculated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1653294519569
    }
   },
   "outputs": [],
   "source": [
    "derived_df = clean_data_df.copy()\n",
    "derived_df[\n",
    "    \"arrival_day_of_week\"\n",
    "] = derived_df.START_DATE_TIME_HOSPITAL_PROVIDER_SPELL.dt.day_name().str[:3]\n",
    "derived_df[\n",
    "    \"arrival_month_name\"\n",
    "] = derived_df.START_DATE_TIME_HOSPITAL_PROVIDER_SPELL.dt.month_name().str[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Select agreed columns\n",
    "\n",
    "As agreed with data SME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1653294520617
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "columns = [\n",
    "    \"ae_arrival_mode\",\n",
    "    \"IS_major\",\n",
    "    \"AGE_ON_ADMISSION\",\n",
    "    \"EL CountLast12m\",\n",
    "    \"IS_elective\",\n",
    "    \"EMCountLast12m\",\n",
    "    \"IS_illness_not_injury\",\n",
    "    \"IS_cancer\",\n",
    "    \"IS_care_home_on_admission\",\n",
    "    \"IS_chronic_kidney_disease\",\n",
    "    \"IS_COPD\",\n",
    "    \"IS_coronary_heart_disease\",\n",
    "    \"IS_dementia\",\n",
    "    \"IS_diabetes\",\n",
    "    \"IS_frailty_proxy\",\n",
    "    \"IS_hypertension\",\n",
    "    \"IS_mental_health\",\n",
    "    \"MAIN_SPECIALTY_CODE_AT_ADMISSION_DESCRIPTION\",\n",
    "    \"OP First CountLast12m\",\n",
    "    \"OP FU CountLast12m\",\n",
    "    \"SOURCE_OF_ADMISSION_HOSPITAL_PROVIDER_SPELL_DESCRIPTION\",\n",
    "    \"stroke_ward_stay\",\n",
    "    \"LENGTH_OF_STAY\",\n",
    "    \"arrival_day_of_week\",\n",
    "    \"arrival_month_name\",\n",
    "]\n",
    "\n",
    "# define sensitive columns for fairness testing later\n",
    "sensitive_columns = [\n",
    "    \"ETHNIC_CATEGORY_CODE_DESCRIPTION\",\n",
    "    \"IMD county decile\",\n",
    "    \"OAC Group Name\",\n",
    "    \"OAC Subgroup Name\",\n",
    "    \"OAC Supergroup Name\",\n",
    "    \"PATIENT_GENDER_CURRENT_DESCRIPTION\",\n",
    "    \"POST_CODE_AT_ADMISSION_DATE_DISTRICT\",\n",
    "    \"Rural urban classification\",\n",
    "]\n",
    "\n",
    "subset_df = derived_df[columns + sensitive_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Focus on MAJOR, non-elective cases only\n",
    "\n",
    "SME requests model built for MAJOR and non-elective cases only, as these will require longer stay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1653294520766
    }
   },
   "outputs": [],
   "source": [
    "major_df = subset_df[subset_df.IS_major == 1]\n",
    "major_df = major_df[major_df.IS_elective == 0]\n",
    "# These columns no longer contain additional information:\n",
    "columns.remove(\"IS_major\")\n",
    "columns.remove(\"IS_elective\")\n",
    "major_df.drop(columns=[\"IS_major\", \"IS_elective\"], inplace=True)\n",
    "major_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encode categorical data\n",
    "\n",
    "One-hot encoding is performed twice; once without sensitive features, and once with. This is so that when we are testing for fairness later, we can compare model performance on models trained without sensitive features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1653294523554
    }
   },
   "outputs": [],
   "source": [
    "# To avoid the \"dummy variable trap\", we could drop the first category of these features to reduce duplication.\n",
    "# However, we may lose interpretability if e.g. Monday is dropped and is an important feature?\n",
    "encoded_df = pd.get_dummies(major_df.drop(columns=sensitive_columns), drop_first=False)\n",
    "print(encoded_df.shape)\n",
    "# Add back in the sensitive columns, without encoding\n",
    "encoded_sensitive_df = encoded_df.copy()\n",
    "encoded_sensitive_df[sensitive_columns] = major_df[sensitive_columns]\n",
    "print(encoded_sensitive_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1653294532601
    }
   },
   "outputs": [],
   "source": [
    "corr = encoded_df.corr()\n",
    "# check for correlation of feature with LENGTH_OF_STAY\n",
    "corr.LENGTH_OF_STAY[corr.LENGTH_OF_STAY.abs().sort_values(ascending=False).index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1653294535597
    }
   },
   "outputs": [],
   "source": [
    "encoded_df.to_parquet(\"../../data/features.parquet\")\n",
    "encoded_sensitive_df.to_parquet(\"../../data/features-sensitive.parquet\")\n",
    "\n",
    "# Some machine learning algorithms e.g. catboost require NOT to one-hot encode data, so export for these\n",
    "major_df.drop(columns=sensitive_columns).to_parquet(\n",
    "    \"../../data/features-catboost.parquet\"\n",
    ")\n",
    "major_df.to_parquet(\"../../data/features-sensitive-catboost.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
