{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling - Regression\n",
    "\n",
    "This notebook explores regression models to **predict Length of Stay (days)** as a baseline to the [Long Stayer Risk Stratification](https://github.com/nhsx/skunkworks-long-stayer-risk-stratification) model.\n",
    "\n",
    "The best performing model used catboost and achieved a Mean Absolute Error (MAE) of **4.1 days**.\n",
    "\n",
    "This notebook is broken down into:\n",
    "\n",
    "1. Statistical tests to check the validity of linear models using Ordinary Least Squares (OLS)\n",
    "2. Splitting the data into a training, validation and test set\n",
    "3. Training a range of baseline models\n",
    "4. Testing models on the validation dataset\n",
    "5. Tuning the best model and testing on the test set\n",
    "\n",
    "Regression models selected:\n",
    "\n",
    "Model|Rationale\n",
    "---|---\n",
    "[Mean](https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyRegressor.html)|The simplest baseline, uses the mean length of stay as the prediction in all cases\n",
    "[ElasticNet](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html)|A regularised implementation of linear regression that can be used for multi-colinear datasets such as in this dataset\n",
    "[DecisionTreeRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html)|A simple, single tree regressor that is highly explainable\n",
    "[RandomForestRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)|An ensemble technique with potentially better performance than a single tree\n",
    "[CatBoostRegressor](https://catboost.ai/en/docs/concepts/python-reference_catboostregressor)|A boosted tree technique designed specifically for datasets with high levels of categorical features as in this dataset\n",
    "[XGBRegressor](https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.XGBRegressor)|A boosted tree technique that can improve on ensemble techniques such as RandomForest\n",
    "\n",
    "Inputs|Outputs\n",
    "---|---\n",
    "`processed/features.parquet`|`models/regression.pickle`\n",
    "`processed/features-sensitive.parquet`|&nbsp;\n",
    "`processed/features-catboost.parquet`|&nbsp;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1657729155796
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from catboost import CatBoostRegressor\n",
    "from scipy.stats import anderson, kstest, shapiro\n",
    "from sklearn import linear_model\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "sys.path.append(\"../src/\")\n",
    "\n",
    "from utils import train_and_test_model, train_test_validate_split\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = [15, 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1657729156235
    }
   },
   "outputs": [],
   "source": [
    "features_df = pd.read_parquet(\"../../data/processed/features.parquet\")\n",
    "features_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will also load all features so we can analyse the impact of train/validate/split on sensitive features\n",
    "features_sensitive_df = pd.read_parquet(\n",
    "    \"../../data/processed/features-sensitive.parquet\"\n",
    ")\n",
    "features_sensitive_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1657729156476
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# note catboost requires not one-hot encoding features, as it deals with them during training\n",
    "features_catboost_df = pd.read_parquet(\"../../data/processed/features-catboost.parquet\")\n",
    "features_catboost_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define sensitive columns for fairness testing later\n",
    "sensitive_columns = [\n",
    "    \"ETHNIC_CATEGORY_CODE_DESCRIPTION\",\n",
    "    \"IMD county decile\",\n",
    "    \"OAC Group Name\",\n",
    "    \"OAC Subgroup Name\",\n",
    "    \"OAC Supergroup Name\",\n",
    "    \"PATIENT_GENDER_CURRENT_DESCRIPTION\",\n",
    "    \"POST_CODE_AT_ADMISSION_DATE_DISTRICT\",\n",
    "    \"Rural urban classification\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define target and training features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1657729156906
    }
   },
   "outputs": [],
   "source": [
    "X = features_df.drop(columns=\"LENGTH_OF_STAY\")\n",
    "y = features_df.LENGTH_OF_STAY\n",
    "\n",
    "X_sensitive = features_sensitive_df.drop(columns=\"LENGTH_OF_STAY\")\n",
    "y_sensitive = features_sensitive_df.LENGTH_OF_STAY\n",
    "\n",
    "X_catboost = features_catboost_df.drop(columns=\"LENGTH_OF_STAY\")\n",
    "y_catboost = features_catboost_df.LENGTH_OF_STAY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance Inflaction Factors\n",
    "\n",
    "Simple linear models (ordinary least squares) assume there is no multi-collinearity.\n",
    "\n",
    "Variance inflaction factors (VIF) help quantify the extent of any collinearity present.\n",
    "\n",
    "We are looking for VIF ~< 10 across our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1657726818378
    }
   },
   "outputs": [],
   "source": [
    "# Takes ~6 minutes to run on a STANDARD_DS3_V2\n",
    "vif = pd.DataFrame()\n",
    "vif[\"feature\"] = X.columns\n",
    "\n",
    "vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(len(X.columns))]\n",
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual distributions of [OLS](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)\n",
    "\n",
    "If VIF factors indicate a lack of multi-collinearity (they do not), check for normality of residuals aka homoescadisticity.\n",
    "\n",
    "This requires training a linear model, calculating the residuals and checking visually and statistically that they are normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1657726821347
    }
   },
   "outputs": [],
   "source": [
    "# Train basic OLS model for statistical testing only\n",
    "reg = linear_model.LinearRegression()\n",
    "reg.fit(X, y)\n",
    "pred = pd.Series(reg.predict(X))\n",
    "resid = y - pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1657726821728
    }
   },
   "outputs": [],
   "source": [
    "# Visual inspection\n",
    "resid.hist(bins=30);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shapiro-Wilk test for normality\n",
    "\n",
    "* Null hypothesis = our residuals are drawn from normal distribution\n",
    "* Alternate hypothesis = our residuals are not drawn from normal distribution (and fail requirements of OLS model)\n",
    "* Test statistic shows how much distribution differs to normal distribution\n",
    "* p-value is probability null hypothesis true\n",
    "* p-value < 0.05 leads us to reject null hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1657726821893
    }
   },
   "outputs": [],
   "source": [
    "shapiro(resid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-sided Kolmogorov-Smirnov test for normality\n",
    "\n",
    "* Null hypothesis = our residuals are drawn from normal distribution\n",
    "* Alternate hypothesis = our residuals are not drawn from normal distribution (and fail requirements of OLS model)\n",
    "* Test statistic shows how much distribution differs to normal distribution\n",
    "* p-value is probability null hypothesis true\n",
    "* p-value < 0.05 leads us to reject null hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1657726822016
    }
   },
   "outputs": [],
   "source": [
    "kstest(resid, \"norm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anderson-Darling test for normality\n",
    "\n",
    "* Null hypothesis = our residuals are drawn from normal distribution\n",
    "* Alternate hypothesis = our residuals are not drawn from normal distribution (and fail requirements of OLS model)\n",
    "* Test statistic is compared to critical value at the significance level required (e.g. 5%)\n",
    "* Test statistic > critical value for 5% significance level leads us to reject null hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1657726822149
    }
   },
   "outputs": [],
   "source": [
    "anderson(resid, \"norm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Statistical testing invalidate assumptions for OLS models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/validation/test split\n",
    "\n",
    "We will use 70% of the data for training, leaving 15% for validation and 15% for testing.\n",
    "\n",
    "This split was decided based on the quantity of the data (>100,000 rows of data) and will include some simple checks to make sure that the splits are representative across Length of Stay, age, ethnicity and gender.\n",
    "\n",
    "Models will be trained on the training set, and tested on the validation set. This will help select a basic model using data it was not trained on, to reduce the risk of overfitting.\n",
    "\n",
    "A final model will be trained using GridSearchCV across the training and validation data, before final performance metrics are generated using the unseen test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1657729161981
    }
   },
   "outputs": [],
   "source": [
    "# Split data for train/validate+test\n",
    "X_train, X_validate, X_test, y_train, y_validate, y_test = train_test_validate_split(\n",
    "    X_sensitive,\n",
    "    y_sensitive,\n",
    "    train_size=0.70,\n",
    "    validate_size=0.15,\n",
    "    test_size=0.15,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "print(X_train.shape, X_validate.shape, X_test.shape)\n",
    "print(y_train.shape, y_validate.shape, y_test.shape)\n",
    "\n",
    "# Split data for train/validate+test\n",
    "(\n",
    "    X_train_catboost,\n",
    "    X_validate_catboost,\n",
    "    X_test_catboost,\n",
    "    y_train_catboost,\n",
    "    y_validate_catboost,\n",
    "    y_test_catboost,\n",
    ") = train_test_validate_split(\n",
    "    X_catboost,\n",
    "    y_catboost,\n",
    "    train_size=0.70,\n",
    "    validate_size=0.15,\n",
    "    test_size=0.15,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "print(X_train_catboost.shape, X_validate_catboost.shape, X_test_catboost.shape)\n",
    "print(y_train_catboost.shape, y_validate_catboost.shape, y_test_catboost.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now explore the distributions of data across the splits to ensure we are not accidentally introducing selection bias into the different datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define splits\n",
    "x_splits = {\"train\": X_train, \"validate\": X_validate, \"test\": X_test}\n",
    "y_splits = {\"train\": y_train, \"validate\": y_validate, \"test\": y_test}\n",
    "colours = {\"train\": \"#f00\", \"validate\": \"#0f0\", \"test\": \"#00f\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Length of stay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "num_bins = 30\n",
    "for split in y_splits:\n",
    "    y_splits[split].hist(\n",
    "        density=True, alpha=0.5, bins=num_bins, label=split, color=colours[split]\n",
    "    )\n",
    "plt.legend()\n",
    "plt.ylabel(\"Length of stay (days)\")\n",
    "plt.xlabel(\"Density\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_bins = 15\n",
    "for split in x_splits:\n",
    "    x_splits[split].AGE_ON_ADMISSION.hist(\n",
    "        density=True, alpha=0.5, bins=num_bins, label=split, color=colours[split]\n",
    "    )\n",
    "plt.legend()\n",
    "plt.ylabel(\"Age (years)\")\n",
    "plt.xlabel(\"Density\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in x_splits:\n",
    "    print(\n",
    "        f\"{split}: {x_splits[split].PATIENT_GENDER_CURRENT_DESCRIPTION.value_counts(dropna=False, normalize=True).round(2).tolist()}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ethnic category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for split in x_splits:\n",
    "    print(\n",
    "        f\"{split}: {x_splits[split].ETHNIC_CATEGORY_CODE_DESCRIPTION.value_counts(dropna=False, normalize=True).round(2).tolist()}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having established that the train/validate/test split does not introduce any bias in the representation of age, gender or ethnicity, we will drop the sensitive features which are not included in the model, but will be used to explore model bias later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop(columns=sensitive_columns)\n",
    "X_validate = X_validate.drop(columns=sensitive_columns)\n",
    "X_test = X_test.drop(columns=sensitive_columns)\n",
    "print(X_train.shape, X_validate.shape, X_test.shape)\n",
    "print(y_train.shape, y_validate.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "Strategy is to try a number of regression models with:\n",
    "\n",
    "* Baseline models for each algorithm trained on the training set with default parameters\n",
    "* Baseline models tested on the test set\n",
    "* GridsearchCV for hyperparameter tuning on best performing model\n",
    "* Explore feature importance of final model\n",
    "* Explore fairness (next notebook) of final model\n",
    "\n",
    "OLS models are excluded due to statistical assumptions not being met. NN are excluded due to complexity/interpretability issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1657729163183
    }
   },
   "outputs": [],
   "source": [
    "# Initiate empty models dictionary\n",
    "models = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean model\n",
    "\n",
    "The simplest baseline model takes the mean length of stay as its prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1657729164026
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"mean\"\n",
    "\n",
    "# define an estimator for this model\n",
    "estimator = DummyRegressor(strategy=\"mean\")\n",
    "\n",
    "# takes ~1 second to run on a STANDARD_D13_V2\n",
    "models[model_name] = train_and_test_model(\n",
    "    estimator, X_train, y_train, X_validate, y_validate, scoring_metric=\"rmse\"\n",
    ")\n",
    "models[model_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic net regression\n",
    "\n",
    "A regularised linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1657729175798
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"elastic\"\n",
    "\n",
    "# define an estimator for this model\n",
    "estimator = linear_model.ElasticNet(random_state=42)\n",
    "\n",
    "# takes ~1 second to run on a STANDARD_D13_V2\n",
    "models[model_name] = train_and_test_model(\n",
    "    estimator, X_train, y_train, X_validate, y_validate, scoring_metric=\"rmse\"\n",
    ")\n",
    "models[model_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Decision Tree regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1657729187851
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"decisiontree\"\n",
    "\n",
    "estimator = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "# takes ~10 seconds to run on a STANDARD_D13_V2\n",
    "models[model_name] = train_and_test_model(\n",
    "    estimator, X_train, y_train, X_validate, y_validate, scoring_metric=\"rmse\"\n",
    ")\n",
    "\n",
    "models[model_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1657729321863
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"randomforest\"\n",
    "\n",
    "estimator = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# takes ~3 mins to run on a STANDARD_D13_V2\n",
    "models[model_name] = train_and_test_model(\n",
    "    estimator, X_train, y_train, X_validate, y_validate, scoring_metric=\"rmse\"\n",
    ")\n",
    "models[model_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catboost\n",
    "\n",
    "Boosted tree optimised for categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1657729364152
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"catboost\"\n",
    "\n",
    "# extract categorical features\n",
    "num_features = [\n",
    "    \"AGE_ON_ADMISSION\",\n",
    "    \"EL CountLast12m\",\n",
    "    \"EMCountLast12m\",\n",
    "    \"OP First CountLast12m\",\n",
    "    \"OP FU CountLast12m\",\n",
    "]\n",
    "cat_features = list(set(X_train_catboost.columns) - set(num_features))\n",
    "\n",
    "estimator = CatBoostRegressor(verbose=False, cat_features=cat_features, random_state=42)\n",
    "\n",
    "# takes ~30 secs to run on a STANDARD_D13_V2\n",
    "models[model_name] = train_and_test_model(\n",
    "    estimator,\n",
    "    X_train_catboost,\n",
    "    y_train_catboost,\n",
    "    X_validate_catboost,\n",
    "    y_validate_catboost,\n",
    "    scoring_metric=\"rmse\",\n",
    ")\n",
    "models[model_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1657729379221
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"xgboost\"\n",
    "\n",
    "estimator = XGBRegressor(random_state=42)\n",
    "\n",
    "# takes ~10s to run on a STANDARD_D13_V2\n",
    "models[model_name] = train_and_test_model(\n",
    "    estimator, X_train, y_train, X_validate, y_validate, scoring_metric=\"rmse\"\n",
    ")\n",
    "models[model_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model performance visually\n",
    "\n",
    "We will evalulate how the models performed so we can select the best model for hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1657785104297
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# setup a subplot figure\n",
    "fig, axs = plt.subplots(len(models), 2)\n",
    "fig.set_size_inches(15, 7 * len(models))\n",
    "\n",
    "i = 0\n",
    "\n",
    "for model in models:\n",
    "    if model == \"catboost\":\n",
    "        model_X_test = X_test_catboost\n",
    "        model_y_test = y_test_catboost\n",
    "    else:\n",
    "        model_X_test = X_test\n",
    "        model_y_test = y_test\n",
    "\n",
    "    # inference - ensure smallest LoS is 0 days (not negative value)\n",
    "    preds = np.clip(models[model][\"model\"].predict(model_X_test), 0, None)\n",
    "\n",
    "    # calculate RMSE and range of LoS\n",
    "    rmse = mean_squared_error(model_y_test, preds, squared=False)\n",
    "    mae = mean_absolute_error(model_y_test, preds)\n",
    "\n",
    "    print(\n",
    "        f\"{model} test rmse: {rmse.round(3)} days, mae: {mae.round(3)}, range ({preds.min().round(1)} - {preds.max().round(1)} days)\"\n",
    "    )\n",
    "\n",
    "    # create prediction dataframe\n",
    "    predictions_df = pd.DataFrame(data=model_y_test.reset_index(drop=True))\n",
    "    predictions_df[\"pred\"] = preds\n",
    "\n",
    "    # calculate relative error\n",
    "    predictions_df[\"error\"] = predictions_df.pred - predictions_df.LENGTH_OF_STAY\n",
    "\n",
    "    # plot predicted vs actual\n",
    "    axs[i, 0].scatter(predictions_df.pred, predictions_df.LENGTH_OF_STAY, alpha=0.1)\n",
    "    # plot ideal 1:1 prediction line. Max LoS = 30\n",
    "    axs[i, 0].plot(np.arange(0, 31), np.arange(0, 31), \"r--\")\n",
    "    axs[i, 0].set_xlabel(\"Predicted Length of Stay (days)\")\n",
    "    axs[i, 0].set_ylabel(\"Actual Length of Stay (days)\")\n",
    "    axs[i, 0].set_xlim([-1, 31])\n",
    "    axs[i, 0].set_ylim([-1, 31])\n",
    "    axs[i, 0].set_title(f\"{model} - RMSE {rmse.round(2)} days\")\n",
    "\n",
    "    # plot relative error\n",
    "    axs[i, 1].scatter(predictions_df.LENGTH_OF_STAY, predictions_df.error, alpha=0.1)\n",
    "    axs[i, 1].set_xlabel(\"Length of Stay (days)\")\n",
    "    axs[i, 1].set_ylabel(\"Error (days)\")\n",
    "    # plot mean relative error and 95% confidence intervals\n",
    "    axs[i, 1].plot(np.arange(0, 31), np.ones(31) * predictions_df.error.mean(), \"r\")\n",
    "    axs[i, 1].plot(\n",
    "        np.arange(0, 51),\n",
    "        np.ones(51) * (predictions_df.error.mean() + (2 * predictions_df.error.std())),\n",
    "        \"g--\",\n",
    "    )\n",
    "    axs[i, 1].plot(\n",
    "        np.arange(0, 51),\n",
    "        np.ones(51) * (predictions_df.error.mean() - (2 * predictions_df.error.std())),\n",
    "        \"g--\",\n",
    "    )\n",
    "    # scale plot\n",
    "    axs[i, 1].set_xlim([-1, 31])\n",
    "    # add statistical data in legend. LoA = limits of agreement\n",
    "    # flag: errors are not normally distibuted so does 2*std capture 95% interval?\n",
    "\n",
    "    axs[i, 1].legend(\n",
    "        [\n",
    "            f\"\\u03bc ({np.round(predictions_df.error.mean(),2)} days)\",\n",
    "            f\"95% LoA (\\u03C3 {np.round(predictions_df.error.std(),2)} days gives {2*np.round(predictions_df.error.std(),2)})\",\n",
    "        ]\n",
    "    )\n",
    "    axs[i, 1].set_title(f\"{model} - RMSE {rmse.round(2)} days\")\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Model tuning\n",
    "\n",
    "We will select the best performing model using default parameters, `catboost` and use GridSearchCV to fine tune its hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1657729387396
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# note the baseline performance of the chosen model\n",
    "model_name = \"catboost\"\n",
    "\n",
    "print(models[model_name][\"test_metric\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Re-train best model\n",
    "\n",
    "Using GridsearchCV and an appropriate parameter array for the chosen model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1657740369530
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"catboost\"\n",
    "\n",
    "final_model = {model_name: {}}\n",
    "\n",
    "# example adapted from https://catboost.ai/en/docs/concepts/python-reference_catboostregressor_grid_search\n",
    "# see https://catboost.ai/en/docs/concepts/parameter-tuning for other options\n",
    "\n",
    "param_grid = {\n",
    "    \"learning_rate\": [0.03, 0.1],\n",
    "    \"depth\": [4, 6, 10],\n",
    "    \"l2_leaf_reg\": [1, 5, 9],\n",
    "}\n",
    "\n",
    "# extract categorical features\n",
    "num_features = [\n",
    "    \"AGE_ON_ADMISSION\",\n",
    "    \"EL CountLast12m\",\n",
    "    \"EMCountLast12m\",\n",
    "    \"OP First CountLast12m\",\n",
    "    \"OP FU CountLast12m\",\n",
    "]\n",
    "cat_features = list(set(X_train_catboost.columns) - set(num_features))\n",
    "\n",
    "gsc = GridSearchCV(\n",
    "    estimator=CatBoostRegressor(verbose=False, cat_features=cat_features),\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    refit=True,\n",
    ")\n",
    "\n",
    "# note: we will use both the training and validation dataset as we are using GridSearchCV across 5-folds\n",
    "# takes ~70 mins to run on a STANDARD_D13_V2\n",
    "grid_result = gsc.fit(\n",
    "    pd.concat([X_train_catboost, X_validate_catboost]),\n",
    "    pd.concat([y_train_catboost, y_validate_catboost]),\n",
    ")\n",
    "\n",
    "# store performance; neg_root_mean_squared_error = -root_mean_squared_error\n",
    "final_model[model_name][\"train_rmse\"] = -grid_result.best_score_\n",
    "# store model and parameters\n",
    "final_model[model_name][\"model\"] = grid_result.best_estimator_\n",
    "final_model[model_name][\"params\"] = grid_result.best_params_\n",
    "final_model[model_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Model evaluation\n",
    "\n",
    "Now we have tuned the best model given the parameters specified, we will test the model on the test set and also calculate the `mean_absolute_error` which is an additional metric that is more understandable - it is the number of days error in the length of stay prediction across all predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1657740370334
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# generate predictions\n",
    "preds_test = final_model[model_name][\"model\"].predict(X_test_catboost)\n",
    "\n",
    "# append the test metrics to the model\n",
    "final_model[model_name][\"test_rmse\"] = mean_squared_error(\n",
    "    y_test_catboost, preds_test, squared=False\n",
    ")\n",
    "final_model[model_name][\"test_mae\"] = mean_absolute_error(y_test_catboost, preds_test)\n",
    "final_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "We now have our best performing baseline model, which achieves an **MAE of 4.1 days**, 0.3 days greater than the original neural network approach.\n",
    "\n",
    "However, the predicted vs actual plots and error plots show that the model struggles to capture long stayers, and is biased to short stays. Further work is needed to improve the modelling approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1657784729985
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "models[\"final_model\"] = final_model\n",
    "\n",
    "# save models outside the git tree\n",
    "with open(\"../../models/regression.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(models, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model exploration\n",
    "\n",
    "A single performance metric can be a misleading summary of how a model performs. We will take the \"best performing\" baseline model, and explore in more detail how the model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1657784731045
    }
   },
   "outputs": [],
   "source": [
    "if model_name == \"catboost\":\n",
    "    model_X_test = X_test_catboost\n",
    "    model_y_test = y_test_catboost\n",
    "else:\n",
    "    model_X_test = X_test\n",
    "    model_y_test = y_test\n",
    "\n",
    "# setup a subplot figure\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "fig.set_size_inches(15, 7)\n",
    "\n",
    "# inference - ensure smallest LoS is 0 days (not negative value)\n",
    "preds = np.clip(\n",
    "    models[\"final_model\"][model_name][\"model\"].predict(model_X_test), 0, None\n",
    ")\n",
    "\n",
    "# calculate RMSE and range of LoS\n",
    "rmse = mean_squared_error(model_y_test, preds, squared=False)\n",
    "print(\n",
    "    f\"{model_name} test rmse: {rmse.round(3)} days, range ({preds.min().round(1)} - {preds.max().round(1)} days)\"\n",
    ")\n",
    "\n",
    "# create prediction dataframe\n",
    "predictions_df = pd.DataFrame(data=model_y_test.reset_index(drop=True))\n",
    "predictions_df[\"pred\"] = preds\n",
    "\n",
    "# calculate relative error\n",
    "predictions_df[\"error\"] = predictions_df.pred - predictions_df.LENGTH_OF_STAY\n",
    "\n",
    "# plot predicted vs actual\n",
    "axs[0].scatter(predictions_df.pred, predictions_df.LENGTH_OF_STAY, alpha=0.1)\n",
    "# plot ideal 1:1 prediction line. Max LoS = 30\n",
    "axs[0].plot(np.arange(0, 31), np.arange(0, 31), \"r--\")\n",
    "axs[0].set_xlabel(\"Predicted Length of Stay (days)\")\n",
    "axs[0].set_ylabel(\"Actual Length of Stay (days)\")\n",
    "axs[0].set_xlim([-1, 31])\n",
    "axs[0].set_ylim([-1, 31])\n",
    "axs[0].set_title(f\"{model_name} - RMSE {rmse.round(2)} days\")\n",
    "\n",
    "# plot relative error\n",
    "axs[1].scatter(predictions_df.LENGTH_OF_STAY, predictions_df.error, alpha=0.1)\n",
    "axs[1].set_xlabel(\"Length of Stay (days)\")\n",
    "axs[1].set_ylabel(\"Error (days)\")\n",
    "# plot mean relative error and 95% confidence intervals\n",
    "axs[1].plot(np.arange(0, 31), np.ones(31) * predictions_df.error.mean(), \"r\")\n",
    "axs[1].plot(\n",
    "    np.arange(0, 51),\n",
    "    np.ones(51) * (predictions_df.error.mean() + (2 * predictions_df.error.std())),\n",
    "    \"g--\",\n",
    ")\n",
    "axs[1].plot(\n",
    "    np.arange(0, 51),\n",
    "    np.ones(51) * (predictions_df.error.mean() - (2 * predictions_df.error.std())),\n",
    "    \"g--\",\n",
    ")\n",
    "# scale plot\n",
    "axs[1].set_xlim([-1, 31])\n",
    "# add statistical data in legend. LoA = limits of agreement\n",
    "# flag: errors are not normally distibuted so does 2*std capture 95% interval?\n",
    "\n",
    "axs[1].legend(\n",
    "    [\n",
    "        f\"\\u03bc ({np.round(predictions_df.error.mean(),2)} days)\",\n",
    "        f\"95% LoA (\\u03C3 {np.round(predictions_df.error.std(),2)} days gives {2*np.round(predictions_df.error.std(),2)})\",\n",
    "    ]\n",
    ")\n",
    "axs[1].set_title(f\"{model_name} - RMSE {rmse.round(2)} days\")\n",
    "\n",
    "fig.suptitle(\"Final model\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Feature importance\n",
    "\n",
    "Which features does the model ascribe predictive power to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1657740372079
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Feature names\n",
    "coef = pd.DataFrame(data=list(model_X_test.columns))\n",
    "# Feature importances, sorted\n",
    "coef[\"coef\"] = models[\"final_model\"][model_name][\"model\"].feature_importances_\n",
    "coef.sort_values(\"coef\", ascending=False, inplace=True)\n",
    "coef.set_index(0, inplace=True)\n",
    "# Plot interactive plot\n",
    "# Hover over a feature for full feature name\n",
    "fig = px.bar(coef, x=coef.index, y=\"coef\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Extensions\n",
    "\n",
    "- Build two separate regression models - one for long stayers (21+ days), one for not long-stayers.\n",
    "- Include IS_MINOR data in conjunction with above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6d65a8c07f5b6469e0fc613f182488c0dccce05038bbda39e5ac9075c0454d11"
  },
  "kernel_info": {
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
