{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling - Regression\n",
    "\n",
    "This notebook explores regression models to **predict Length of Stay (days)** as a baseline to the [Long Stayer Risk Stratification](https://github.com/nhsx/skunkworks-long-stayer-risk-stratification) model which achieved a Mean Absolute Error **(MAE) of 3.8 days** (2.2 median absolute error).\n",
    "\n",
    "This notebook is broken down into:\n",
    "\n",
    "1. Statistical tests to check the validity of linear models using Ordinary Least Squares (OLS)\n",
    "2. Training a range of baseline models using cross validation\n",
    "3. Testing final models on a test dataset\n",
    "4. Exploring in more detail the best performing baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1652704470089
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn import linear_model\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from scipy.stats import kstest, shapiro, anderson\n",
    "from catboost import CatBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = [15, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1652705474591
    }
   },
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "\n",
    "def train_model(gsc, X_train, y_train):\n",
    "    \"\"\"Uses a GridSearchCV instance to find a reasonable model, and store\n",
    "    performance and fitted model into a python dict\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "        gsc (sklearn.model_selection.GridSearchCV object): defined model\n",
    "        X_train (pandas dataframe): training dataframe with features\n",
    "        y_train (pandas dataframe): training dataframe with targets\n",
    "\n",
    "    Returns:\n",
    "\n",
    "        (dict): resulting fitted model and performance metrics\n",
    "    \"\"\"\n",
    "\n",
    "    grid_result = gsc.fit(X_train, y_train)\n",
    "\n",
    "    # note model fitted/scored on MSE\n",
    "    model = {\n",
    "        \"cv_mse_mean\": np.round(\n",
    "            -grid_result.cv_results_[\"mean_test_score\"][grid_result.best_index_], 3\n",
    "        ),\n",
    "        \"cv_mse_std\": np.round(\n",
    "            grid_result.cv_results_[\"std_test_score\"][grid_result.best_index_], 2\n",
    "        ),\n",
    "        \"model\": grid_result.best_estimator_,\n",
    "    }\n",
    "\n",
    "    # retrain the best estimator on the full training set - note that refit=True does not appear to do this\n",
    "    # note we calculate MAE as final metric\n",
    "    model[\"model\"].fit(X_train, y_train)\n",
    "    model[\"mae\"] = np.round(\n",
    "        mean_absolute_error(y_train, model[\"model\"].predict(X_train)), 3\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def risk_score(los):\n",
    "    \"\"\"Return risk score (1-5) based on LoS\n",
    "\n",
    "    Parameters:\n",
    "        los (float): length of stay in days\n",
    "\n",
    "    Returns:\n",
    "        (int): risk score (1 = Very low risk, 5 = High risk)\n",
    "    \"\"\"\n",
    "\n",
    "    # round los up to whole days\n",
    "    los = math.ceil(los)\n",
    "\n",
    "    if los > 15:\n",
    "        return 5\n",
    "    elif los > 13:\n",
    "        return 4\n",
    "    elif los > 10:\n",
    "        return 3\n",
    "    elif los > 6:\n",
    "        return 2\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1652704470877
    }
   },
   "outputs": [],
   "source": [
    "features = pd.read_parquet(\"../../data/features.parquet\")\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cap length of stay\n",
    "\n",
    "The highest length of stay is ~250 days, so we will check the distribution of length of stay and cap high values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1652704471349
    }
   },
   "outputs": [],
   "source": [
    "# Check distribution of length of stay\n",
    "features.groupby(by=\"LENGTH_OF_STAY\").count().AGE_ON_ADMISSION.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1652704471534
    }
   },
   "outputs": [],
   "source": [
    "# Cap maximum length of stay to 30 days\n",
    "features.LENGTH_OF_STAY = features.LENGTH_OF_STAY.apply(lambda x: 30 if x > 30 else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define target and training features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1652704471728
    }
   },
   "outputs": [],
   "source": [
    "X = features.drop(columns=\"LENGTH_OF_STAY\")\n",
    "y = features.LENGTH_OF_STAY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance Inflaction Factors\n",
    "\n",
    "Simple linear models (ordinary least squares) assume there is no multi-collinearity.\n",
    "\n",
    "Variance inflaction factors (VIF) help quantify the extent of any collinearity present.\n",
    "\n",
    "We are looking for VIF ~< 10 across our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes ~6 minutes to run on a STANDARD_DS3_V2\n",
    "vif = pd.DataFrame()\n",
    "vif[\"feature\"] = X.columns\n",
    "\n",
    "vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(len(X.columns))]\n",
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual distributions of OLS\n",
    "\n",
    "If VIF factors indicate a lack of multi-collinearity (they do not), check for normality of residuals aka homoescadisticity.\n",
    "\n",
    "This requires training a linear model, calculating the residuals and checking visually and statistically that they are normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train basic OLS model for statistical testing only\n",
    "reg = linear_model.LinearRegression()\n",
    "reg.fit(X, y)\n",
    "pred = pd.Series(reg.predict(X))\n",
    "resid = y - pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual inspection\n",
    "resid.hist(bins=30);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shapiro-Wilk test for normality\n",
    "\n",
    "* Null hypothesis = our residuals are drawn from normal distribution\n",
    "* Alternate hypothesis = our residuals are not drawn from normal distribution (and fail requirements of OLS model)\n",
    "* Test statistic shows how much distribution differs to normal distribution\n",
    "* p-value is probability null hypothesis true\n",
    "* p-value < 0.05 leads us to reject null hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapiro(resid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-sided Kolmogorov-Smirnov test for normality\n",
    "\n",
    "* Null hypothesis = our residuals are drawn from normal distribution\n",
    "* Alternate hypothesis = our residuals are not drawn from normal distribution (and fail requirements of OLS model)\n",
    "* Test statistic shows how much distribution differs to normal distribution\n",
    "* p-value is probability null hypothesis true\n",
    "* p-value < 0.05 leads us to reject null hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kstest(resid, \"norm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anderson-Darling test for normality\n",
    "\n",
    "* Null hypothesis = our residuals are drawn from normal distribution\n",
    "* Alternate hypothesis = our residuals are not drawn from normal distribution (and fail requirements of OLS model)\n",
    "* Test statistic is compared to critical value at the significance level required (e.g. 5%)\n",
    "* Test statistic > critical value for 5% significance level leads us to reject null hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anderson(resid, \"norm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Statistical testing invalidate assumptions for OLS models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/test split\n",
    "\n",
    "For model evaluation, we will hold back a 25% test set, and use cross-validation on the remaining 75% for all models until the final comparison is made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1652704594667
    }
   },
   "outputs": [],
   "source": [
    "# Split data for train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, train_size=0.25, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "Strategy is to try a number of regression models with:\n",
    "\n",
    "* GridsearchCV for hyperparameter tuning with cross validation, refitting full training set for best model\n",
    "* Test all final models against the held-out test set.\n",
    "\n",
    "OLS models are excluded due to statistical assumptions not being met. NN are excluded due to complexity/interpretability issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1652705481582
    }
   },
   "outputs": [],
   "source": [
    "# Initiate empty models dictionary\n",
    "models = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean model\n",
    "\n",
    "The simplest baseline model takes the mean length of stay as its prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1652705486989
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"mean\"\n",
    "\n",
    "# define gridsearch parameters\n",
    "gsc = GridSearchCV(\n",
    "    estimator=DummyRegressor(strategy=\"mean\"),\n",
    "    param_grid={},\n",
    "    cv=5,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    refit=True,\n",
    ")\n",
    "\n",
    "# takes ~1 seconds to run on a STANDARD_DS3_V2\n",
    "models[model_name] = train_model(gsc, X_train, y_train)\n",
    "models[model_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic net regression\n",
    "\n",
    "A regularised linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1652705516671
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"elastic\"\n",
    "\n",
    "# define gridsearch parameters\n",
    "gsc = GridSearchCV(\n",
    "    estimator=linear_model.ElasticNet(),\n",
    "    param_grid={\n",
    "        \"l1_ratio\": [0.1, 0.5, 0.7, 0.9, 0.95, 0.99, 1],\n",
    "    },\n",
    "    cv=5,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    refit=True,\n",
    ")\n",
    "\n",
    "# takes ~2 seconds to run on a STANDARD_DS3_V2\n",
    "models[model_name] = train_model(gsc, X_train, y_train)\n",
    "models[model_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1652706477018
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"randomforest\"\n",
    "\n",
    "# define gridsearch parameters\n",
    "gsc = GridSearchCV(\n",
    "    estimator=RandomForestRegressor(),\n",
    "    param_grid={\"n_estimators\": [10, 100, 1000], \"max_depth\": [5, 10, None]},\n",
    "    cv=5,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    refit=True,\n",
    ")\n",
    "\n",
    "# takes ~15 mins to run on a STANDARD_DS3_V2\n",
    "models[model_name] = train_model(gsc, X_train, y_train)\n",
    "models[model_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catboost\n",
    "\n",
    "Boosted tree optimised for categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1652706888015
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"catboost\"\n",
    "\n",
    "# extract categorical features\n",
    "num_features = [\n",
    "    \"AGE_ON_ADMISSION\",\n",
    "    \"EL CountLast12m\",\n",
    "    \"EMCountLast12m\",\n",
    "    \"OP First CountLast12m\",\n",
    "    \"OP FU CountLast12m\",\n",
    "]\n",
    "cat_features = list(set(X_train.columns) - set(num_features))\n",
    "\n",
    "# define gridsearch parameters\n",
    "gsc = GridSearchCV(\n",
    "    estimator=CatBoostRegressor(verbose=False, cat_features=cat_features),\n",
    "    param_grid={\n",
    "        \"max_depth\": [5, 10, None],\n",
    "        \"learning_rate\": [0.01, 0.1, 1],\n",
    "        \"iterations\": [10, 100, 1000],\n",
    "    },\n",
    "    cv=5,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    refit=True,\n",
    ")\n",
    "\n",
    "# takes ~7 mins to run on a STANDARD_DS3_V2\n",
    "models[model_name] = train_model(gsc, X_train, y_train)\n",
    "models[model_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1652706970050
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"xgboost\"\n",
    "\n",
    "# define gridsearch parameters\n",
    "gsc = GridSearchCV(\n",
    "    estimator=XGBRegressor(random_state=42),\n",
    "    param_grid={\n",
    "        \"n_estimators\": [1, 5],\n",
    "        \"learning_rate\": [0.01, 0.1, 1],\n",
    "        \"max_depth\": [5, 10, None],\n",
    "    },\n",
    "    cv=5,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    refit=True,\n",
    ")\n",
    "\n",
    "# takes ~1 mins to run on a STANDARD_DS3_V2\n",
    "models[model_name] = train_model(gsc, X_train, y_train)\n",
    "models[model_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1652706991554
    }
   },
   "outputs": [],
   "source": [
    "# save models outside the git tree\n",
    "with open(\"../../models/regression.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(models, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1652706998600
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# load models from outside the git tree\n",
    "with open(\"../../models/regression.pickle\", \"rb\") as handle:\n",
    "    models = pickle.load(handle)\n",
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate models\n",
    "\n",
    "Use the held-out test set to evaluate the performance of all the tuned models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1652707415391
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    # ensure smallest LoS is 0 days (not negative value)\n",
    "    preds = np.clip(models[model][\"model\"].predict(X_test), 0, None)\n",
    "    # calculate MAE and range of LoS\n",
    "    mae = mean_absolute_error(y_test, preds)\n",
    "    print(\n",
    "        f\"{model} test mae: {mae.round(3)} ({preds.min().round(1)} - {preds.max().round(1)} days)\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model exploration\n",
    "\n",
    "A single performance metric can be a misleading summary of how a model performs. We will take the \"best performing\" baseline model, and explore in more detail how the model performs.\n",
    "\n",
    "Todo:\n",
    "\n",
    "- [ ] Retrain simpler model with only top ~10 features?\n",
    "- [ ] Fairness analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1652707586368
    }
   },
   "outputs": [],
   "source": [
    "model = \"catboost\"\n",
    "actual = y_test.reset_index(drop=True)  # extract actual LoS\n",
    "preds = pd.Series(\n",
    "    np.clip(models[model][\"model\"].predict(X_test), 0, None)\n",
    ")  # cast to series for consistency\n",
    "mae = mean_absolute_error(actual, preds)\n",
    "print(f\"MAE: {mae}, min LoS: {preds.min()}, max LoS: {preds.max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual vs predicted plot\n",
    "\n",
    "The maximum length of stay the model can predict is ~9 days, which is well short of the 30 day actual. \n",
    "\n",
    "Let's visualise model performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1652707586866
    }
   },
   "outputs": [],
   "source": [
    "# plot predicted vs actual\n",
    "plt.scatter(x=preds, y=actual, alpha=0.1)\n",
    "# plot ideal 1:1 prediction line. Max LoS = 30\n",
    "plt.plot(np.arange(0, 31), np.arange(0, 31), \"r--\")\n",
    "plt.xlabel(\"Predicted LoS\")\n",
    "plt.ylabel(\"Actual LoS\")\n",
    "# scale to ~[0,30] for consistency with other plots\n",
    "plt.xlim([-1, 31])\n",
    "plt.ylim([-1, 31]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relative error by length of stay plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1652707588411
    }
   },
   "outputs": [],
   "source": [
    "# Create dataframe with actual values\n",
    "errors_df = pd.DataFrame(data=actual).reset_index(drop=True)\n",
    "# Calculate relative error\n",
    "errors_df[\"error\"] = errors_df.LENGTH_OF_STAY - preds\n",
    "# Plot relative error\n",
    "errors_df.plot.scatter(x=\"LENGTH_OF_STAY\", y=\"error\", alpha=0.1)\n",
    "# Plot mean relative error and 95% confidence intervals\n",
    "# Flag: errors are not normally distibuted so does 2*std capture 5%?\n",
    "plt.plot(np.arange(0, 31), np.ones(31) * errors_df.error.mean(), \"r\")\n",
    "plt.plot(\n",
    "    np.arange(0, 51),\n",
    "    np.ones(51) * (errors_df.error.mean() + (2 * errors_df.error.std())),\n",
    "    \"g--\",\n",
    ")\n",
    "plt.plot(\n",
    "    np.arange(0, 51),\n",
    "    np.ones(51) * (errors_df.error.mean() - (2 * errors_df.error.std())),\n",
    "    \"g--\",\n",
    ")\n",
    "plt.xlim([-1, 31])\n",
    "# scale plot\n",
    "plt.legend(\n",
    "    [\n",
    "        f\"\\u03bc ({np.round(errors_df.error.mean(),2)} days)\",\n",
    "        f\"95% LoA (\\u03C3 {np.round(errors_df.error.std(),2)} days gives {2*np.round(errors_df.error.std(),2)})\",\n",
    "    ]\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Risk score equivalence\n",
    "\n",
    "In addition to a length of stay prediction (days), we will define a risk score of a patient becoming a long stayer:\n",
    "\n",
    "Risk Category|Day Range for Risk Category\n",
    "-----|------\n",
    "1 - Very low risk|0-6\n",
    "2 - Low risk|7-10\n",
    "3 - Normal risk|11-13\n",
    "4 - Elevated risk|14-15\n",
    "5 - High risk|>15\n",
    "\n",
    "We can convert the length of stay prediction into a risk score, and visualise the performance of this approach. Later, we will look at classification models to do this directly.\n",
    "\n",
    "#### Calculate risk scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1652707589497
    }
   },
   "outputs": [],
   "source": [
    "# calculate risk scores in a new dataframe\n",
    "risk = pd.DataFrame()\n",
    "# extract LoS values\n",
    "risk[\"los\"] = actual\n",
    "risk[\"los_predicted\"] = preds\n",
    "# calculate equivalent risk categories\n",
    "risk[\"risk\"] = risk.los.apply(lambda x: risk_score(x))  # actual risk\n",
    "risk[\"risk_predicted\"] = risk.los_predicted.apply(\n",
    "    lambda x: risk_score(x)\n",
    ")  # risk as predicted by regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate errors in risk stratification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1652707591451
    }
   },
   "outputs": [],
   "source": [
    "risk_labels = [\n",
    "    \"1 - Very low risk\",\n",
    "    \"2 - Low risk\",\n",
    "    \"3 - Normal risk\",\n",
    "    \"4 - Elevated risk\",\n",
    "    \"5 - High risk\",\n",
    "]\n",
    "risks = dict.fromkeys(risk_labels)\n",
    "for r in risks:\n",
    "    risks[r] = np.array([0.0, 0.0, 0.0, 0.0, 0.0])  # create empty values\n",
    "\n",
    "    for label in risk_labels:\n",
    "        this_risk = int(label[0])\n",
    "        # extract the real los for each predicted risk category\n",
    "        subset = risk[risk.risk_predicted == this_risk]\n",
    "\n",
    "        if r == \"1 - Very low risk\":\n",
    "            prop = (subset.los < 7).sum() / subset.shape[0]\n",
    "        elif r == \"2 - Low risk\":\n",
    "            prop = (subset[subset.los >= 7].los <= 10).sum() / subset.shape[0]\n",
    "        elif r == \"3 - Normal risk\":\n",
    "            prop = (subset[subset.los >= 11].los <= 13).sum() / subset.shape[0]\n",
    "        elif r == \"4 - Elevated risk\":\n",
    "            prop = (subset[subset.los >= 14].los <= 15).sum() / subset.shape[0]\n",
    "        elif r == \"5 - High risk\":\n",
    "            prop = (subset.los > 15).sum() / subset.shape[0]\n",
    "\n",
    "        risks[r][this_risk - 1] = prop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot actual risk vs predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1652707592799
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "bottom = np.array([0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "for r in risks:\n",
    "    if r == \"1 - Very low risk\":\n",
    "        data = risks[r]\n",
    "        ax.bar(risk_labels, data, label=r, width=0.35)\n",
    "    else:\n",
    "        bottom += data\n",
    "        data = risks[r]\n",
    "        ax.bar(risk_labels, data, label=r, bottom=bottom, width=0.35)\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles[::-1], risk_labels[::-1], bbox_to_anchor=(1.05, 1))\n",
    "ax.set_ylabel(\"Actual risk category\")\n",
    "ax.set_xlabel(\"Predicted risk cateogry\");"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6d65a8c07f5b6469e0fc613f182488c0dccce05038bbda39e5ac9075c0454d11"
  },
  "kernel_info": {
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
