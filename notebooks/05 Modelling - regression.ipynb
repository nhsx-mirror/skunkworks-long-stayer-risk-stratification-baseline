{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling - Regression\n",
    "\n",
    "This notebook explores regression models to **predict Length of Stay (days)** as a baseline to the [Long Stayer Risk Stratification](https://github.com/nhsx/skunkworks-long-stayer-risk-stratification) model which achieved a Mean Absolute Error **(MAE) of 3.8 days** (2.2 median absolute error).\n",
    "\n",
    "This notebook is broken down into:\n",
    "\n",
    "1. Statistical tests to check the validity of linear models using Ordinary Least Squares (OLS)\n",
    "2. Training a range of baseline models using cross validation\n",
    "3. Testing final models on a test dataset\n",
    "4. Exploring in more detail the best performing baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1651741729555
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from sklearn import linear_model\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from scipy.stats import kstest, shapiro, anderson\n",
    "from catboost import CatBoostRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = [15, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(gsc, X_train, y_train):\n",
    "    \"\"\"Uses a GridSearchCV instance to find a reasonable model, and store\n",
    "    performance and fitted model into a python dict\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "        gsc (sklearn.model_selection.GridSearchCV object): defined model\n",
    "        X_train (pandas dataframe): training dataframe with features\n",
    "        y_train (pandas dataframe): training dataframe with targets\n",
    "\n",
    "    Returns:\n",
    "\n",
    "        (dict): resulting fitted model and performance metrics\n",
    "    \"\"\"\n",
    "\n",
    "    grid_result = gsc.fit(X_train, y_train)\n",
    "\n",
    "    model = {\n",
    "        \"cv_mae_mean\": np.round(\n",
    "            -grid_result.cv_results_[\"mean_test_score\"][grid_result.best_index_], 3\n",
    "        ),\n",
    "        \"cv_mae_std\": np.round(\n",
    "            grid_result.cv_results_[\"std_test_score\"][grid_result.best_index_], 2\n",
    "        ),\n",
    "        \"model\": grid_result.best_estimator_,\n",
    "    }\n",
    "\n",
    "    # retrain the best estimator on the full training set - note that refit=True does not appear to do this\n",
    "    model[\"model\"].fit(X_train, y_train)\n",
    "    model[\"mae\"] = np.round(\n",
    "        mean_absolute_error(y_train, model[\"model\"].predict(X_train)), 3\n",
    "    )\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1651741399055
    }
   },
   "outputs": [],
   "source": [
    "features = pd.read_parquet(\"../../data/features.parquet\")\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cap length of stay\n",
    "\n",
    "The highest length of stay is ~250 days, so we will check the distribution of length of stay and cap high values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1651600413781
    }
   },
   "outputs": [],
   "source": [
    "# Check distribution of length of stay\n",
    "features.groupby(by=\"LENGTH_OF_STAY\").count().AGE_ON_ADMISSION.plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1651741399290
    }
   },
   "outputs": [],
   "source": [
    "# Cap maximum length of stay to 30 days\n",
    "features.LENGTH_OF_STAY = features.LENGTH_OF_STAY.apply(lambda x: 30 if x > 30 else x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define target and training features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1651741399519
    }
   },
   "outputs": [],
   "source": [
    "X = features.drop(columns=\"LENGTH_OF_STAY\")\n",
    "y = features.LENGTH_OF_STAY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance Inflaction Factors\n",
    "\n",
    "Simple linear models (ordinary least squares) assume there is no multi-collinearity.\n",
    "\n",
    "Variance inflaction factors (VIF) help quantify the extent of any collinearity present.\n",
    "\n",
    "We are looking for VIF ~< 10 across our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes ~6 minutes to run on a STANDARD_DS3_V2\n",
    "vif = pd.DataFrame()\n",
    "vif[\"feature\"] = X.columns\n",
    "\n",
    "vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(len(X.columns))]\n",
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual distributions of OLS\n",
    "\n",
    "If VIF factors indicate a lack of multi-collinearity (they do not), check for normality of residuals aka homoescadisticity.\n",
    "\n",
    "This requires training a linear model, calculating the residuals and checking visually and statistically that they are normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train basic OLS model for statistical testing only\n",
    "reg = linear_model.LinearRegression()\n",
    "reg.fit(X, y)\n",
    "pred = pd.Series(reg.predict(X))\n",
    "resid = y - pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visual inspection\n",
    "resid.hist(bins=30);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shapiro-Wilk test for normality\n",
    "\n",
    "* Null hypothesis = our residuals are drawn from normal distribution\n",
    "* Alternate hypothesis = our residuals are not drawn from normal distribution (and fail requirements of OLS model)\n",
    "* Test statistic shows how much distribution differs to normal distribution\n",
    "* p-value is probability null hypothesis true\n",
    "* p-value < 0.05 leads us to reject null hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapiro(resid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-sided Kolmogorov-Smirnov test for normality\n",
    "\n",
    "* Null hypothesis = our residuals are drawn from normal distribution\n",
    "* Alternate hypothesis = our residuals are not drawn from normal distribution (and fail requirements of OLS model)\n",
    "* Test statistic shows how much distribution differs to normal distribution\n",
    "* p-value is probability null hypothesis true\n",
    "* p-value < 0.05 leads us to reject null hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kstest(resid, \"norm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anderson-Darling test for normality\n",
    "\n",
    "* Null hypothesis = our residuals are drawn from normal distribution\n",
    "* Alternate hypothesis = our residuals are not drawn from normal distribution (and fail requirements of OLS model)\n",
    "* Test statistic is compared to critical value at the significance level required (e.g. 5%)\n",
    "* Test statistic > critical value for 5% significance level leads us to reject null hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anderson(resid, \"norm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Statistical testing invalidate assumptions for OLS models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/test split\n",
    "\n",
    "For model evaluation, we will hold back a 25% test set, and use cross-validation on the remaining 75% for all models until the final comparison is made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1651741399698
    }
   },
   "outputs": [],
   "source": [
    "# Split data for train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, train_size=0.25, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "Strategy is to try a number of regression models with:\n",
    "\n",
    "* GridsearchCV for hyperparameter tuning with cross validation, refitting full training set for best model\n",
    "* Test all final models against the held-out test set.\n",
    "\n",
    "OLS models are excluded due to statistical assumptions not being met. NN are excluded due to complexity/interpretability issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate empty models dictionary\n",
    "models = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean model\n",
    "\n",
    "The simplest baseline model takes the mean length of stay as its prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mean\"\n",
    "\n",
    "# define gridsearch parameters\n",
    "gsc = GridSearchCV(\n",
    "    estimator=DummyRegressor(strategy=\"mean\"),\n",
    "    param_grid={},\n",
    "    cv=5,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    refit=True,\n",
    ")\n",
    "\n",
    "# takes ~1 seconds to run on a STANDARD_DS3_V2\n",
    "models[model_name] = train_model(gsc, X_train, y_train)\n",
    "models[model_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic net regression\n",
    "\n",
    "A regularised linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"elastic\"\n",
    "\n",
    "# define gridsearch parameters\n",
    "gsc = GridSearchCV(\n",
    "    estimator=linear_model.ElasticNet(),\n",
    "    param_grid={\n",
    "        \"l1_ratio\": [0.1, 0.5, 0.7, 0.9, 0.95, 0.99, 1],\n",
    "    },\n",
    "    cv=5,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    refit=True,\n",
    ")\n",
    "\n",
    "# takes ~2 seconds to run on a STANDARD_DS3_V2\n",
    "models[model_name] = train_model(gsc, X_train, y_train)\n",
    "models[model_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1651601553361
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"randomforest\"\n",
    "\n",
    "# define gridsearch parameters\n",
    "gsc = GridSearchCV(\n",
    "    estimator=RandomForestRegressor(),\n",
    "    param_grid={\"n_estimators\": [10, 100, 1000], \"max_depth\": [5, 10, None]},\n",
    "    cv=5,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    refit=True,\n",
    ")\n",
    "\n",
    "# takes ~15 mins to run on a STANDARD_DS3_V2\n",
    "models[model_name] = train_model(gsc, X_train, y_train)\n",
    "models[model_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catboost\n",
    "\n",
    "Boosted tree optimised for categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1651601553471
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"catboost\"\n",
    "\n",
    "# extract categorical features\n",
    "num_features = [\n",
    "    \"AGE_ON_ADMISSION\",\n",
    "    \"EL CountLast12m\",\n",
    "    \"EMCountLast12m\",\n",
    "    \"OP First CountLast12m\",\n",
    "    \"OP FU CountLast12m\",\n",
    "]\n",
    "cat_features = list(set(X_train.columns) - set(num_features))\n",
    "\n",
    "# define gridsearch parameters\n",
    "gsc = GridSearchCV(\n",
    "    estimator=CatBoostRegressor(verbose=False, cat_features=cat_features),\n",
    "    param_grid={\n",
    "        \"max_depth\": [5, 10, None],\n",
    "        \"learning_rate\": [0.01, 0.1, 1],\n",
    "        \"iterations\": [10, 100, 1000],\n",
    "    },\n",
    "    cv=5,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    refit=True,\n",
    ")\n",
    "\n",
    "# takes ~20 mins to run on a STANDARD_DS3_V2\n",
    "models[model_name] = train_model(gsc, X_train, y_train)\n",
    "models[model_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"xgboost\"\n",
    "\n",
    "# define gridsearch parameters\n",
    "gsc = GridSearchCV(\n",
    "    estimator=XGBRegressor(random_state=42),\n",
    "    param_grid={\n",
    "        \"n_estimators\": [1, 5],\n",
    "        \"learning_rate\": [0.01, 0.1, 1],\n",
    "        \"max_depth\": [5, 10, None],\n",
    "    },\n",
    "    cv=5,\n",
    "    scoring=\"neg_mean_absolute_error\",\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    refit=True,\n",
    ")\n",
    "\n",
    "# takes ~1 mins to run on a STANDARD_DS3_V2\n",
    "models[model_name] = train_model(gsc, X_train, y_train)\n",
    "models[model_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save models outside the git tree\n",
    "with open(\"../../models/regression.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(models, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1651654183991
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# load models from outside the git tree\n",
    "with open(\"../../models/regression.pickle\", \"rb\") as handle:\n",
    "    models = pickle.load(handle)\n",
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate models\n",
    "\n",
    "Use the held-out test set to evaluate the performance of all the tuned models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    preds = models[model][\"model\"].predict(X_test)\n",
    "    mae = mean_absolute_error(y_test, preds)\n",
    "    print(f\"{model} test mae: {mae.round(3)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model exploration\n",
    "\n",
    "A single performance metric can be a misleading summary of how a model performs. We will take the \"best performing\" baseline model, an XGBoostRegressor, and explore in more detail how the model performs.\n",
    "\n",
    "Todo:\n",
    "\n",
    "- [ ] Plot predicted vs actual\n",
    "- [ ] Plot errors\n",
    "- [ ] Convert into risk scores\n",
    "- [ ] Explore feature importance\n",
    "- [ ] Retrain simpler model with only top ~10 features?\n",
    "- [ ] Fairness analysis"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6d65a8c07f5b6469e0fc613f182488c0dccce05038bbda39e5ac9075c0454d11"
  },
  "kernel_info": {
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
