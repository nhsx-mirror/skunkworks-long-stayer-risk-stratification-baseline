{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Feature engineering\n",
    "\n",
    "This notebook processes cleaned data into the feature set used for modelling.\n",
    "\n",
    "The decisions around feature engineering are the culmination of a number of explorations of the data, including modelling of the full dataset, which is not included in this repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1655740449241
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1655740450386
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "clean_data_df = pd.read_parquet(\"../../data/interim/clean-data.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add derived fields\n",
    "\n",
    "These were removed during cleaning due to missing data, and can be recalculated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1655740450664
    }
   },
   "outputs": [],
   "source": [
    "derived_df = clean_data_df.copy()\n",
    "derived_df[\n",
    "    \"arrival_day_of_week\"\n",
    "] = derived_df.START_DATE_TIME_HOSPITAL_PROVIDER_SPELL.dt.day_name().str[:3]\n",
    "derived_df[\n",
    "    \"arrival_month_name\"\n",
    "] = derived_df.START_DATE_TIME_HOSPITAL_PROVIDER_SPELL.dt.month_name().str[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Select agreed columns\n",
    "\n",
    "As agreed with data SME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1655741368491
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "columns = [\n",
    "    \"ae_arrival_mode\",\n",
    "    \"AGE_ON_ADMISSION\",\n",
    "    \"EL CountLast12m\",\n",
    "    \"EMCountLast12m\",\n",
    "    \"IS_illness_not_injury\",\n",
    "    \"IS_cancer\",\n",
    "    \"IS_care_home_on_admission\",\n",
    "    \"IS_chronic_kidney_disease\",\n",
    "    \"IS_COPD\",\n",
    "    \"IS_coronary_heart_disease\",\n",
    "    \"IS_dementia\",\n",
    "    \"IS_diabetes\",\n",
    "    \"IS_frailty_proxy\",\n",
    "    \"IS_hypertension\",\n",
    "    \"IS_mental_health\",\n",
    "    \"MAIN_SPECIALTY_CODE_AT_ADMISSION_DESCRIPTION\",\n",
    "    \"OP First CountLast12m\",\n",
    "    \"OP FU CountLast12m\",\n",
    "    \"SOURCE_OF_ADMISSION_HOSPITAL_PROVIDER_SPELL_DESCRIPTION\",\n",
    "    \"stroke_ward_stay\",\n",
    "    \"LENGTH_OF_STAY\",\n",
    "    \"arrival_day_of_week\",\n",
    "    \"arrival_month_name\",\n",
    "]\n",
    "\n",
    "# define sensitive columns for fairness testing later\n",
    "sensitive_columns = [\n",
    "    \"ETHNIC_CATEGORY_CODE_DESCRIPTION\",\n",
    "    \"IMD county decile\",\n",
    "    \"OAC Group Name\",\n",
    "    \"OAC Subgroup Name\",\n",
    "    \"OAC Supergroup Name\",\n",
    "    \"PATIENT_GENDER_CURRENT_DESCRIPTION\",\n",
    "    \"POST_CODE_AT_ADMISSION_DATE_DISTRICT\",\n",
    "    \"Rural urban classification\",\n",
    "]\n",
    "\n",
    "subset_df = derived_df[columns + sensitive_columns]\n",
    "\n",
    "# Check if we have null values in the resulting dataset:\n",
    "print(subset_df.isna().sum().sum(), \"null value(s)\")\n",
    "\n",
    "subset_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot encode categorical data\n",
    "\n",
    "One-hot encoding is performed without sensitive features. This is so that when we are testing for fairness later, we can compare model performance on models trained without sensitive features.\n",
    "\n",
    "* For categories with null values, we will use `dummy_na=True` to encode null values.\n",
    "* For categories without null values, we will set `dummy_na=False` to ensure we don't have an empty column.\n",
    "\n",
    "We will have to split out categorical and non-categorical columns first, then check for null values to pass in the correct parameters for `get_dummies`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1655741471127
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "training_features = list(set(subset_df.columns) - set(sensitive_columns))\n",
    "categorical_features = list(\n",
    "    subset_df[training_features]\n",
    "    .dtypes[subset_df[training_features].dtypes == \"object\"]\n",
    "    .index\n",
    ")\n",
    "numerical_features = list(\n",
    "    subset_df[training_features]\n",
    "    .dtypes[subset_df[training_features].dtypes != \"object\"]\n",
    "    .index\n",
    ")\n",
    "\n",
    "# calculate null values in categorical features\n",
    "\n",
    "null_categorical_features = list(\n",
    "    subset_df[categorical_features]\n",
    "    .isna()\n",
    "    .sum()[subset_df[categorical_features].isna().sum() > 0]\n",
    "    .index\n",
    ")\n",
    "not_null_categorical_features = list(\n",
    "    set(categorical_features) - set(null_categorical_features)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1655741480363
    }
   },
   "outputs": [],
   "source": [
    "# To avoid the \"dummy variable trap\", we could drop the first category of these features to reduce duplication.\n",
    "# However, we may lose interpretability if e.g. Monday is dropped and is an important feature?\n",
    "encoded_df = pd.get_dummies(\n",
    "    subset_df.drop(columns=sensitive_columns),\n",
    "    columns=not_null_categorical_features,\n",
    "    drop_first=False,\n",
    "    dummy_na=False,\n",
    ")\n",
    "encoded_df = pd.get_dummies(\n",
    "    encoded_df, columns=null_categorical_features, drop_first=False, dummy_na=True\n",
    ")\n",
    "print(encoded_df.shape)\n",
    "# Add back in the sensitive columns, without encoding\n",
    "encoded_sensitive_df = encoded_df.copy()\n",
    "encoded_sensitive_df[sensitive_columns] = subset_df[sensitive_columns]\n",
    "print(encoded_sensitive_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1655741518122
    }
   },
   "outputs": [],
   "source": [
    "corr = encoded_df.corr()\n",
    "# check for correlation of feature with LENGTH_OF_STAY\n",
    "corr.LENGTH_OF_STAY[corr.LENGTH_OF_STAY.abs().sort_values(ascending=False).index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1655741520118
    }
   },
   "outputs": [],
   "source": [
    "encoded_df.to_parquet(\"../../data/processed/features.parquet\")\n",
    "encoded_sensitive_df.to_parquet(\"../../data/processed/features-sensitive.parquet\")\n",
    "\n",
    "# Some machine learning algorithms e.g. catboost require NOT to one-hot encode data, so export for these\n",
    "subset_df.drop(columns=sensitive_columns).to_parquet(\n",
    "    \"../../data/processed/features-catboost.parquet\"\n",
    ")\n",
    "subset_df.to_parquet(\"../../data/processed/features-sensitive-catboost.parquet\")"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
