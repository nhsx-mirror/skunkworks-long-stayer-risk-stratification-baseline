{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Model comparison\n",
    "\n",
    "We have now trained a Regression model (length of stay) and a Classification model (risk 1-5).\n",
    "\n",
    "We can create an equivalent risk model from the risk categories using the predicted Length of Stay:\n",
    "\n",
    "Risk Category|Day Range for Risk Category\n",
    "-----|------\n",
    "1 - Very low risk|0-6\n",
    "2 - Low risk|7-10\n",
    "3 - Normal risk|11-13\n",
    "4 - Elevated risk|14-15\n",
    "5 - High risk|>15\n",
    "\n",
    "We have a number of approaches where we will compare side by side plots for risk stratification:\n",
    "\n",
    "Model|Regression version|Classification version\n",
    "---|---|---\n",
    "Dummy|Mean|Prior\n",
    "Simple|LogReg|ElasticNet\n",
    "Decision Tree|DecisionTreeRegressor|DecisionTreeClassifier\n",
    "Random Forest|RandomForestRegressor|RandomForestClassifier\n",
    "Catboost|CatBoostRegressor|CatBoostClassifier\n",
    "XGBoost|XGBRegressor|XGBClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1653386731477
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = [15, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1653386709738
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "\n",
    "def risk_score(los):\n",
    "    \"\"\"Return risk score (1-5) based on LoS\n",
    "\n",
    "    Parameters:\n",
    "        los (float): length of stay in days\n",
    "\n",
    "    Returns:\n",
    "        (int): risk score (1 = Very low risk, 5 = High risk)\n",
    "    \"\"\"\n",
    "\n",
    "    # round los up to whole days\n",
    "    los = math.ceil(los)\n",
    "\n",
    "    if los > 15:\n",
    "        return 5\n",
    "    elif los > 13:\n",
    "        return 4\n",
    "    elif los > 10:\n",
    "        return 3\n",
    "    elif los > 6:\n",
    "        return 2\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1653387625710
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "features_df = pd.read_parquet(\"../../data/features.parquet\")\n",
    "features_catboost_df = pd.read_parquet(\"../../data/features-catboost.parquet\")\n",
    "# add actual risk scores\n",
    "risk_labels = [\n",
    "    \"1 - Very Low Risk\",\n",
    "    \"2 - Low Risk\",\n",
    "    \"3 - Normal Risk\",\n",
    "    \"4 - Elevated Risk\",\n",
    "    \"5 - High Risk\",\n",
    "]\n",
    "features_df[\"risk\"] = [risk_score(los) for los in features_df.LENGTH_OF_STAY]\n",
    "features_catboost_df[\"risk\"] = [\n",
    "    risk_score(los) for los in features_catboost_df.LENGTH_OF_STAY\n",
    "]\n",
    "# separate training and target features\n",
    "X = features_df.drop(columns=[\"LENGTH_OF_STAY\"])\n",
    "y = features_df.risk\n",
    "\n",
    "# non-one-hot encoded data for catboost\n",
    "X_catboost = features_catboost_df.drop(columns=[\"LENGTH_OF_STAY\"])\n",
    "y_catboost = features_catboost_df.risk\n",
    "\n",
    "# separate training and test data\n",
    "# split data for train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, train_size=0.75, random_state=42\n",
    ")\n",
    "print(X_train.shape, X_test.shape)\n",
    "\n",
    "# Scale data for LogReg only using training data\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train_scaled = pd.DataFrame(\n",
    "    scaler.transform(X_train), index=X_train.index, columns=X_train.columns\n",
    ")\n",
    "X_test_scaled = pd.DataFrame(\n",
    "    scaler.transform(X_test), index=X_test.index, columns=X_test.columns\n",
    ")\n",
    "print(X_train_scaled.shape, X_test_scaled.shape)\n",
    "\n",
    "# Split data for train/test\n",
    "X_train_catboost, X_test_catboost, y_train_catboost, y_test_catboost = train_test_split(\n",
    "    X_catboost, y_catboost, train_size=0.75, random_state=42\n",
    ")\n",
    "print(X_train_catboost.shape, X_test_catboost.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1653388056956
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# load models from outside the git tree\n",
    "with open(\"../../models/regression.pickle\", \"rb\") as handle:\n",
    "    models_regression = pickle.load(handle)\n",
    "models_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1653386416631
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# load models from outside the git tree\n",
    "with open(\"../../models/classification.pickle\", \"rb\") as handle:\n",
    "    models_classification = pickle.load(handle)\n",
    "models_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Compare predicted risk score (classification) with equivalent-predicted risk score (regression)\n",
    "\n",
    "Classification -> Risk score\n",
    "\n",
    "Regression -> Length Of Stay -> Equivalent risk score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1653388742572
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# setup a subplot figure\n",
    "fig, axs = plt.subplots(len(models_classification), 2)\n",
    "fig.set_size_inches(15, 7 * len(models_classification))\n",
    "\n",
    "i = 0\n",
    "\n",
    "for model_classification in models_classification:\n",
    "    if model_classification == \"catboost\":\n",
    "        model_classification_X_test = X_test_catboost.drop(columns=\"risk\")\n",
    "        model_classification_y_test = y_test\n",
    "    elif model_classification == \"logreg\":\n",
    "        model_classification_X_test = X_test_scaled.drop(columns=\"risk\")\n",
    "        model_classification_y_test = y_test\n",
    "    else:\n",
    "        model_classification_X_test = X_test.drop(columns=\"risk\")\n",
    "        model_classification_y_test = y_test\n",
    "\n",
    "    # logreg is being compared to elastic net regression\n",
    "    if model_classification == \"logreg\":\n",
    "        model_regression_X_test = X_test.drop(columns=\"risk\")\n",
    "        model_regression_y_test = y_test\n",
    "    else:\n",
    "        model_regression_X_test = model_classification_X_test\n",
    "        model_regression_y_test = model_classification_y_test\n",
    "\n",
    "    # logreg is being compared to elastic net regression\n",
    "    # prior is being compared to mean\n",
    "    if model_classification == \"logreg\":\n",
    "        model_regression = \"elastic\"\n",
    "    elif model_classification == \"prior\":\n",
    "        model_regression = \"mean\"\n",
    "    else:\n",
    "        model_regression = model_classification\n",
    "\n",
    "    # perform inference\n",
    "    preds_regression = models_regression[model_regression][\"model\"].predict(\n",
    "        model_regression_X_test\n",
    "    )\n",
    "    preds_classification = models_classification[model_classification][\"model\"].predict(\n",
    "        model_classification_X_test\n",
    "    )\n",
    "\n",
    "    # create a prediction dataframe\n",
    "    predictions_df = pd.DataFrame(\n",
    "        data=model_classification_y_test.reset_index(drop=True)\n",
    "    )\n",
    "    predictions_df[\"pred_regression_los\"] = preds_regression\n",
    "    # calculate equivalent risk score from regression model\n",
    "    predictions_df[\"pred_regression\"] = [\n",
    "        risk_score(los) for los in predictions_df.pred_regression_los\n",
    "    ]\n",
    "    predictions_df[\"pred_classification\"] = preds_classification\n",
    "\n",
    "    # plot actual vs predicted CLASSES for classification\n",
    "    risks = dict.fromkeys(risk_labels)\n",
    "    for proportion in risks:\n",
    "        risks[proportion] = np.array([0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "\n",
    "        for label in risk_labels:\n",
    "            this_risk = int(label[0])\n",
    "\n",
    "            # extract the real risk\n",
    "            subset = predictions_df[predictions_df.risk == this_risk]\n",
    "\n",
    "            if proportion == \"1 - Very Low Risk\":\n",
    "                prop = (subset.pred_classification == 1).sum() / subset.shape[0]\n",
    "            elif proportion == \"2 - Low Risk\":\n",
    "                prop = (subset.pred_classification == 2).sum() / subset.shape[0]\n",
    "            elif proportion == \"3 - Normal Risk\":\n",
    "                prop = (subset.pred_classification == 3).sum() / subset.shape[0]\n",
    "            elif proportion == \"4 - Elevated Risk\":\n",
    "                prop = (subset.pred_classification == 4).sum() / subset.shape[0]\n",
    "            else:\n",
    "                prop = (subset.pred_classification == 5).sum() / subset.shape[0]\n",
    "\n",
    "            risks[proportion][this_risk - 1] = prop\n",
    "\n",
    "    bottom = np.array([0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "    for proportion in risks:\n",
    "        if proportion == \"1 - Very Low Risk\":\n",
    "            data = risks[proportion]\n",
    "            axs[i, 0].bar(risk_labels, data, label=proportion, width=0.35)\n",
    "        else:\n",
    "            bottom += data\n",
    "            data = risks[proportion]\n",
    "            axs[i, 0].bar(\n",
    "                risk_labels, data, label=proportion, bottom=bottom, width=0.35\n",
    "            )\n",
    "    # skip legend as will be same on RHS\n",
    "    # handles, labels = axs[i, 0].get_legend_handles_labels()\n",
    "    # axs[i, 0].legend(handles[::-1], labels[::-1], bbox_to_anchor=(1.05, 1))\n",
    "    axs[i, 0].set_xlabel(\"Actual risk\")\n",
    "    axs[i, 0].set_ylabel(\"Predicted risk\")\n",
    "    axs[i, 0].set_title(f\"classification: {model_classification}\")\n",
    "\n",
    "    # plot actual vs predicted CLASSES for regression\n",
    "    risks = dict.fromkeys(risk_labels)\n",
    "    for proportion in risks:\n",
    "        risks[proportion] = np.array([0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "\n",
    "        for label in risk_labels:\n",
    "            this_risk = int(label[0])\n",
    "\n",
    "            # extract the real risk\n",
    "            subset = predictions_df[predictions_df.risk == this_risk]\n",
    "\n",
    "            if proportion == \"1 - Very Low Risk\":\n",
    "                prop = (subset.pred_regression == 1).sum() / subset.shape[0]\n",
    "            elif proportion == \"2 - Low Risk\":\n",
    "                prop = (subset.pred_regression == 2).sum() / subset.shape[0]\n",
    "            elif proportion == \"3 - Normal Risk\":\n",
    "                prop = (subset.pred_regression == 3).sum() / subset.shape[0]\n",
    "            elif proportion == \"4 - Elevated Risk\":\n",
    "                prop = (subset.pred_regression == 4).sum() / subset.shape[0]\n",
    "            else:\n",
    "                prop = (subset.pred_regression == 5).sum() / subset.shape[0]\n",
    "\n",
    "            risks[proportion][this_risk - 1] = prop\n",
    "\n",
    "    bottom = np.array([0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "    for proportion in risks:\n",
    "        if proportion == \"1 - Very Low Risk\":\n",
    "            data = risks[proportion]\n",
    "            axs[i, 1].bar(risk_labels, data, label=proportion, width=0.35)\n",
    "        else:\n",
    "            bottom += data\n",
    "            data = risks[proportion]\n",
    "            axs[i, 1].bar(\n",
    "                risk_labels, data, label=proportion, bottom=bottom, width=0.35\n",
    "            )\n",
    "    handles, labels = axs[i, 1].get_legend_handles_labels()\n",
    "    axs[i, 1].legend(handles[::-1], labels[::-1], bbox_to_anchor=(1.05, 1))\n",
    "    axs[i, 1].set_xlabel(\"Actual risk\")\n",
    "    axs[i, 1].set_ylabel(\"Predicted risk\")\n",
    "    axs[i, 1].set_title(f\"regression: {model_regression}\")\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Extensions\n",
    "\n",
    "* Refactor visualisation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
