{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 Data Loading\n",
    "\n",
    "Notebook goal: load in data and perform basic data quality checks:\n",
    "\n",
    "* check row and column count aligns with data warehouse version\n",
    "* check column names match previous work\n",
    "\n",
    "Inputs|Outputs\n",
    "---|---\n",
    "`raw/original-data.csv`|`raw/original-data.parquet`\n",
    "\n",
    "## 1. Load Azure ML dataset into pandas dataframe\n",
    "\n",
    "The directory structure of this project includes **data stored outside of the git tree**. This is to ensure that, when coding in the open, no data can accidentally be committed to the repository through either the use of `git push -f` to override a `.gitignore` file, or through ignoring the `pre-commit` hooks.\n",
    "\n",
    "A `project-directory` must first be created, inside of which this repository can be cloned (into e.g. `repo-directory`).\n",
    "\n",
    "`data` and `models` folders will be stored at the highest level, outside the git tree, and must be created manually first, including the subdirectories data/interim, data/processed and data/raw:\n",
    "\n",
    "```\n",
    "project-directory\n",
    "├── repo-directory\n",
    "│   ├── .git\n",
    "│   ├── .github\n",
    "│   ├── config\n",
    "│   ├── docs\n",
    "│   ├── notebooks\n",
    "│   └── src\n",
    "├── data\n",
    "│   ├── interim\n",
    "│   ├── processed\n",
    "│   └── raw\n",
    "└── models\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1650363642475
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# azureml-core of version 1.0.72 or higher is required\n",
    "# azureml-dataprep[pandas] of version 1.1.34 or higher is required\n",
    "from azureml.core import Dataset, Workspace\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# import azure ml environment variables\n",
    "load_dotenv(\"../config/.env\")\n",
    "\n",
    "for var in [\n",
    "    \"subscription_id\",\n",
    "    \"resource_group\",\n",
    "    \"workspace_name\",\n",
    "    \"dataset_name\",\n",
    "    \"datafile_name\",\n",
    "]:\n",
    "    if var not in os.environ:\n",
    "        raise NameError(f\"Please define {var} in ../config/.env\")\n",
    "\n",
    "subscription_id = os.environ.get(\"subscription_id\")\n",
    "resource_group = os.environ.get(\"resource_group\")\n",
    "workspace_name = os.environ.get(\"workspace_name\")\n",
    "dataset_name = os.environ.get(\"dataset_name\")\n",
    "datafile_name = os.environ.get(\"datafile_name\")\n",
    "\n",
    "workspace = Workspace(subscription_id, resource_group, workspace_name)\n",
    "\n",
    "dataset = Dataset.get_by_name(workspace, name=dataset_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1650363649738
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Convert tabular dataset to a pandas dataframe\n",
    "# df = dataset.to_pandas_dataframe()\n",
    "\n",
    "# Download file dataset locally, outside of git tree\n",
    "dataset.download(target_path=\"../../data/raw/\", overwrite=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1650363701550
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Load into pandas\n",
    "df = pd.read_csv(f\"../../data/raw/{datafile_name}\", low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Quality checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1650363701671
    }
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1650363702142
    }
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Check columns are the same as Github columns\n",
    "\n",
    "[Previous work](https://github.com/nhsx/skunkworks-long-stayer-risk-stratification) implemented a convolutional neural network to predict length of stay using the same data.\n",
    "\n",
    "Here we check we have the same data, as it was delivered under a separate DPA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1650363702252
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# From https://github.com/nhsx/skunkworks-long-stayer-risk-stratification/blob/main/training/README.md\n",
    "original_header = \"LENGTH_OF_STAY,LENGTH_OF_STAY_IN_MINUTES,ADMISSION_METHOD_HOSPITAL_PROVIDER_SPELL_DESCRIPTION,AGE_ON_ADMISSION,DISCHARGE_DATE_HOSPITAL_PROVIDER_SPELL,ETHNIC_CATEGORY_CODE_DESCRIPTION,DISCHARGE_READY_DATE,DIVISION_NAME_AT_ADMISSION,EXPECTED_DISCHARGE_DATE,EXPECTED_DISCHARGE_DATE_TIME,FIRST_REGULAR_DAY_OR_NIGHT_ADMISSION_DESCRIPTION,FIRST_START_DATE_TIME_WARD_STAY,FIRST_WARD_STAY_IDENTIFIER,IS_PATIENT_DEATH_DURING_SPELL,MAIN_SPECIALTY_CODE_AT_ADMISSION,MAIN_SPECIALTY_CODE_AT_ADMISSION_DESCRIPTION,PATIENT_CLASSIFICATION,PATIENT_CLASSIFICATION_DESCRIPTION,POST_CODE_AT_ADMISSION_DATE_DISTRICT,SOURCE_OF_ADMISSION_HOSPITAL_PROVIDER_SPELL,SOURCE_OF_ADMISSION_HOSPITAL_PROVIDER_SPELL_DESCRIPTION,START_DATE_HOSPITAL_PROVIDER_SPELL,START_DATE_TIME_HOSPITAL_PROVIDER_SPELL,TREATMENT_FUNCTION_CODE_AT_ADMISSION,TREATMENT_FUNCTION_CODE_AT_ADMISSION_DESCRIPTION,elective_or_non_elective,stroke_ward_stay,PATIENT_GENDER_CURRENT,PATIENT_GENDER_CURRENT_DESCRIPTION,LOCAL_PATIENT_IDENTIFIER,SpellDominantProcedure,all_diagnoses,cds_unique_identifier,previous_30_day_hospital_provider_spell_number,ED_attendance_episode_number,unique_internal_ED_admission_number,unique_internal_IP_admission_number,reason_for_admission,IS_care_home_on_admission,IS_care_home_on_discharge,ae_attendance_category,ae_arrival_mode,ae_attendance_disposal,ae_attendance_category_code,healthcare_resource_group_code,presenting_complaint_code,presenting_complaint,wait,wait_minutes,all_investigation_codes,all_diagnosis_codes,all_treatment_codes,all_breach_reason_codes,all_location_codes,all_investigations,all_diagnosis,all_treatments,all_local_investigation_codes,all_local_investigations,all_local_treatment_codes,all_local_treatments,attendance_type,initial_wait,initial_wait_minutes,major_minor,IS_major,ae_patient_group_code,ae_patient_group,ae_initial_assessment_triage_category_code,ae_initial_assessment_triage_category,manchester_triage_category,arrival_day_of_week,arrival_month_name,Illness Injury Flag,Mental Health Flag,Frailty Proxy,Presenting Complaint Group,IS_cancer,cancer_type,IS_chronic_kidney_disease,IS_COPD,IS_coronary_heart_disease,IS_dementia,IS_diabetes,diabetes_type,IS_frailty_proxy,IS_hypertension,IS_mental_health,IMD county decile,District,Rural urban classification,OAC Group Name,OAC Subgroup Name,OAC Supergroup Name,EMCountLast12m,EL CountLast12m,ED CountLast12m,OP First CountLast12m,OP FU CountLast12m\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1650363702352
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "original_headers = original_header.split(\",\")\n",
    "\n",
    "for col in df.columns:\n",
    "    if col not in original_headers:\n",
    "        print(f\"{col} additional to original columns\")\n",
    "\n",
    "for col in original_headers:\n",
    "    if col not in df.columns:\n",
    "        print(f\"{col} not found\")\n",
    "\n",
    "print(df.columns.size)\n",
    "print(len(original_headers))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Save to parquet\n",
    "\n",
    "More performant data format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1650363715155
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# nb. this is outside the git tree\n",
    "df.to_parquet(\"../../data/raw/original-data.parquet\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6d65a8c07f5b6469e0fc613f182488c0dccce05038bbda39e5ac9075c0454d11"
  },
  "kernel_info": {
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
