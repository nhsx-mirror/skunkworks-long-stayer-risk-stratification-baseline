{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling - Classification\n",
    "\n",
    "This notebook explores classification models to **predict risk of becoming a long stayer** as a baseline to the [Long Stayer Risk Stratification](https://github.com/nhsx/skunkworks-long-stayer-risk-stratification) model.\n",
    "\n",
    "This notebook is broken down into:\n",
    "\n",
    "1. Converting the length of stay into a relative risk\n",
    "1. Training a range of baseline models using cross validation\n",
    "3. Testing final models on a test dataset\n",
    "4. Exploring in more detail the best performing baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1653643592936
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import balanced_accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV, cross_validate, train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = [15, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1653643561513
    }
   },
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "\n",
    "def train_model(gsc, X_train, y_train):\n",
    "    \"\"\"Uses a GridSearchCV instance to find a reasonable model, and store\n",
    "    performance and fitted model into a python dict\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "        gsc (sklearn.model_selection.GridSearchCV object): defined model\n",
    "        X_train (pandas dataframe): training dataframe with features\n",
    "        y_train (pandas dataframe): training dataframe with targets\n",
    "\n",
    "    Returns:\n",
    "\n",
    "        (dict): resulting fitted model and performance metrics\n",
    "    \"\"\"\n",
    "\n",
    "    grid_result = gsc.fit(X_train, y_train)\n",
    "\n",
    "    model = {\n",
    "        \"cv_metric_mean\": np.round(\n",
    "            grid_result.cv_results_[\"mean_test_score\"][grid_result.best_index_], 3\n",
    "        ),\n",
    "        \"cv_metric_std\": np.round(\n",
    "            grid_result.cv_results_[\"std_test_score\"][grid_result.best_index_], 2\n",
    "        ),\n",
    "        \"model\": grid_result.best_estimator_,\n",
    "    }\n",
    "\n",
    "    # retrain the best estimator on the full training set - note that refit=True does not appear to do this\n",
    "    # note we specify which metric to use for final evaluation here - this may be same/different to metric\n",
    "    # defined in the gsc object above\n",
    "    model[\"model\"].fit(X_train, y_train)\n",
    "    model[\"f1_weighted\"] = np.round(\n",
    "        f1_score(y_train, model[\"model\"].predict(X_train), average=\"weighted\"), 3\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def risk_score(los):\n",
    "    \"\"\"Return risk score (1-5) based on LoS\n",
    "\n",
    "    Parameters:\n",
    "        los (float): length of stay in days\n",
    "\n",
    "    Returns:\n",
    "        (int): risk score (1 = Very low risk, 5 = High risk)\n",
    "    \"\"\"\n",
    "\n",
    "    # round los up to whole days\n",
    "    los = math.ceil(los)\n",
    "\n",
    "    if los > 15:\n",
    "        return 5\n",
    "    elif los > 13:\n",
    "        return 4\n",
    "    elif los > 10:\n",
    "        return 3\n",
    "    elif los > 6:\n",
    "        return 2\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1653643561819
    }
   },
   "outputs": [],
   "source": [
    "features_df = pd.read_parquet(\"../../data/features.parquet\")\n",
    "features_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1653643562172
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "features_catboost_df = pd.read_parquet(\"../../data/features-catboost.parquet\")\n",
    "features_catboost_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate risk scores\n",
    "\n",
    "We will convert actual Length of Stay (days) into a risk score defined as:\n",
    "\n",
    "Risk Category|Day Range for Risk Category\n",
    "-----|------\n",
    "1 - Very low risk|0-6\n",
    "2 - Low risk|7-10\n",
    "3 - Normal risk|11-13\n",
    "4 - Elevated risk|14-15\n",
    "5 - High risk|>15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1653643562368
    }
   },
   "outputs": [],
   "source": [
    "# actual risk scores\n",
    "risk_labels = [\n",
    "    \"1 - Very Low Risk\",\n",
    "    \"2 - Low Risk\",\n",
    "    \"3 - Normal Risk\",\n",
    "    \"4 - Elevated Risk\",\n",
    "    \"5 - High Risk\",\n",
    "]\n",
    "features_df[\"risk\"] = [risk_score(los) for los in features_df.LENGTH_OF_STAY]\n",
    "features_catboost_df[\"risk\"] = [\n",
    "    risk_score(los) for los in features_catboost_df.LENGTH_OF_STAY\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define target and training features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1653643562553
    }
   },
   "outputs": [],
   "source": [
    "X = features_df.drop(columns=[\"LENGTH_OF_STAY\"])\n",
    "y = features_df.risk\n",
    "\n",
    "# Non-one-hot encoded data for catboost\n",
    "X_catboost = features_catboost_df.drop(columns=[\"LENGTH_OF_STAY\"])\n",
    "y_catboost = features_catboost_df.risk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/test split\n",
    "\n",
    "For model evaluation, we will hold back a 25% test set, and use cross-validation on the remaining 75% for all models until the final comparison is made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1653643562781
    }
   },
   "outputs": [],
   "source": [
    "# Split data for train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, train_size=0.75, random_state=42\n",
    ")\n",
    "print(X_train.shape, X_test.shape)\n",
    "\n",
    "# Scale data for LogReg only using training data\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train_scaled = pd.DataFrame(\n",
    "    scaler.transform(X_train), index=X_train.index, columns=X_train.columns\n",
    ")\n",
    "X_test_scaled = pd.DataFrame(\n",
    "    scaler.transform(X_test), index=X_test.index, columns=X_test.columns\n",
    ")\n",
    "print(X_train_scaled.shape, X_test_scaled.shape)\n",
    "\n",
    "# Split data for train/test\n",
    "X_train_catboost, X_test_catboost, y_train_catboost, y_test_catboost = train_test_split(\n",
    "    X_catboost, y_catboost, train_size=0.75, random_state=42\n",
    ")\n",
    "print(X_train_catboost.shape, X_test_catboost.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore class imbalance\n",
    "\n",
    "This is a multi-class classification model, so we need to understand where any class imbalance lies otherwise we will skew to larger classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1653643562981
    }
   },
   "outputs": [],
   "source": [
    "# Show how many of each class are present in the training set:\n",
    "X_train.risk.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight samples\n",
    "\n",
    "Given significant class imbalance, we will weight samples where the smallest class has a weight of 1, and other classes as a (<1) proportion. This is called _balanced_ weighting as can be automatically calculated for many algorithms, but not all (e.g. XGBoost).\n",
    "\n",
    "e.g.\n",
    "\n",
    "class|count|weight\n",
    "---|---|---\n",
    "1|1000|0.1\n",
    "2|100|1\n",
    "3|500|0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1653643563692
    }
   },
   "outputs": [],
   "source": [
    "# note these are same for one-hot encoded and unencoded dataframes\n",
    "# note this is same as compute_class_weight/(max weight from compute_class_weight)\n",
    "weights = X_train.risk.value_counts().min() / X_train.risk.value_counts().sort_index()\n",
    "sample_weights = [weights[risk] for risk in X_train[\"risk\"].values]\n",
    "\n",
    "## Drop risk from training/test set\n",
    "X_train.drop(columns=\"risk\", inplace=True)\n",
    "X_test.drop(columns=\"risk\", inplace=True)\n",
    "\n",
    "X_train_scaled.drop(columns=\"risk\", inplace=True)\n",
    "X_test_scaled.drop(columns=\"risk\", inplace=True)\n",
    "\n",
    "X_train_catboost.drop(columns=\"risk\", inplace=True)\n",
    "X_test_catboost.drop(columns=\"risk\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "Strategy is to try a number of classification models with:\n",
    "\n",
    "* GridsearchCV framework, although default parameters will be used for baselining\n",
    "* Refitting full training set for best model\n",
    "* Test all final models against the held-out test set\n",
    "* Explore feature importance of best performing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1653468045907
    }
   },
   "outputs": [],
   "source": [
    "# Initiate empty models dictionary\n",
    "models = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior model\n",
    "\n",
    "The simplest baseline model takes the most frequent class label as its prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1653468088844
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"prior\"\n",
    "\n",
    "# define gridsearch parameters\n",
    "gsc = GridSearchCV(\n",
    "    estimator=DummyClassifier(strategy=\"prior\"),\n",
    "    param_grid={},\n",
    "    cv=5,\n",
    "    scoring=\"f1_weighted\",\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    refit=True,\n",
    ")\n",
    "\n",
    "# takes ~1 second to run on a STANDARD_D13_V2\n",
    "models[model_name] = train_model(gsc, X_train, y_train)\n",
    "models[model_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Logistic Regression (elastic net regularisation)\n",
    "\n",
    "Multi-class balanced and regularised (l1/l2 ratio of 0.5) logistic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1653315115111
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"elastic\"\n",
    "\n",
    "# define gridsearch parameters\n",
    "gsc = GridSearchCV(\n",
    "    estimator=LogisticRegression(\n",
    "        class_weight=\"balanced\", penalty=\"elasticnet\", solver=\"saga\", l1_ratio=0.5\n",
    "    ),\n",
    "    param_grid={},\n",
    "    cv=5,\n",
    "    scoring=\"f1_weighted\",\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    refit=True,\n",
    ")\n",
    "\n",
    "# takes ~4 minutes to run on a STANDARD_D13_V2\n",
    "models[model_name] = train_model(gsc, X_train_scaled, y_train)\n",
    "models[model_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree classifier (weighted)\n",
    "\n",
    "Simplest tree classifier using one tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1653315136126
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"decisiontree\"\n",
    "\n",
    "# define gridsearch parameters\n",
    "gsc = GridSearchCV(\n",
    "    estimator=DecisionTreeClassifier(class_weight=\"balanced\"),\n",
    "    param_grid={},\n",
    "    cv=5,\n",
    "    scoring=\"f1_weighted\",\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    refit=True,\n",
    ")\n",
    "\n",
    "# takes ~5s to run on a STANDARD_D13_V2\n",
    "models[model_name] = train_model(gsc, X_train, y_train)\n",
    "models[model_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest (weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1653315230236
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"randomforest\"\n",
    "\n",
    "gsc = GridSearchCV(\n",
    "    estimator=RandomForestClassifier(class_weight=\"balanced\"),\n",
    "    param_grid={},\n",
    "    cv=5,\n",
    "    scoring=\"f1_weighted\",\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    refit=True,\n",
    ")\n",
    "\n",
    "# takes ~1 mins to run on a STANDARD_D13_V2\n",
    "models[model_name] = train_model(gsc, X_train, y_train)\n",
    "models[model_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catboost\n",
    "\n",
    "Boosted tree optimised for categorical features. Note this requires **non-one-hot encoded features**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1653316502289
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"catboost\"\n",
    "\n",
    "# extract categorical features\n",
    "num_features = [\n",
    "    \"AGE_ON_ADMISSION\",\n",
    "    \"EL CountLast12m\",\n",
    "    \"EMCountLast12m\",\n",
    "    \"OP First CountLast12m\",\n",
    "    \"OP FU CountLast12m\",\n",
    "]\n",
    "cat_features = list(set(X_train_catboost.columns) - set(num_features))\n",
    "\n",
    "gsc = GridSearchCV(\n",
    "    estimator=CatBoostClassifier(\n",
    "        verbose=False,\n",
    "        auto_class_weights=\"Balanced\",\n",
    "        cat_features=cat_features,\n",
    "    ),\n",
    "    param_grid={},\n",
    "    cv=5,\n",
    "    scoring=\"f1_weighted\",\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    refit=True,\n",
    ")\n",
    "\n",
    "# takes ~18 mins to run on a STANDARD_D13_V2\n",
    "models[model_name] = train_model(gsc, X_train_catboost, y_train_catboost)\n",
    "models[model_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1653644170743
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"xgboost\"\n",
    "\n",
    "# note GridSearchCV does not currently pass in sample weights to XGBoost\n",
    "# implement sklearn cross_validate instead\n",
    "# see https://discuss.xgboost.ai/t/multi-class-classification-weighting-for-unbalanced-datasets/2789\n",
    "\n",
    "clf = XGBClassifier(random_state=42)\n",
    "cv_results = cross_validate(\n",
    "    clf,\n",
    "    X_train,\n",
    "    y_train,\n",
    "    scoring=\"f1_weighted\",\n",
    "    fit_params={\"sample_weight\": sample_weights},\n",
    ")\n",
    "\n",
    "models[model_name][\"cv_metric_mean\"] = np.round(cv_results[\"test_score\"].mean(), 3)\n",
    "models[model_name][\"cv_metric_std\"] = np.round(cv_results[\"test_score\"].std(), 3)\n",
    "\n",
    "# re-train across the full training set to calculate final training metric\n",
    "clf.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "preds = clf.predict(X_train)\n",
    "models[model_name][\"f1_weighted\"] = np.round(\n",
    "    f1_score(y_train, preds, average=\"weighted\"), 3\n",
    ")\n",
    "\n",
    "# store model\n",
    "models[model_name][\"model\"] = clf\n",
    "models[model_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1653470972392
    }
   },
   "outputs": [],
   "source": [
    "# save models outside the git tree\n",
    "with open(\"../../models/classification.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(models, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1653643585740
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# load models from outside the git tree\n",
    "with open(\"../../models/classification.pickle\", \"rb\") as handle:\n",
    "    models = pickle.load(handle)\n",
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate models\n",
    "\n",
    "Use the held-out test set to evaluate and visualise the performance of all the tuned models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1653644178778
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# setup a subplot figure\n",
    "fig, axs = plt.subplots(len(models), 2)\n",
    "fig.set_size_inches(15, 7 * len(models))\n",
    "\n",
    "i = 0\n",
    "\n",
    "for model in models:\n",
    "    if model == \"catboost\":\n",
    "        model_X_test = X_test_catboost\n",
    "        model_y_test = y_test\n",
    "    elif model == \"elastic\":\n",
    "        model_X_test = X_test_scaled\n",
    "        model_y_test = y_test\n",
    "    else:\n",
    "        model_X_test = X_test\n",
    "        model_y_test = y_test\n",
    "\n",
    "    # perform inference\n",
    "    preds = models[model][\"model\"].predict(model_X_test)\n",
    "    probs = models[model][\"model\"].predict_proba(model_X_test)\n",
    "\n",
    "    # calculate performance metrics\n",
    "    balanced_accuracy = balanced_accuracy_score(model_y_test, preds)\n",
    "    f1_score_weighted = f1_score(model_y_test, preds, average=\"weighted\")\n",
    "    auc = roc_auc_score(\n",
    "        model_y_test, probs, multi_class=\"ovr\", average=\"weighted\"\n",
    "    )  # one-vs-rest\n",
    "\n",
    "    # output metrics\n",
    "    print(\n",
    "        f\"{model} test balanced accuracy: {balanced_accuracy.round(3)}, f1 score (weighted): {f1_score_weighted.round(3)}, auc (ovr, weighted): {auc.round(3)}\"\n",
    "    )\n",
    "\n",
    "    # create a prediction dataframe\n",
    "    predictions_df = pd.DataFrame(data=model_y_test.reset_index(drop=True))\n",
    "    predictions_df[\"pred\"] = preds\n",
    "\n",
    "    # plot actual vs predicted COUNTS\n",
    "    axs[i, 0].hist([predictions_df.risk, predictions_df.pred])\n",
    "    axs[i, 0].legend([\"Actual risk\", \"Predicted risk\"])\n",
    "    axs[i, 0].set_title(f\"{model} - f1 weighted: {f1_score_weighted.round(2)}\")\n",
    "    axs[i, 0].set_xticks([1, 2, 3, 4, 5], labels=risk_labels, minor=False)\n",
    "    axs[i, 0].set_ylabel(\"Count of risk\")\n",
    "\n",
    "    # plot predicted vs actual CLASSES\n",
    "    risks = dict.fromkeys(risk_labels)\n",
    "    for proportion in risks:\n",
    "        risks[proportion] = np.array([0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "\n",
    "        for label in risk_labels:\n",
    "            this_risk = int(label[0])\n",
    "\n",
    "            # extract the predicted risk\n",
    "            subset = predictions_df[predictions_df.pred == this_risk]\n",
    "\n",
    "            if proportion == \"1 - Very Low Risk\":\n",
    "                count = (subset.risk == 1).sum()\n",
    "            elif proportion == \"2 - Low Risk\":\n",
    "                count = (subset.risk == 2).sum()\n",
    "            elif proportion == \"3 - Normal Risk\":\n",
    "                count = (subset.risk == 3).sum()\n",
    "            elif proportion == \"4 - Elevated Risk\":\n",
    "                count = (subset.risk == 4).sum()\n",
    "            else:\n",
    "                count = (subset.risk == 5).sum()\n",
    "\n",
    "            prop = 0 if count == 0 else count / subset.shape[0]\n",
    "\n",
    "            risks[proportion][this_risk - 1] = prop\n",
    "\n",
    "    bottom = np.array([0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "    for proportion in risks:\n",
    "        if proportion == \"1 - Very Low Risk\":\n",
    "            data = risks[proportion]\n",
    "            axs[i, 1].bar(risk_labels, data, label=proportion, width=0.35)\n",
    "        else:\n",
    "            bottom += data\n",
    "            data = risks[proportion]\n",
    "            axs[i, 1].bar(\n",
    "                risk_labels, data, label=proportion, bottom=bottom, width=0.35\n",
    "            )\n",
    "    handles, labels = axs[i, 1].get_legend_handles_labels()\n",
    "    axs[i, 1].legend(handles[::-1], labels[::-1], bbox_to_anchor=(1.05, 1))\n",
    "    axs[i, 1].set_xlabel(\"Predicted risk\")\n",
    "    axs[i, 1].set_ylabel(\"Actual risk proportion\")\n",
    "    axs[i, 1].set_title(f\"{model} - f1 weighted: {f1_score_weighted.round(2)}\")\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model exploration\n",
    "\n",
    "We will take the \"best performing\" baseline model, and explore in more detail how the model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1653471031535
    }
   },
   "outputs": [],
   "source": [
    "model = \"catboost\"\n",
    "if model == \"catboost\":\n",
    "    model_X_test = X_test_catboost\n",
    "    model_y_test = y_test\n",
    "elif model == \"elastic\":\n",
    "    model_X_test = X_test_scaled\n",
    "    model_y_test = y_test\n",
    "else:\n",
    "    model_X_test = X_test\n",
    "    model_y_test = y_test\n",
    "\n",
    "# generate predictions\n",
    "predictions_df = pd.DataFrame(data=model_y_test.reset_index(drop=True))\n",
    "predictions_df[\"pred\"] = models[model][\"model\"].predict(model_X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Severity of misclassification\n",
    "\n",
    "When the model incorrectly predicts a class, how badly does it do this?\n",
    "\n",
    "Because risk categories are numerical (1-5), we can calculate the difference between them as the number of classes incorrect the prediction was."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1653318178012
    }
   },
   "outputs": [],
   "source": [
    "predictions_df[\"diff\"] = predictions_df.diff().risk\n",
    "fig = px.histogram(predictions_df, x=\"diff\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Feature importance\n",
    "\n",
    "Which features does the model ascribe predictive power to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1653318184887
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Feature names\n",
    "coef = pd.DataFrame(data=list(model_X_test.columns))\n",
    "# Feature importances, sorted\n",
    "coef[\"coef\"] = models[model][\"model\"].feature_importances_\n",
    "coef.sort_values(\"coef\", ascending=False, inplace=True)\n",
    "coef.set_index(0, inplace=True)\n",
    "# Plot interactive plot\n",
    "# Hover over a feature for full feature name\n",
    "fig = px.bar(coef, x=coef.index, y=\"coef\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions\n",
    "\n",
    "- Tune hyperparameters using existing gridsearch framework\n",
    "- Fairness analysis\n",
    "- Analysis of distribution of probabilities e.g. `predict_proba` to see how changes to threshold affect performance\n",
    "- Plot PR curves per class as per https://stackoverflow.com/questions/56090541/how-to-plot-precision-and-recall-of-multiclass-classifier\n",
    "- Train a binary classifier on Long Stay (21+ days) or not, use it as a precursor to two different regression models (one for long stayer, one for not)\n",
    "- Include IS_MINOR data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6d65a8c07f5b6469e0fc613f182488c0dccce05038bbda39e5ac9075c0454d11"
  },
  "kernel_info": {
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
