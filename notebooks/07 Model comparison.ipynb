{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Model comparison\n",
    "\n",
    "We have now trained a series of basic Regression models (length of stay) and Classification models (risk 1-5).\n",
    "\n",
    "We can create an equivalent risk model from the risk categories using the predicted Length of Stay:\n",
    "\n",
    "Risk Category|Day Range for Risk Category\n",
    "-----|------\n",
    "1 - Very low risk|0-6\n",
    "2 - Low risk|7-10\n",
    "3 - Normal risk|11-13\n",
    "4 - Elevated risk|14-15\n",
    "5 - High risk|>15\n",
    "\n",
    "We have a number of approaches where we will compare side by side plots for risk stratification:\n",
    "\n",
    "Model|Regression version|Classification version\n",
    "---|---|---\n",
    "Dummy|Mean|Prior\n",
    "ElasticNet|ElasticNet|LogisticRegression\n",
    "Decision Tree|DecisionTreeRegressor|DecisionTreeClassifier\n",
    "Random Forest|RandomForestRegressor|RandomForestClassifier\n",
    "Catboost|CatBoostRegressor|CatBoostClassifier\n",
    "XGBoost|XGBRegressor|XGBClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1657787374141
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import f1_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sys.path.append(\"../src/\")\n",
    "\n",
    "from utils import risk_score\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = [15, 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1657787159118
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "features_df = pd.read_parquet(\"../../data/processed/features.parquet\")\n",
    "features_catboost_df = pd.read_parquet(\"../../data/processed/features-catboost.parquet\")\n",
    "\n",
    "# add actual risk scores\n",
    "risk_labels = [\n",
    "    \"1 - Very Low Risk\",\n",
    "    \"2 - Low Risk\",\n",
    "    \"3 - Normal Risk\",\n",
    "    \"4 - Elevated Risk\",\n",
    "    \"5 - High Risk\",\n",
    "]\n",
    "features_df[\"risk\"] = [risk_score(los) for los in features_df.LENGTH_OF_STAY]\n",
    "features_catboost_df[\"risk\"] = [\n",
    "    risk_score(los) for los in features_catboost_df.LENGTH_OF_STAY\n",
    "]\n",
    "# separate training and target features\n",
    "X = features_df.drop(columns=[\"LENGTH_OF_STAY\", \"risk\"])\n",
    "y_reg = features_df[\"LENGTH_OF_STAY\"]\n",
    "y_clf = features_df[\"risk\"]\n",
    "\n",
    "# non-one-hot encoded data for catboost\n",
    "X_catboost = features_catboost_df.drop(columns=[\"LENGTH_OF_STAY\"])\n",
    "y_catboost_reg = features_catboost_df[\"LENGTH_OF_STAY\"]\n",
    "y_catboost_clf = features_catboost_df[\"risk\"]\n",
    "\n",
    "# separate training and test data\n",
    "# split data for train/test\n",
    "X_train, X_test, y_train_reg, y_test_reg, y_train_clf, y_test_clf = train_test_split(\n",
    "    X, y_reg, y_clf, train_size=0.75, random_state=42\n",
    ")\n",
    "print(\n",
    "    X_train.shape,\n",
    "    X_test.shape,\n",
    "    y_train_reg.shape,\n",
    "    y_train_clf.shape,\n",
    "    y_test_reg.shape,\n",
    "    y_test_clf.shape,\n",
    ")\n",
    "\n",
    "# Scale data for LogReg only using training data\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train_scaled = pd.DataFrame(\n",
    "    scaler.transform(X_train), index=X_train.index, columns=X_train.columns\n",
    ")\n",
    "X_test_scaled = pd.DataFrame(\n",
    "    scaler.transform(X_test), index=X_test.index, columns=X_test.columns\n",
    ")\n",
    "print(X_train_scaled.shape, X_test_scaled.shape)\n",
    "\n",
    "# Split data for train/test\n",
    "(\n",
    "    X_train_catboost,\n",
    "    X_test_catboost,\n",
    "    y_train_catboost_reg,\n",
    "    y_test_catboost_reg,\n",
    "    y_train_catboost_clf,\n",
    "    y_test_catboost_clf,\n",
    ") = train_test_split(\n",
    "    X_catboost, y_catboost_reg, y_catboost_clf, train_size=0.75, random_state=42\n",
    ")\n",
    "print(\n",
    "    X_train_catboost.shape,\n",
    "    X_test_catboost.shape,\n",
    "    y_train_catboost_reg.shape,\n",
    "    y_train_catboost_clf.shape,\n",
    "    y_test_catboost_reg.shape,\n",
    "    y_test_catboost_clf.shape,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1657787176134
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# load models from outside the git tree\n",
    "with open(\"../../models/regression.pickle\", \"rb\") as handle:\n",
    "    models_regression = pickle.load(handle)\n",
    "# remove the final model as we are comparing all baseline models\n",
    "models_regression.pop(\"final_model\")\n",
    "models_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1657787194144
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# load models from outside the git tree\n",
    "with open(\"../../models/classification.pickle\", \"rb\") as handle:\n",
    "    models_classification = pickle.load(handle)\n",
    "\n",
    "# remove the final model as we are comparing all baseline models\n",
    "models_classification.pop(\"final_model\")\n",
    "models_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Compare predicted risk score (classification) with equivalent-predicted risk score (regression)\n",
    "\n",
    "Classification -> Risk score\n",
    "\n",
    "Regression -> Length Of Stay -> Equivalent risk score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1657787400168
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# setup a subplot figure\n",
    "fig, axs = plt.subplots(len(models_classification), 2)\n",
    "fig.set_size_inches(15, 7 * len(models_classification))\n",
    "\n",
    "i = 0\n",
    "\n",
    "for model_classification in models_classification:\n",
    "    # specify which test sets to use for each classification model\n",
    "    if model_classification == \"catboost\":\n",
    "        # catboost has non one-hot encoded features\n",
    "        model_classification_X_test = X_test_catboost\n",
    "        model_classification_y_test = y_test_catboost_clf\n",
    "    elif model_classification == \"elastic\":\n",
    "        # elastic logreg has normalised features\n",
    "        model_classification_X_test = X_test_scaled\n",
    "        model_classification_y_test = y_test_clf\n",
    "    else:\n",
    "        model_classification_X_test = X_test\n",
    "        model_classification_y_test = y_test_clf\n",
    "\n",
    "    # specify which test sets to use for each corresponding regression model\n",
    "    if model_classification == \"catboost\":\n",
    "        model_regression_X_test = X_test_catboost\n",
    "        model_regression_y_test = y_test_catboost_reg\n",
    "    # elasticnet is scaled for classification (LogisticRegression), but not for regression (ElasticNet)\n",
    "    elif model_classification == \"elastic\":\n",
    "        model_regression_X_test = X_test\n",
    "        model_regression_y_test = y_test_reg\n",
    "    else:\n",
    "        model_regression_X_test = X_test\n",
    "        model_regression_y_test = y_test_reg\n",
    "\n",
    "    # prior is being compared to mean\n",
    "    if model_classification == \"prior\":\n",
    "        model_regression = \"mean\"\n",
    "    else:\n",
    "        model_regression = model_classification\n",
    "\n",
    "    # perform inference\n",
    "    preds_regression = np.clip(\n",
    "        models_regression[model_regression][\"model\"].predict(model_regression_X_test),\n",
    "        0,\n",
    "        None,\n",
    "    )\n",
    "    preds_classification = models_classification[model_classification][\"model\"].predict(\n",
    "        model_classification_X_test\n",
    "    )\n",
    "\n",
    "    # calculate performance metrics\n",
    "    rmse = mean_squared_error(model_regression_y_test, preds_regression, squared=False)\n",
    "    mae = mean_absolute_error(model_regression_y_test, preds_regression)\n",
    "\n",
    "    f1_score_weighted = f1_score(\n",
    "        model_classification_y_test, preds_classification, average=\"weighted\"\n",
    "    )\n",
    "\n",
    "    # create a prediction dataframe\n",
    "    predictions_df = pd.DataFrame(\n",
    "        data=model_classification_y_test.reset_index(drop=True)\n",
    "    )\n",
    "    predictions_df[\"pred_regression_los\"] = preds_regression\n",
    "    # calculate equivalent risk score from regression model\n",
    "    predictions_df[\"pred_regression\"] = [\n",
    "        risk_score(los) for los in predictions_df.pred_regression_los\n",
    "    ]\n",
    "    predictions_df[\"pred_classification\"] = preds_classification\n",
    "\n",
    "    #### Predicted vs Actual ####\n",
    "\n",
    "    # plot predicted vs actual CLASSES for classification\n",
    "    risks = dict.fromkeys(risk_labels)\n",
    "    for proportion in risks:\n",
    "        risks[proportion] = np.array([0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "\n",
    "        for label in risk_labels:\n",
    "            this_risk = int(label[0])\n",
    "\n",
    "            # extract the predicted risk\n",
    "            subset = predictions_df[predictions_df.pred_classification == this_risk]\n",
    "\n",
    "            if proportion == \"1 - Very Low Risk\":\n",
    "                count = (subset.risk == 1).sum()\n",
    "            elif proportion == \"2 - Low Risk\":\n",
    "                count = (subset.risk == 2).sum()\n",
    "            elif proportion == \"3 - Normal Risk\":\n",
    "                count = (subset.risk == 3).sum()\n",
    "            elif proportion == \"4 - Elevated Risk\":\n",
    "                count = (subset.risk == 4).sum()\n",
    "            else:\n",
    "                count = (subset.risk == 5).sum()\n",
    "\n",
    "            prop = 0 if count == 0 else count / subset.shape[0]\n",
    "\n",
    "            risks[proportion][this_risk - 1] = prop\n",
    "\n",
    "    bottom = np.array([0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "    for proportion in risks:\n",
    "        if proportion == \"1 - Very Low Risk\":\n",
    "            data = risks[proportion]\n",
    "            axs[i, 0].bar(risk_labels, data, label=proportion, width=0.35)\n",
    "        else:\n",
    "            bottom += data\n",
    "            data = risks[proportion]\n",
    "            axs[i, 0].bar(\n",
    "                risk_labels, data, label=proportion, bottom=bottom, width=0.35\n",
    "            )\n",
    "\n",
    "    axs[i, 0].set_xlabel(\"Predicted risk\")\n",
    "    axs[i, 0].set_ylabel(\"Actual risk proportion\")\n",
    "    axs[i, 0].set_title(\n",
    "        f\"classification: {model_classification} - f1 weighted: {f1_score_weighted.round(2)}\"\n",
    "    )\n",
    "\n",
    "    # plot actual vs predicted CLASSES for regression\n",
    "    risks = dict.fromkeys(risk_labels)\n",
    "    for proportion in risks:\n",
    "        risks[proportion] = np.array([0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "\n",
    "        for label in risk_labels:\n",
    "            this_risk = int(label[0])\n",
    "\n",
    "            # extract the predicted risk\n",
    "            subset = predictions_df[predictions_df.pred_regression == this_risk]\n",
    "\n",
    "            if proportion == \"1 - Very Low Risk\":\n",
    "                count = (subset.risk == 1).sum()\n",
    "            elif proportion == \"2 - Low Risk\":\n",
    "                count = (subset.risk == 2).sum()\n",
    "            elif proportion == \"3 - Normal Risk\":\n",
    "                count = (subset.risk == 3).sum()\n",
    "            elif proportion == \"4 - Elevated Risk\":\n",
    "                count = (subset.risk == 4).sum()\n",
    "            else:\n",
    "                count = (subset.risk == 5).sum()\n",
    "\n",
    "            prop = 0 if count == 0 else count / subset.shape[0]\n",
    "\n",
    "            risks[proportion][this_risk - 1] = prop\n",
    "\n",
    "    bottom = np.array([0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "    for proportion in risks:\n",
    "        if proportion == \"1 - Very Low Risk\":\n",
    "            data = risks[proportion]\n",
    "            axs[i, 1].bar(risk_labels, data, label=proportion, width=0.35)\n",
    "        else:\n",
    "            bottom += data\n",
    "            data = risks[proportion]\n",
    "            axs[i, 1].bar(\n",
    "                risk_labels, data, label=proportion, bottom=bottom, width=0.35\n",
    "            )\n",
    "    handles, labels = axs[i, 1].get_legend_handles_labels()\n",
    "    axs[i, 1].legend(handles[::-1], labels[::-1], bbox_to_anchor=(1.05, 1))\n",
    "    axs[i, 1].set_xlabel(\"Predicted risk\")\n",
    "    axs[i, 1].set_ylabel(\"Actual risk proportion\")\n",
    "    axs[i, 1].set_title(\n",
    "        f\"regression: {model_regression} - RMSE {rmse.round(2)} days, MAE {mae.round(2)} days\"\n",
    "    )\n",
    "\n",
    "    fig.suptitle(\"Predicted vs Actual risk\")\n",
    "    fig.tight_layout()\n",
    "    fig.subplots_adjust(top=0.95)\n",
    "\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Extensions\n",
    "\n",
    "* Add number of predictions to bins in plots using e.g. https://stackoverflow.com/questions/30228069/how-to-display-the-value-of-the-bar-on-each-bar-with-pyplot-barh\n",
    "* Refactor visualisation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
