{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling - Classification\n",
    "\n",
    "This notebook explores classification models to **predict risk of becoming a long stayer** as a baseline to the [Long Stayer Risk Stratification](https://github.com/nhsx/skunkworks-long-stayer-risk-stratification) model.\n",
    "\n",
    "This notebook is broken down into:\n",
    "\n",
    "1. Converting the length of stay into a relative risk\n",
    "1. Training a range of baseline models using cross validation\n",
    "3. Testing final models on a test dataset\n",
    "4. Exploring in more detail the best performing baseline model\n",
    "\n",
    "Inputs|Outputs\n",
    "---|---\n",
    "`processed/features.parquet`|`models/classification.pickle`\n",
    "`processed/features-catboost.parquet`|&nbsp;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1659538244226
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import balanced_accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "sys.path.append(\"../src/\")\n",
    "\n",
    "from utils import risk_score, train_and_test_model, train_test_validate_split\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = [15, 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1659538246406
    }
   },
   "outputs": [],
   "source": [
    "features_df = pd.read_parquet(\"../../data/processed/features.parquet\")\n",
    "features_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1659538246628
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "features_catboost_df = pd.read_parquet(\"../../data/processed/features-catboost.parquet\")\n",
    "features_catboost_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate risk scores\n",
    "\n",
    "We will convert actual Length of Stay (days) into a risk score defined as:\n",
    "\n",
    "Risk Category|Day Range for Risk Category\n",
    "-----|------\n",
    "1 - Very low risk|0-6\n",
    "2 - Low risk|7-10\n",
    "3 - Normal risk|11-13\n",
    "4 - Elevated risk|14-15\n",
    "5 - High risk|>15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1659538246857
    }
   },
   "outputs": [],
   "source": [
    "# actual risk scores\n",
    "risk_labels = [\n",
    "    \"1 - Very Low Risk\",\n",
    "    \"2 - Low Risk\",\n",
    "    \"3 - Normal Risk\",\n",
    "    \"4 - Elevated Risk\",\n",
    "    \"5 - High Risk\",\n",
    "]\n",
    "features_df[\"risk\"] = [risk_score(los) for los in features_df.LENGTH_OF_STAY]\n",
    "features_catboost_df[\"risk\"] = [\n",
    "    risk_score(los) for los in features_catboost_df.LENGTH_OF_STAY\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define target and training features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1659538247009
    }
   },
   "outputs": [],
   "source": [
    "X = features_df.drop(columns=[\"LENGTH_OF_STAY\"])\n",
    "y = features_df.risk\n",
    "\n",
    "# Non-one-hot encoded data for catboost\n",
    "X_catboost = features_catboost_df.drop(columns=[\"LENGTH_OF_STAY\"])\n",
    "y_catboost = features_catboost_df.risk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/validation/test split\n",
    "\n",
    "We will use 70% of the data for training, leaving 15% for validation and 15% for testing.\n",
    "\n",
    "This split was decided based on the quantity of the data (>100,000 rows of data) and will use the same random seed as with regression, where simple checks to make sure that the splits are representative across Length of Stay, age, ethnicity and gender were conducted.\n",
    "\n",
    "Models will be trained on the training set, and tested on the validation set. This will help select a basic model using data it was not trained on, to reduce the risk of overfitting.\n",
    "\n",
    "A final model will be trained using GridSearchCV across the training and validation data, before final performance metrics are generated using the unseen test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1659538247558
    }
   },
   "outputs": [],
   "source": [
    "# Split data for train/validate+test\n",
    "X_train, X_validate, X_test, y_train, y_validate, y_test = train_test_validate_split(\n",
    "    X,\n",
    "    y,\n",
    "    train_size=0.70,\n",
    "    validate_size=0.15,\n",
    "    test_size=0.15,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "print(X_train.shape, X_validate.shape, X_test.shape)\n",
    "print(y_train.shape, y_validate.shape, y_test.shape)\n",
    "\n",
    "# Scale data for LogReg only using training data\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train_scaled = pd.DataFrame(\n",
    "    scaler.transform(X_train), index=X_train.index, columns=X_train.columns\n",
    ")\n",
    "X_validate_scaled = pd.DataFrame(\n",
    "    scaler.transform(X_validate), index=X_validate.index, columns=X_validate.columns\n",
    ")\n",
    "X_test_scaled = pd.DataFrame(\n",
    "    scaler.transform(X_test), index=X_test.index, columns=X_test.columns\n",
    ")\n",
    "print(X_train_scaled.shape, X_validate_scaled.shape, X_test_scaled.shape)\n",
    "\n",
    "# Split data for train/validate+test\n",
    "(\n",
    "    X_train_catboost,\n",
    "    X_validate_catboost,\n",
    "    X_test_catboost,\n",
    "    y_train_catboost,\n",
    "    y_validate_catboost,\n",
    "    y_test_catboost,\n",
    ") = train_test_validate_split(\n",
    "    X_catboost,\n",
    "    y_catboost,\n",
    "    train_size=0.70,\n",
    "    validate_size=0.15,\n",
    "    test_size=0.15,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "print(X_train_catboost.shape, X_validate_catboost.shape, X_test_catboost.shape)\n",
    "print(y_train_catboost.shape, y_validate_catboost.shape, y_test_catboost.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore class imbalance\n",
    "\n",
    "This is a multi-class classification model, so we need to understand where any class imbalance lies otherwise we will skew to larger classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1659538247720
    }
   },
   "outputs": [],
   "source": [
    "# Show how many of each class are present in the training set:\n",
    "X_train.risk.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight samples\n",
    "\n",
    "Given significant class imbalance, we will weight samples where the smallest class has a weight of 1, and other classes as a (<1) proportion. This is called _balanced_ weighting as can be automatically calculated for many algorithms, but not all (e.g. XGBoost).\n",
    "\n",
    "e.g.\n",
    "\n",
    "class|count|weight\n",
    "---|---|---\n",
    "1|1000|0.1\n",
    "2|100|1\n",
    "3|500|0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1659538248215
    }
   },
   "outputs": [],
   "source": [
    "# note these are same for one-hot encoded and unencoded dataframes\n",
    "# note this is same as compute_class_weight/(max weight from compute_class_weight)\n",
    "weights = X_train.risk.value_counts().min() / X_train.risk.value_counts().sort_index()\n",
    "sample_weights = [weights[risk] for risk in X_train[\"risk\"].values]\n",
    "\n",
    "## Drop risk from training/test set\n",
    "X_train.drop(columns=\"risk\", inplace=True)\n",
    "X_validate.drop(columns=\"risk\", inplace=True)\n",
    "X_test.drop(columns=\"risk\", inplace=True)\n",
    "\n",
    "X_train_scaled.drop(columns=\"risk\", inplace=True)\n",
    "X_validate_scaled.drop(columns=\"risk\", inplace=True)\n",
    "X_test_scaled.drop(columns=\"risk\", inplace=True)\n",
    "\n",
    "X_train_catboost.drop(columns=\"risk\", inplace=True)\n",
    "X_validate_catboost.drop(columns=\"risk\", inplace=True)\n",
    "X_test_catboost.drop(columns=\"risk\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "Strategy is to try a number of classification models with:\n",
    "\n",
    "* Baseline models for each algorithm trained on the training set with default parameters\n",
    "* Baseline models tested on the test set\n",
    "* GridsearchCV for hyperparameter tuning on best performing model\n",
    "* Explore feature importance of final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1659538248343
    }
   },
   "outputs": [],
   "source": [
    "# Initiate empty models dictionary\n",
    "models = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior model\n",
    "\n",
    "The simplest baseline model takes the most frequent class label as its prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1659538248497
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"prior\"\n",
    "\n",
    "# define estimator\n",
    "estimator = DummyClassifier(strategy=\"prior\")\n",
    "\n",
    "# takes ~1 s to run on a STANDARD_D13_V2\n",
    "models[model_name] = train_and_test_model(\n",
    "    estimator, X_train, y_train, X_validate, y_validate, scoring_metric=\"f1_weighted\"\n",
    ")\n",
    "models[model_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Logistic Regression (elastic net regularisation)\n",
    "\n",
    "Multi-class balanced and regularised (l1/l2 ratio of 0.5) logistic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1659538313625
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"elastic\"\n",
    "\n",
    "# define estimator\n",
    "estimator = LogisticRegression(\n",
    "    class_weight=\"balanced\",\n",
    "    penalty=\"elasticnet\",\n",
    "    solver=\"saga\",\n",
    "    l1_ratio=0.5,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# takes ~2 minutes to run on a STANDARD_D13_V2\n",
    "models[model_name] = train_and_test_model(\n",
    "    estimator,\n",
    "    X_train_scaled,\n",
    "    y_train,\n",
    "    X_validate_scaled,\n",
    "    y_validate,\n",
    "    scoring_metric=\"f1_weighted\",\n",
    ")\n",
    "models[model_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree classifier (weighted)\n",
    "\n",
    "Simplest tree classifier using one tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1659538315410
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"decisiontree\"\n",
    "\n",
    "# define estimator\n",
    "estimator = DecisionTreeClassifier(class_weight=\"balanced\", random_state=42)\n",
    "\n",
    "# takes ~5s to run on a STANDARD_D13_V2\n",
    "models[model_name] = train_and_test_model(\n",
    "    estimator, X_train, y_train, X_validate, y_validate, scoring_metric=\"f1_weighted\"\n",
    ")\n",
    "models[model_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest (weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1659538344281
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"randomforest\"\n",
    "\n",
    "estimator = RandomForestClassifier(class_weight=\"balanced\", random_state=42)\n",
    "\n",
    "# takes ~1 mins to run on a STANDARD_D13_V2\n",
    "models[model_name] = train_and_test_model(\n",
    "    estimator, X_train, y_train, X_validate, y_validate, scoring_metric=\"f1_weighted\"\n",
    ")\n",
    "models[model_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catboost\n",
    "\n",
    "Boosted tree optimised for categorical features. Note this requires **non-one-hot encoded features**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1659538536622
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"catboost\"\n",
    "\n",
    "# extract categorical features\n",
    "num_features = [\n",
    "    \"AGE_ON_ADMISSION\",\n",
    "    \"EL CountLast12m\",\n",
    "    \"EMCountLast12m\",\n",
    "    \"OP First CountLast12m\",\n",
    "    \"OP FU CountLast12m\",\n",
    "]\n",
    "cat_features = list(set(X_train_catboost.columns) - set(num_features))\n",
    "\n",
    "estimator = CatBoostClassifier(\n",
    "    verbose=False,\n",
    "    auto_class_weights=\"Balanced\",\n",
    "    cat_features=cat_features,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# takes ~8 mins to run on a STANDARD_D13_V2\n",
    "models[model_name] = train_and_test_model(\n",
    "    estimator,\n",
    "    X_train_catboost,\n",
    "    y_train_catboost,\n",
    "    X_validate_catboost,\n",
    "    y_validate_catboost,\n",
    "    scoring_metric=\"f1_weighted\",\n",
    ")\n",
    "models[model_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1659538569631
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"xgboost\"\n",
    "models[model_name] = {}\n",
    "\n",
    "# note XGBoost only accepts the \"sample_weight\" parameter in the .fit() function\n",
    "# and must be trained explicitly\n",
    "# see https://discuss.xgboost.ai/t/multi-class-classification-weighting-for-unbalanced-datasets/2789\n",
    "\n",
    "clf = XGBClassifier(random_state=42)\n",
    "\n",
    "# takes ~1 mins to run on a STANDARD_D13_V2\n",
    "models[model_name][\"model\"] = clf.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "\n",
    "# perform inference on both training and test set\n",
    "preds_train = np.clip(models[model_name][\"model\"].predict(X_train), 0, None)\n",
    "preds_test = np.clip(models[model_name][\"model\"].predict(X_validate), 0, None)\n",
    "\n",
    "# calculate performance\n",
    "models[model_name][\"train_metric\"] = f1_score(y_train, preds_train, average=\"weighted\")\n",
    "models[model_name][\"test_metric\"] = f1_score(y_validate, preds_test, average=\"weighted\")\n",
    "\n",
    "models[model_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model performance visually\n",
    "\n",
    "Use the held-out test set to evaluate and visualise the performance of all the tuned models.\n",
    "\n",
    "We will also calculate a range of metrics for the classification models:\n",
    "\n",
    "* balanced_accuracy - the overall % correct predictions, weighted per class\n",
    "* f1_score_weighted - the harmonic mean of precision and recall, weighted per class\n",
    "* auc - the area under the receiver operator characteristic (roc) curve, one class-versus rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1659538573333
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# setup a subplot figure\n",
    "fig, axs = plt.subplots(len(models), 2)\n",
    "fig.set_size_inches(15, 7 * len(models))\n",
    "\n",
    "i = 0\n",
    "\n",
    "for model in models:\n",
    "    if model == \"catboost\":\n",
    "        model_X_test = X_test_catboost\n",
    "        model_y_test = y_test\n",
    "    elif model == \"elastic\":\n",
    "        model_X_test = X_test_scaled\n",
    "        model_y_test = y_test\n",
    "    else:\n",
    "        model_X_test = X_test\n",
    "        model_y_test = y_test\n",
    "\n",
    "    # perform inference\n",
    "    preds = models[model][\"model\"].predict(model_X_test)\n",
    "    probs = models[model][\"model\"].predict_proba(model_X_test)\n",
    "\n",
    "    # calculate performance metrics\n",
    "    balanced_accuracy = balanced_accuracy_score(model_y_test, preds)\n",
    "    f1_score_weighted = f1_score(model_y_test, preds, average=\"weighted\")\n",
    "    auc = roc_auc_score(\n",
    "        model_y_test, probs, multi_class=\"ovr\", average=\"weighted\"\n",
    "    )  # one-vs-rest\n",
    "\n",
    "    # output metrics\n",
    "    print(\n",
    "        f\"{model} test balanced accuracy: {balanced_accuracy.round(3)}, f1 score (weighted): {f1_score_weighted.round(3)}, auc (ovr, weighted): {auc.round(3)}\"\n",
    "    )\n",
    "\n",
    "    # create a prediction dataframe\n",
    "    predictions_df = pd.DataFrame(data=model_y_test.reset_index(drop=True))\n",
    "    predictions_df[\"pred\"] = preds\n",
    "\n",
    "    # plot actual vs predicted COUNTS\n",
    "    axs[i, 0].hist([predictions_df.risk, predictions_df.pred])\n",
    "    axs[i, 0].legend([\"Actual risk\", \"Predicted risk\"])\n",
    "    axs[i, 0].set_title(f\"{model} - f1 weighted: {f1_score_weighted.round(2)}\")\n",
    "    axs[i, 0].set_xticks([1, 2, 3, 4, 5], labels=risk_labels, minor=False)\n",
    "    axs[i, 0].set_ylabel(\"Count of risk\")\n",
    "\n",
    "    # plot predicted vs actual CLASSES\n",
    "    risks = dict.fromkeys(risk_labels)\n",
    "    for proportion in risks:\n",
    "        risks[proportion] = np.array([0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "\n",
    "        for label in risk_labels:\n",
    "            this_risk = int(label[0])\n",
    "\n",
    "            # extract the predicted risk\n",
    "            subset = predictions_df[predictions_df.pred == this_risk]\n",
    "\n",
    "            if proportion == \"1 - Very Low Risk\":\n",
    "                count = (subset.risk == 1).sum()\n",
    "            elif proportion == \"2 - Low Risk\":\n",
    "                count = (subset.risk == 2).sum()\n",
    "            elif proportion == \"3 - Normal Risk\":\n",
    "                count = (subset.risk == 3).sum()\n",
    "            elif proportion == \"4 - Elevated Risk\":\n",
    "                count = (subset.risk == 4).sum()\n",
    "            else:\n",
    "                count = (subset.risk == 5).sum()\n",
    "\n",
    "            prop = 0 if count == 0 else count / subset.shape[0]\n",
    "\n",
    "            risks[proportion][this_risk - 1] = prop\n",
    "\n",
    "    bottom = np.array([0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "    for proportion in risks:\n",
    "        if proportion == \"1 - Very Low Risk\":\n",
    "            data = risks[proportion]\n",
    "            axs[i, 1].bar(risk_labels, data, label=proportion, width=0.35)\n",
    "        else:\n",
    "            bottom += data\n",
    "            data = risks[proportion]\n",
    "            axs[i, 1].bar(\n",
    "                risk_labels, data, label=proportion, bottom=bottom, width=0.35\n",
    "            )\n",
    "    handles, labels = axs[i, 1].get_legend_handles_labels()\n",
    "    axs[i, 1].legend(handles[::-1], labels[::-1], bbox_to_anchor=(1.05, 1))\n",
    "    axs[i, 1].set_xlabel(\"Predicted risk\")\n",
    "    axs[i, 1].set_ylabel(\"Actual risk proportion\")\n",
    "    axs[i, 1].set_title(f\"{model} - f1 weighted: {f1_score_weighted.round(2)}\")\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "While the randomforest has a higher f1_score, catboost has a higher auc score and is able to predict across the classes.\n",
    "\n",
    "We select catboost as the best performing model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Model tuning\n",
    "\n",
    "We will select the best performing model using default parameters, `catboost` and use GridSearchCV to fine tune its hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1659538573491
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# note the baseline performance of the chosen model\n",
    "model_name = \"catboost\"\n",
    "\n",
    "print(models[model_name][\"test_metric\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Re-train best model\n",
    "\n",
    "Using GridsearchCV and an appropriate parameter array for the chosen model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1659538232206
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"catboost\"\n",
    "\n",
    "final_model = {model_name: {}}\n",
    "\n",
    "# example from https://catboost.ai/en/docs/concepts/python-reference_catboostregressor_grid_search\n",
    "# see https://catboost.ai/en/docs/concepts/parameter-tuning for other options\n",
    "\n",
    "# param_grid = {\n",
    "#     \"learning_rate\": [0.03, 0.1],\n",
    "#     \"depth\": [4, 6, 10],\n",
    "#     \"l2_leaf_reg\": [1, 3, 5, 7, 9],\n",
    "# }\n",
    "\n",
    "# the catboost classifier takes ~5 minutes per fit, so the above parameter space\n",
    "# would take ~ 12.5 hours to complete. We will define a reduced parameter space:\n",
    "param_grid = {\"depth\": [4, 10], \"l2_leaf_reg\": [1, 9], \"learning_rate\": [0.03, 0.1]}\n",
    "\n",
    "# extract categorical features\n",
    "num_features = [\n",
    "    \"AGE_ON_ADMISSION\",\n",
    "    \"EL CountLast12m\",\n",
    "    \"EMCountLast12m\",\n",
    "    \"OP First CountLast12m\",\n",
    "    \"OP FU CountLast12m\",\n",
    "]\n",
    "cat_features = list(set(X_train_catboost.columns) - set(num_features))\n",
    "\n",
    "gsc = GridSearchCV(\n",
    "    estimator=CatBoostClassifier(\n",
    "        verbose=False,\n",
    "        auto_class_weights=\"Balanced\",\n",
    "        cat_features=cat_features,\n",
    "        random_state=42,\n",
    "    ),\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"f1_weighted\",\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    refit=True,\n",
    ")\n",
    "\n",
    "# note: we will use both the training and validation dataset as we are using GridSearchCV across 5-folds\n",
    "# takes ~3h40 mins to run on a STANDARD_D13_V2\n",
    "grid_result = gsc.fit(\n",
    "    pd.concat([X_train_catboost, X_validate_catboost]),\n",
    "    pd.concat([y_train_catboost, y_validate_catboost]),\n",
    ")\n",
    "\n",
    "final_model[model_name][\"train_metric\"] = grid_result.best_score_\n",
    "# store model and parameters\n",
    "final_model[model_name][\"model\"] = grid_result.best_estimator_\n",
    "final_model[model_name][\"params\"] = grid_result.best_params_\n",
    "final_model[model_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Model evaluation\n",
    "\n",
    "Now we have tuned the best model given the parameters specified, we will test the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1659599106040
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# generate predictions\n",
    "preds_test = final_model[model_name][\"model\"].predict(X_test_catboost)\n",
    "\n",
    "# append the test metrics to the model\n",
    "final_model[model_name][\"test_metric\"] = f1_score(\n",
    "    y_test_catboost, preds_test, average=\"weighted\"\n",
    ")\n",
    "final_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "We now have our best performing baseline model, which achieves a **weighted f1 score of 0.6**.\n",
    "\n",
    "However, the predicted vs actual plots show that the model struggles to capture long stayers, as low risk stayers are present in a high proportion (50%+) in each risk category. Further work is needed to improve the modelling approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1656016238290
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "models[\"final_model\"] = final_model\n",
    "\n",
    "# save models outside the git tree\n",
    "with open(\"../../models/classification.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(models, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model exploration\n",
    "\n",
    "We will take the \"best performing\" baseline model, and explore in more detail how the model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1659599358889
    }
   },
   "outputs": [],
   "source": [
    "# setup a subplot figure\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "fig.set_size_inches(15, 7)\n",
    "\n",
    "if model_name == \"catboost\":\n",
    "    model_X_test = X_test_catboost\n",
    "    model_y_test = y_test\n",
    "elif model_name == \"elastic\":\n",
    "    model_X_test = X_test_scaled\n",
    "    model_y_test = y_test\n",
    "else:\n",
    "    model_X_test = X_test\n",
    "    model_y_test = y_test\n",
    "\n",
    "# perform inference\n",
    "preds = models[\"final_model\"][model_name][\"model\"].predict(model_X_test)\n",
    "probs = models[\"final_model\"][model_name][\"model\"].predict_proba(model_X_test)\n",
    "\n",
    "# calculate performance metrics\n",
    "balanced_accuracy = balanced_accuracy_score(model_y_test, preds)\n",
    "f1_score_weighted = f1_score(model_y_test, preds, average=\"weighted\")\n",
    "auc = roc_auc_score(\n",
    "    model_y_test, probs, multi_class=\"ovr\", average=\"weighted\"\n",
    ")  # one-vs-rest\n",
    "\n",
    "# output metrics\n",
    "print(\n",
    "    f\"{model_name} test balanced accuracy: {balanced_accuracy.round(3)}, f1 score (weighted): {f1_score_weighted.round(3)}, auc (ovr, weighted): {auc.round(3)}\"\n",
    ")\n",
    "\n",
    "# create a prediction dataframe\n",
    "predictions_df = pd.DataFrame(data=model_y_test.reset_index(drop=True))\n",
    "predictions_df[\"pred\"] = preds\n",
    "\n",
    "# plot actual vs predicted COUNTS\n",
    "axs[0].hist([predictions_df.risk, predictions_df.pred])\n",
    "axs[0].legend([\"Actual risk\", \"Predicted risk\"])\n",
    "axs[0].set_title(f\"{model_name} - f1 weighted: {f1_score_weighted.round(2)}\")\n",
    "axs[0].set_xticks([1, 2, 3, 4, 5], labels=risk_labels, minor=False)\n",
    "axs[0].set_ylabel(\"Count of risk\")\n",
    "\n",
    "# plot predicted vs actual CLASSES\n",
    "risks = dict.fromkeys(risk_labels)\n",
    "for proportion in risks:\n",
    "    risks[proportion] = np.array([0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "\n",
    "    for label in risk_labels:\n",
    "        this_risk = int(label[0])\n",
    "\n",
    "        # extract the predicted risk\n",
    "        subset = predictions_df[predictions_df.pred == this_risk]\n",
    "\n",
    "        if proportion == \"1 - Very Low Risk\":\n",
    "            count = (subset.risk == 1).sum()\n",
    "        elif proportion == \"2 - Low Risk\":\n",
    "            count = (subset.risk == 2).sum()\n",
    "        elif proportion == \"3 - Normal Risk\":\n",
    "            count = (subset.risk == 3).sum()\n",
    "        elif proportion == \"4 - Elevated Risk\":\n",
    "            count = (subset.risk == 4).sum()\n",
    "        else:\n",
    "            count = (subset.risk == 5).sum()\n",
    "\n",
    "        prop = 0 if count == 0 else count / subset.shape[0]\n",
    "\n",
    "        risks[proportion][this_risk - 1] = prop\n",
    "\n",
    "bottom = np.array([0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "for proportion in risks:\n",
    "    if proportion == \"1 - Very Low Risk\":\n",
    "        data = risks[proportion]\n",
    "        axs[1].bar(risk_labels, data, label=proportion, width=0.35)\n",
    "    else:\n",
    "        bottom += data\n",
    "        data = risks[proportion]\n",
    "        axs[1].bar(risk_labels, data, label=proportion, bottom=bottom, width=0.35)\n",
    "handles, labels = axs[1].get_legend_handles_labels()\n",
    "axs[1].legend(handles[::-1], labels[::-1], bbox_to_anchor=(1.05, 1))\n",
    "axs[1].set_xlabel(\"Predicted risk\")\n",
    "axs[1].set_ylabel(\"Actual risk proportion\")\n",
    "axs[1].set_title(f\"{model_name} - f1 weighted: {f1_score_weighted.round(2)}\")\n",
    "\n",
    "fig.suptitle(\"Final model\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Severity of misclassification\n",
    "\n",
    "When the model incorrectly predicts a class, how badly does it do this?\n",
    "\n",
    "Because risk categories are numerical (1-5), we can calculate the difference between them as the number of classes incorrect the prediction was."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1659599382377
    }
   },
   "outputs": [],
   "source": [
    "predictions_df[\"diff\"] = predictions_df.diff().risk\n",
    "fig = px.histogram(predictions_df, x=\"diff\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Feature importance\n",
    "\n",
    "Which features does the model ascribe predictive power to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1659599388116
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Feature names\n",
    "coef = pd.DataFrame(data=list(model_X_test.columns))\n",
    "# Feature importances, sorted\n",
    "coef[\"coef\"] = models[\"final_model\"][model_name][\"model\"].feature_importances_\n",
    "coef.sort_values(\"coef\", ascending=False, inplace=True)\n",
    "coef.set_index(0, inplace=True)\n",
    "# Plot interactive plot\n",
    "# Hover over a feature for full feature name\n",
    "fig = px.bar(coef, x=coef.index, y=\"coef\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions\n",
    "\n",
    "- Fairness analysis\n",
    "- Analysis of distribution of probabilities e.g. `predict_proba` to see how changes to threshold affect performance\n",
    "- Plot PR curves per class as per https://stackoverflow.com/questions/56090541/how-to-plot-precision-and-recall-of-multiclass-classifier\n",
    "- Train a binary classifier on Long Stay (21+ days) or not, use it as a precursor to two different regression models (one for long stayer, one for not)\n",
    "- Include IS_MINOR data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6d65a8c07f5b6469e0fc613f182488c0dccce05038bbda39e5ac9075c0454d11"
  },
  "kernel_info": {
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
