{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling - Regression\n",
    "\n",
    "This notebook explores regression models to **predict Length of Stay (days)** as a baseline to the [Long Stayer Risk Stratification](https://github.com/nhsx/skunkworks-long-stayer-risk-stratification) model which achieved a Mean Absolute Error **(MAE) of 3.8 days** (2.2 median absolute error).\n",
    "\n",
    "This notebook is broken down into:\n",
    "\n",
    "1. Statistical tests to check the validity of linear models using Ordinary Least Squares (OLS)\n",
    "2. Training a range of baseline models using cross validation\n",
    "3. Testing final models on a test dataset\n",
    "4. Exploring in more detail the best performing baseline model\n",
    "\n",
    "Regression models selected:\n",
    "\n",
    "\n",
    "Model|Rationale\n",
    "---|---\n",
    "[Mean](https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyRegressor.html)|The simplest baseline, uses the mean length of stay as the prediction in all cases\n",
    "[ElasticNet](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html)|A regularised implementation of linear regression that can be used for multi-colinear datasets such as in this dataset\n",
    "[DecisionTreeRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html)|A simple, single tree regressor that is highly explainable\n",
    "[RandomForestRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)|An ensemble technique with potentially better performance than a single tree\n",
    "[CatBoostRegressor](https://catboost.ai/en/docs/concepts/python-reference_catboostregressor)|A boosted tree technique designed specifically for datasets with high levels of categorical features as in this dataset\n",
    "[XGBRegressor](https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.XGBRegressor)|A boosted tree technique that can improve on ensemble techniques such as RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1655889457976
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from catboost import CatBoostRegressor\n",
    "from scipy.stats import anderson, kstest, shapiro\n",
    "from sklearn import linear_model\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = [15, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1655889458126
    }
   },
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "\n",
    "def train_model(gsc, X_train, y_train):\n",
    "    \"\"\"Uses a GridSearchCV instance to find a reasonable model, and store\n",
    "    performance and fitted model into a python dict\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "        gsc (sklearn.model_selection.GridSearchCV object): defined model\n",
    "        X_train (pandas dataframe): training dataframe with features\n",
    "        y_train (pandas dataframe): training dataframe with targets\n",
    "\n",
    "    Returns:\n",
    "\n",
    "        (dict): resulting fitted model and performance metrics\n",
    "    \"\"\"\n",
    "\n",
    "    grid_result = gsc.fit(X_train, y_train)\n",
    "\n",
    "    # note model scoring approach defined in gsc\n",
    "    model = {\n",
    "        \"cv_metric_mean\": np.round(\n",
    "            -grid_result.cv_results_[\"mean_test_score\"][grid_result.best_index_], 3\n",
    "        ),\n",
    "        \"cv_metric_std\": np.round(\n",
    "            grid_result.cv_results_[\"std_test_score\"][grid_result.best_index_], 2\n",
    "        ),\n",
    "        \"model\": grid_result.best_estimator_,\n",
    "    }\n",
    "\n",
    "    # retrain the best estimator on the full training set - note that refit=True does not appear to do this\n",
    "    # note we calculate MAE as final metric\n",
    "    model[\"model\"].fit(X_train, y_train)\n",
    "    model[\"mae\"] = np.round(\n",
    "        mean_absolute_error(y_train, model[\"model\"].predict(X_train)), 3\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def risk_score(los):\n",
    "    \"\"\"Return risk score (1-5) based on LoS\n",
    "\n",
    "    Parameters:\n",
    "        los (float): length of stay in days\n",
    "\n",
    "    Returns:\n",
    "        (int): risk score (1 = Very low risk, 5 = High risk)\n",
    "    \"\"\"\n",
    "\n",
    "    # round los up to whole days\n",
    "    los = math.ceil(los)\n",
    "\n",
    "    if los > 15:\n",
    "        return 5\n",
    "    elif los > 13:\n",
    "        return 4\n",
    "    elif los > 10:\n",
    "        return 3\n",
    "    elif los > 6:\n",
    "        return 2\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1655889458331
    }
   },
   "outputs": [],
   "source": [
    "features_df = pd.read_parquet(\"../../data/processed/features.parquet\")\n",
    "features_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1655889458816
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# note catboost requires not one-hot encoding features, as it deals with them during training\n",
    "features_catboost_df = pd.read_parquet(\"../../data/processed/features-catboost.parquet\")\n",
    "features_catboost_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define target and training features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1655889458939
    }
   },
   "outputs": [],
   "source": [
    "X = features_df.drop(columns=\"LENGTH_OF_STAY\")\n",
    "y = features_df.LENGTH_OF_STAY\n",
    "\n",
    "X_catboost = features_catboost_df.drop(columns=\"LENGTH_OF_STAY\")\n",
    "y_catboost = features_catboost_df.LENGTH_OF_STAY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance Inflaction Factors\n",
    "\n",
    "Simple linear models (ordinary least squares) assume there is no multi-collinearity.\n",
    "\n",
    "Variance inflaction factors (VIF) help quantify the extent of any collinearity present.\n",
    "\n",
    "We are looking for VIF ~< 10 across our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1655889705754
    }
   },
   "outputs": [],
   "source": [
    "# Takes ~6 minutes to run on a STANDARD_DS3_V2\n",
    "vif = pd.DataFrame()\n",
    "vif[\"feature\"] = X.columns\n",
    "\n",
    "vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(len(X.columns))]\n",
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual distributions of [OLS](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)\n",
    "\n",
    "If VIF factors indicate a lack of multi-collinearity (they do not), check for normality of residuals aka homoescadisticity.\n",
    "\n",
    "This requires training a linear model, calculating the residuals and checking visually and statistically that they are normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1655889706789
    }
   },
   "outputs": [],
   "source": [
    "# Train basic OLS model for statistical testing only\n",
    "reg = linear_model.LinearRegression()\n",
    "reg.fit(X, y)\n",
    "pred = pd.Series(reg.predict(X))\n",
    "resid = y - pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1655889706960
    }
   },
   "outputs": [],
   "source": [
    "# Visual inspection\n",
    "resid.hist(bins=30);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shapiro-Wilk test for normality\n",
    "\n",
    "* Null hypothesis = our residuals are drawn from normal distribution\n",
    "* Alternate hypothesis = our residuals are not drawn from normal distribution (and fail requirements of OLS model)\n",
    "* Test statistic shows how much distribution differs to normal distribution\n",
    "* p-value is probability null hypothesis true\n",
    "* p-value < 0.05 leads us to reject null hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1655889707110
    }
   },
   "outputs": [],
   "source": [
    "shapiro(resid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-sided Kolmogorov-Smirnov test for normality\n",
    "\n",
    "* Null hypothesis = our residuals are drawn from normal distribution\n",
    "* Alternate hypothesis = our residuals are not drawn from normal distribution (and fail requirements of OLS model)\n",
    "* Test statistic shows how much distribution differs to normal distribution\n",
    "* p-value is probability null hypothesis true\n",
    "* p-value < 0.05 leads us to reject null hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1655889707444
    }
   },
   "outputs": [],
   "source": [
    "kstest(resid, \"norm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anderson-Darling test for normality\n",
    "\n",
    "* Null hypothesis = our residuals are drawn from normal distribution\n",
    "* Alternate hypothesis = our residuals are not drawn from normal distribution (and fail requirements of OLS model)\n",
    "* Test statistic is compared to critical value at the significance level required (e.g. 5%)\n",
    "* Test statistic > critical value for 5% significance level leads us to reject null hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1655889707583
    }
   },
   "outputs": [],
   "source": [
    "anderson(resid, \"norm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Statistical testing invalidate assumptions for OLS models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/test split\n",
    "\n",
    "For model evaluation, we will hold back a 25% test set, leaving a 75% training set. \n",
    "\n",
    "Models will be trained on the training set using 5-fold cross-validation, which helps provide an understanding of robustness by generating performance metrics across different folds - if any folds have large variance in performance, that may indicate overfitting within the folds.\n",
    "\n",
    "A final comparison of all the models will be made on the test set, generating performance metrics on data none of the models have seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1655889707696
    }
   },
   "outputs": [],
   "source": [
    "# Split data for train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, train_size=0.75, random_state=42\n",
    ")\n",
    "print(X_train.shape, X_test.shape)\n",
    "\n",
    "X_train_catboost, X_test_catboost, y_train_catboost, y_test_catboost = train_test_split(\n",
    "    X_catboost, y_catboost, train_size=0.75, random_state=42\n",
    ")\n",
    "print(X_train_catboost.shape, X_test_catboost.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "Strategy is to try a number of regression models with:\n",
    "\n",
    "* GridsearchCV for hyperparameter tuning with cross validation, refitting full training set for best model\n",
    "* Test all final models against the held-out test set.\n",
    "* Explore feature importance of best performing model\n",
    "* Explore fairness (next notebook) of best performing model\n",
    "\n",
    "OLS models are excluded due to statistical assumptions not being met. NN are excluded due to complexity/interpretability issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1655889707803
    }
   },
   "outputs": [],
   "source": [
    "# Initiate empty models dictionary\n",
    "models = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean model\n",
    "\n",
    "The simplest baseline model takes the mean length of stay as its prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1655889708922
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"mean\"\n",
    "\n",
    "# define a gridsearch constructor for this model.\n",
    "# note we are not specifiying hyperparameters to tune for baselining\n",
    "# models will be cross-validated using the default hyperparameter values\n",
    "gsc = GridSearchCV(\n",
    "    estimator=DummyRegressor(strategy=\"mean\"),\n",
    "    param_grid={},\n",
    "    cv=5,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    refit=True,\n",
    ")\n",
    "\n",
    "# takes ~1 second to run on a STANDARD_D13_V2\n",
    "models[model_name] = train_model(gsc, X_train, y_train)\n",
    "models[model_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic net regression\n",
    "\n",
    "A regularised linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1655889709918
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"elastic\"\n",
    "\n",
    "gsc = GridSearchCV(\n",
    "    estimator=linear_model.ElasticNet(),\n",
    "    param_grid={},\n",
    "    cv=5,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    refit=True,\n",
    ")\n",
    "\n",
    "# takes ~1 seconds to run on a STANDARD_D13_V2\n",
    "models[model_name] = train_model(gsc, X_train, y_train)\n",
    "models[model_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Decision Tree regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1655889717070
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"decisiontree\"\n",
    "\n",
    "gsc = GridSearchCV(\n",
    "    estimator=DecisionTreeRegressor(),\n",
    "    param_grid={},\n",
    "    cv=5,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    refit=True,\n",
    ")\n",
    "\n",
    "# takes ~10 seconds to run on a STANDARD_D13_V2\n",
    "models[model_name] = train_model(gsc, X_train, y_train)\n",
    "models[model_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1655890148049
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"randomforest\"\n",
    "\n",
    "gsc = GridSearchCV(\n",
    "    estimator=RandomForestRegressor(),\n",
    "    param_grid={},\n",
    "    cv=5,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    refit=True,\n",
    ")\n",
    "\n",
    "# takes ~7 mins to run on a STANDARD_D13_V2\n",
    "models[model_name] = train_model(gsc, X_train, y_train)\n",
    "models[model_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catboost\n",
    "\n",
    "Boosted tree optimised for categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1655893970132
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"catboost\"\n",
    "\n",
    "# extract categorical features\n",
    "num_features = [\n",
    "    \"AGE_ON_ADMISSION\",\n",
    "    \"EL CountLast12m\",\n",
    "    \"EMCountLast12m\",\n",
    "    \"OP First CountLast12m\",\n",
    "    \"OP FU CountLast12m\",\n",
    "]\n",
    "cat_features = list(set(X_train_catboost.columns) - set(num_features))\n",
    "\n",
    "gsc = GridSearchCV(\n",
    "    estimator=CatBoostRegressor(verbose=False, cat_features=cat_features),\n",
    "    param_grid={},\n",
    "    cv=5,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    refit=True,\n",
    ")\n",
    "\n",
    "# takes ~4 mins to run on a STANDARD_D13_V2\n",
    "models[model_name] = train_model(gsc, X_train_catboost, y_train)\n",
    "models[model_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1655890530957
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"xgboost\"\n",
    "\n",
    "# define gridsearch parameters\n",
    "gsc = GridSearchCV(\n",
    "    estimator=XGBRegressor(random_state=42),\n",
    "    param_grid={},\n",
    "    cv=5,\n",
    "    scoring=\"neg_mean_squared_error\",\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    refit=True,\n",
    ")\n",
    "\n",
    "# takes ~2 mins to run on a STANDARD_D13_V2\n",
    "models[model_name] = train_model(gsc, X_train, y_train)\n",
    "models[model_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1655894065385
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1655894061107
    }
   },
   "outputs": [],
   "source": [
    "# save models outside the git tree\n",
    "with open(\"../../models/regression.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(models, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate models\n",
    "\n",
    "Use the held-out test set to evaluate and visualise the performance of all the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1655894084567
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# setup a subplot figure\n",
    "fig, axs = plt.subplots(len(models), 2)\n",
    "fig.set_size_inches(15, 7 * len(models))\n",
    "\n",
    "i = 0\n",
    "\n",
    "for model in models:\n",
    "    if model == \"catboost\":\n",
    "        model_X_test = X_test_catboost\n",
    "        model_y_test = y_test\n",
    "    else:\n",
    "        model_X_test = X_test\n",
    "        model_y_test = y_test\n",
    "\n",
    "    # inference - ensure smallest LoS is 0 days (not negative value)\n",
    "    preds = np.clip(models[model][\"model\"].predict(model_X_test), 0, None)\n",
    "\n",
    "    # calculate MAE and range of LoS\n",
    "    mae = mean_absolute_error(model_y_test, preds)\n",
    "    print(\n",
    "        f\"{model} test mae: {mae.round(3)} ({preds.min().round(1)} - {preds.max().round(1)} days)\"\n",
    "    )\n",
    "\n",
    "    # create prediction dataframe\n",
    "    predictions_df = pd.DataFrame(data=model_y_test.reset_index(drop=True))\n",
    "    predictions_df[\"pred\"] = preds\n",
    "\n",
    "    # calculate relative error\n",
    "    predictions_df[\"error\"] = predictions_df.pred - predictions_df.LENGTH_OF_STAY\n",
    "\n",
    "    # plot predicted vs actual\n",
    "    axs[i, 0].scatter(predictions_df.pred, predictions_df.LENGTH_OF_STAY, alpha=0.1)\n",
    "    # plot ideal 1:1 prediction line. Max LoS = 30\n",
    "    axs[i, 0].plot(np.arange(0, 31), np.arange(0, 31), \"r--\")\n",
    "    axs[i, 0].set_xlabel(\"Predicted Length of Stay (days)\")\n",
    "    axs[i, 0].set_ylabel(\"Actual Length of Stay (days)\")\n",
    "    axs[i, 0].set_xlim([-1, 31])\n",
    "    axs[i, 0].set_ylim([-1, 31])\n",
    "    axs[i, 0].set_title(f\"{model} - MAE {mae.round(2)} days\")\n",
    "\n",
    "    # plot relative error\n",
    "    axs[i, 1].scatter(predictions_df.LENGTH_OF_STAY, predictions_df.error, alpha=0.1)\n",
    "    axs[i, 1].set_xlabel(\"Length of Stay (days)\")\n",
    "    axs[i, 1].set_ylabel(\"Error (days)\")\n",
    "    # plot mean relative error and 95% confidence intervals\n",
    "    axs[i, 1].plot(np.arange(0, 31), np.ones(31) * predictions_df.error.mean(), \"r\")\n",
    "    axs[i, 1].plot(\n",
    "        np.arange(0, 51),\n",
    "        np.ones(51) * (predictions_df.error.mean() + (2 * predictions_df.error.std())),\n",
    "        \"g--\",\n",
    "    )\n",
    "    axs[i, 1].plot(\n",
    "        np.arange(0, 51),\n",
    "        np.ones(51) * (predictions_df.error.mean() - (2 * predictions_df.error.std())),\n",
    "        \"g--\",\n",
    "    )\n",
    "    # scale plot\n",
    "    axs[i, 1].set_xlim([-1, 31])\n",
    "    # add statistical data in legend. LoA = limits of agreement\n",
    "    # flag: errors are not normally distibuted so does 2*std capture 95% interval?\n",
    "\n",
    "    axs[i, 1].legend(\n",
    "        [\n",
    "            f\"\\u03bc ({np.round(predictions_df.error.mean(),2)} days)\",\n",
    "            f\"95% LoA (\\u03C3 {np.round(predictions_df.error.std(),2)} days gives {2*np.round(predictions_df.error.std(),2)})\",\n",
    "        ]\n",
    "    )\n",
    "    axs[i, 1].set_title(f\"{model} - MAE {mae.round(2)} days\")\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model exploration\n",
    "\n",
    "A single performance metric can be a misleading summary of how a model performs. We will take the \"best performing\" baseline model, and explore in more detail how the model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1655890542308
    }
   },
   "outputs": [],
   "source": [
    "model = \"catboost\"\n",
    "\n",
    "if model == \"catboost\":\n",
    "    model_X_test = X_test_catboost\n",
    "    model_y_test = y_test\n",
    "else:\n",
    "    model_X_test = X_test\n",
    "    model_y_test = y_test\n",
    "\n",
    "actual = y_test.reset_index(drop=True)  # extract actual LoS\n",
    "preds = np.clip(\n",
    "    models[model][\"model\"].predict(model_X_test), 0, None\n",
    ")  # predict with min LoS 0 days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Feature importance\n",
    "\n",
    "Which features does the model ascribe predictive power to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1655890544566
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Feature names\n",
    "coef = pd.DataFrame(data=list(model_X_test.columns))\n",
    "# Feature importances, sorted\n",
    "coef[\"coef\"] = models[model][\"model\"].feature_importances_\n",
    "coef.sort_values(\"coef\", ascending=False, inplace=True)\n",
    "coef.set_index(0, inplace=True)\n",
    "# Plot interactive plot\n",
    "# Hover over a feature for full feature name\n",
    "fig = px.bar(coef, x=coef.index, y=\"coef\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Extensions\n",
    "\n",
    "- Tune hyperparameters using existing gridsearch framework\n",
    "- Build two separate regression models - one for long stayers (21+ days), one for not long-stayers.\n",
    "- Include IS_MINOR data in conjunction with above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6d65a8c07f5b6469e0fc613f182488c0dccce05038bbda39e5ac9075c0454d11"
  },
  "kernel_info": {
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
