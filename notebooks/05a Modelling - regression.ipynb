{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling - Regression\n",
    "\n",
    "This notebook explores regression models to **predict Length of Stay (days)** as a baseline to the [Long Stayer Risk Stratification](https://github.com/nhsx/skunkworks-long-stayer-risk-stratification) model which achieved a Mean Absolute Error **(MAE) of 3.8 days** (2.2 median absolute error).\n",
    "\n",
    "This notebook is broken down into:\n",
    "\n",
    "1. Statistical tests to check the validity of linear models using Ordinary Least Squares (OLS)\n",
    "2. Training a range of baseline models using cross validation\n",
    "3. Testing final models on a test dataset\n",
    "4. Exploring in more detail the best performing baseline model\n",
    "\n",
    "Regression models selected:\n",
    "\n",
    "\n",
    "Model|Rationale\n",
    "---|---\n",
    "[Mean](https://scikit-learn.org/stable/modules/generated/sklearn.dummy.DummyRegressor.html)|The simplest baseline, uses the mean length of stay as the prediction in all cases\n",
    "[ElasticNet](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html)|A regularised implementation of linear regression that can be used for multi-colinear datasets such as in this dataset\n",
    "[DecisionTreeRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html)|A simple, single tree regressor that is highly explainable\n",
    "[RandomForestRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)|An ensemble technique with potentially better performance than a single tree\n",
    "[CatBoostRegressor](https://catboost.ai/en/docs/concepts/python-reference_catboostregressor)|A boosted tree technique designed specifically for datasets with high levels of categorical features as in this dataset\n",
    "[XGBRegressor](https://xgboost.readthedocs.io/en/stable/python/python_api.html#xgboost.XGBRegressor)|A boosted tree technique that can improve on ensemble techniques such as RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1657729155796
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from catboost import CatBoostRegressor\n",
    "from scipy.stats import anderson, kstest, shapiro\n",
    "from sklearn import linear_model\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "sys.path.append(\"../src/\")\n",
    "\n",
    "from utils import train_and_test_model\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = [15, 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1657729156235
    }
   },
   "outputs": [],
   "source": [
    "features_df = pd.read_parquet(\"../../data/processed/features.parquet\")\n",
    "features_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1657729156476
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# note catboost requires not one-hot encoding features, as it deals with them during training\n",
    "features_catboost_df = pd.read_parquet(\"../../data/processed/features-catboost.parquet\")\n",
    "features_catboost_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define target and training features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1657729156906
    }
   },
   "outputs": [],
   "source": [
    "X = features_df.drop(columns=\"LENGTH_OF_STAY\")\n",
    "y = features_df.LENGTH_OF_STAY\n",
    "\n",
    "X_catboost = features_catboost_df.drop(columns=\"LENGTH_OF_STAY\")\n",
    "y_catboost = features_catboost_df.LENGTH_OF_STAY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variance Inflaction Factors\n",
    "\n",
    "Simple linear models (ordinary least squares) assume there is no multi-collinearity.\n",
    "\n",
    "Variance inflaction factors (VIF) help quantify the extent of any collinearity present.\n",
    "\n",
    "We are looking for VIF ~< 10 across our features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1657726818378
    }
   },
   "outputs": [],
   "source": [
    "# Takes ~6 minutes to run on a STANDARD_DS3_V2\n",
    "vif = pd.DataFrame()\n",
    "vif[\"feature\"] = X.columns\n",
    "\n",
    "vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(len(X.columns))]\n",
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Residual distributions of [OLS](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html)\n",
    "\n",
    "If VIF factors indicate a lack of multi-collinearity (they do not), check for normality of residuals aka homoescadisticity.\n",
    "\n",
    "This requires training a linear model, calculating the residuals and checking visually and statistically that they are normally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1657726821347
    }
   },
   "outputs": [],
   "source": [
    "# Train basic OLS model for statistical testing only\n",
    "reg = linear_model.LinearRegression()\n",
    "reg.fit(X, y)\n",
    "pred = pd.Series(reg.predict(X))\n",
    "resid = y - pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1657726821728
    }
   },
   "outputs": [],
   "source": [
    "# Visual inspection\n",
    "resid.hist(bins=30);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shapiro-Wilk test for normality\n",
    "\n",
    "* Null hypothesis = our residuals are drawn from normal distribution\n",
    "* Alternate hypothesis = our residuals are not drawn from normal distribution (and fail requirements of OLS model)\n",
    "* Test statistic shows how much distribution differs to normal distribution\n",
    "* p-value is probability null hypothesis true\n",
    "* p-value < 0.05 leads us to reject null hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1657726821893
    }
   },
   "outputs": [],
   "source": [
    "shapiro(resid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-sided Kolmogorov-Smirnov test for normality\n",
    "\n",
    "* Null hypothesis = our residuals are drawn from normal distribution\n",
    "* Alternate hypothesis = our residuals are not drawn from normal distribution (and fail requirements of OLS model)\n",
    "* Test statistic shows how much distribution differs to normal distribution\n",
    "* p-value is probability null hypothesis true\n",
    "* p-value < 0.05 leads us to reject null hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1657726822016
    }
   },
   "outputs": [],
   "source": [
    "kstest(resid, \"norm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anderson-Darling test for normality\n",
    "\n",
    "* Null hypothesis = our residuals are drawn from normal distribution\n",
    "* Alternate hypothesis = our residuals are not drawn from normal distribution (and fail requirements of OLS model)\n",
    "* Test statistic is compared to critical value at the significance level required (e.g. 5%)\n",
    "* Test statistic > critical value for 5% significance level leads us to reject null hypothesis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1657726822149
    }
   },
   "outputs": [],
   "source": [
    "anderson(resid, \"norm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Statistical testing invalidate assumptions for OLS models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/test split\n",
    "\n",
    "For model evaluation, we will hold back a 25% test set, leaving a 75% training set. This is the default split for scikit-learn's [`train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) function, and a good figure for the size of the dataset we have (>100,000 rows of data).\n",
    "\n",
    "Models will be trained on the training set using 5-fold cross-validation, which helps provide an understanding of robustness by generating performance metrics across different folds - if any folds have large variance in performance, that may indicate overfitting within the folds.\n",
    "\n",
    "A final comparison of all the models will be made on the test set, generating performance metrics on data none of the models have seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1657729161981
    }
   },
   "outputs": [],
   "source": [
    "# Split data for train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, train_size=0.75, random_state=42\n",
    ")\n",
    "print(X_train.shape, X_test.shape)\n",
    "\n",
    "X_train_catboost, X_test_catboost, y_train_catboost, y_test_catboost = train_test_split(\n",
    "    X_catboost, y_catboost, train_size=0.75, random_state=42\n",
    ")\n",
    "print(X_train_catboost.shape, X_test_catboost.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "Strategy is to try a number of regression models with:\n",
    "\n",
    "* Baseline models for each algorithm trained on the training set with default parameters\n",
    "* Baseline models tested on the test set\n",
    "* GridsearchCV for hyperparameter tuning on best performing model\n",
    "* Explore feature importance of final model\n",
    "* Explore fairness (next notebook) of final model\n",
    "\n",
    "OLS models are excluded due to statistical assumptions not being met. NN are excluded due to complexity/interpretability issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1657729163183
    }
   },
   "outputs": [],
   "source": [
    "# Initiate empty models dictionary\n",
    "models = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean model\n",
    "\n",
    "The simplest baseline model takes the mean length of stay as its prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1657729164026
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"mean\"\n",
    "\n",
    "# define an estimator for this model\n",
    "estimator = DummyRegressor(strategy=\"mean\")\n",
    "\n",
    "# takes ~1 second to run on a STANDARD_D13_V2\n",
    "models[model_name] = train_and_test_model(\n",
    "    estimator, X_train, y_train, X_test, y_test, scoring_metric=\"rmse\"\n",
    ")\n",
    "models[model_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elastic net regression\n",
    "\n",
    "A regularised linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1657729175798
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"elastic\"\n",
    "\n",
    "# define an estimator for this model\n",
    "estimator = linear_model.ElasticNet(random_state=42)\n",
    "\n",
    "# takes ~1 second to run on a STANDARD_D13_V2\n",
    "models[model_name] = train_and_test_model(\n",
    "    estimator, X_train, y_train, X_test, y_test, scoring_metric=\"rmse\"\n",
    ")\n",
    "models[model_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Decision Tree regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1657729187851
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"decisiontree\"\n",
    "\n",
    "estimator = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "# takes ~10 seconds to run on a STANDARD_D13_V2\n",
    "models[model_name] = train_and_test_model(\n",
    "    estimator, X_train, y_train, X_test, y_test, scoring_metric=\"rmse\"\n",
    ")\n",
    "\n",
    "models[model_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1657729321863
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"randomforest\"\n",
    "\n",
    "estimator = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# takes ~3 mins to run on a STANDARD_D13_V2\n",
    "models[model_name] = train_and_test_model(\n",
    "    estimator, X_train, y_train, X_test, y_test, scoring_metric=\"rmse\"\n",
    ")\n",
    "models[model_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catboost\n",
    "\n",
    "Boosted tree optimised for categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1657729364152
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"catboost\"\n",
    "\n",
    "# extract categorical features\n",
    "num_features = [\n",
    "    \"AGE_ON_ADMISSION\",\n",
    "    \"EL CountLast12m\",\n",
    "    \"EMCountLast12m\",\n",
    "    \"OP First CountLast12m\",\n",
    "    \"OP FU CountLast12m\",\n",
    "]\n",
    "cat_features = list(set(X_train_catboost.columns) - set(num_features))\n",
    "\n",
    "estimator = CatBoostRegressor(verbose=False, cat_features=cat_features, random_state=42)\n",
    "\n",
    "# takes ~30 secs to run on a STANDARD_D13_V2\n",
    "models[model_name] = train_and_test_model(\n",
    "    estimator,\n",
    "    X_train_catboost,\n",
    "    y_train_catboost,\n",
    "    X_test_catboost,\n",
    "    y_test_catboost,\n",
    "    scoring_metric=\"rmse\",\n",
    ")\n",
    "models[model_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1657729379221
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"xgboost\"\n",
    "\n",
    "estimator = XGBRegressor(random_state=42)\n",
    "\n",
    "# takes ~10s to run on a STANDARD_D13_V2\n",
    "models[model_name] = train_and_test_model(\n",
    "    estimator, X_train, y_train, X_test, y_test, scoring_metric=\"rmse\"\n",
    ")\n",
    "models[model_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate model performance visually\n",
    "\n",
    "We will evalulate how the models performed so we can select the best model for hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1657785104297
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# setup a subplot figure\n",
    "fig, axs = plt.subplots(len(models), 2)\n",
    "fig.set_size_inches(15, 7 * len(models))\n",
    "\n",
    "i = 0\n",
    "\n",
    "for model in models:\n",
    "    if model == \"catboost\":\n",
    "        model_X_test = X_test_catboost\n",
    "        model_y_test = y_test_catboost\n",
    "    else:\n",
    "        model_X_test = X_test\n",
    "        model_y_test = y_test\n",
    "\n",
    "    # inference - ensure smallest LoS is 0 days (not negative value)\n",
    "    preds = np.clip(models[model][\"model\"].predict(model_X_test), 0, None)\n",
    "\n",
    "    # calculate RMSE and range of LoS\n",
    "    rmse = mean_squared_error(model_y_test, preds, squared=False)\n",
    "    mae = mean_absolute_error(model_y_test, preds)\n",
    "\n",
    "    print(\n",
    "        f\"{model} test rmse: {rmse.round(3)} days, mae: {mae.round(3)}, range ({preds.min().round(1)} - {preds.max().round(1)} days)\"\n",
    "    )\n",
    "\n",
    "    # create prediction dataframe\n",
    "    predictions_df = pd.DataFrame(data=model_y_test.reset_index(drop=True))\n",
    "    predictions_df[\"pred\"] = preds\n",
    "\n",
    "    # calculate relative error\n",
    "    predictions_df[\"error\"] = predictions_df.pred - predictions_df.LENGTH_OF_STAY\n",
    "\n",
    "    # plot predicted vs actual\n",
    "    axs[i, 0].scatter(predictions_df.pred, predictions_df.LENGTH_OF_STAY, alpha=0.1)\n",
    "    # plot ideal 1:1 prediction line. Max LoS = 30\n",
    "    axs[i, 0].plot(np.arange(0, 31), np.arange(0, 31), \"r--\")\n",
    "    axs[i, 0].set_xlabel(\"Predicted Length of Stay (days)\")\n",
    "    axs[i, 0].set_ylabel(\"Actual Length of Stay (days)\")\n",
    "    axs[i, 0].set_xlim([-1, 31])\n",
    "    axs[i, 0].set_ylim([-1, 31])\n",
    "    axs[i, 0].set_title(f\"{model} - RMSE {rmse.round(2)} days\")\n",
    "\n",
    "    # plot relative error\n",
    "    axs[i, 1].scatter(predictions_df.LENGTH_OF_STAY, predictions_df.error, alpha=0.1)\n",
    "    axs[i, 1].set_xlabel(\"Length of Stay (days)\")\n",
    "    axs[i, 1].set_ylabel(\"Error (days)\")\n",
    "    # plot mean relative error and 95% confidence intervals\n",
    "    axs[i, 1].plot(np.arange(0, 31), np.ones(31) * predictions_df.error.mean(), \"r\")\n",
    "    axs[i, 1].plot(\n",
    "        np.arange(0, 51),\n",
    "        np.ones(51) * (predictions_df.error.mean() + (2 * predictions_df.error.std())),\n",
    "        \"g--\",\n",
    "    )\n",
    "    axs[i, 1].plot(\n",
    "        np.arange(0, 51),\n",
    "        np.ones(51) * (predictions_df.error.mean() - (2 * predictions_df.error.std())),\n",
    "        \"g--\",\n",
    "    )\n",
    "    # scale plot\n",
    "    axs[i, 1].set_xlim([-1, 31])\n",
    "    # add statistical data in legend. LoA = limits of agreement\n",
    "    # flag: errors are not normally distibuted so does 2*std capture 95% interval?\n",
    "\n",
    "    axs[i, 1].legend(\n",
    "        [\n",
    "            f\"\\u03bc ({np.round(predictions_df.error.mean(),2)} days)\",\n",
    "            f\"95% LoA (\\u03C3 {np.round(predictions_df.error.std(),2)} days gives {2*np.round(predictions_df.error.std(),2)})\",\n",
    "        ]\n",
    "    )\n",
    "    axs[i, 1].set_title(f\"{model} - RMSE {rmse.round(2)} days\")\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Model tuning\n",
    "\n",
    "We will select the best performing model using default parameters, `catboost` and use GridSearchCV to fine tune its hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1657729387396
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# note the baseline performance of the chosen model\n",
    "model_name = \"catboost\"\n",
    "\n",
    "print(models[model_name][\"test_metric\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Re-train best model\n",
    "\n",
    "Using GridsearchCV and an appropriate parameter array for the chosen model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1657740369530
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"catboost\"\n",
    "\n",
    "final_model = {model_name: {}}\n",
    "\n",
    "# example from https://catboost.ai/en/docs/concepts/python-reference_catboostregressor_grid_search\n",
    "# see https://catboost.ai/en/docs/concepts/parameter-tuning for other options\n",
    "\n",
    "param_grid = {\n",
    "    \"learning_rate\": [0.03, 0.1],\n",
    "    \"depth\": [4, 6, 10],\n",
    "    \"l2_leaf_reg\": [1, 3, 5, 7, 9],\n",
    "}\n",
    "\n",
    "# extract categorical features\n",
    "num_features = [\n",
    "    \"AGE_ON_ADMISSION\",\n",
    "    \"EL CountLast12m\",\n",
    "    \"EMCountLast12m\",\n",
    "    \"OP First CountLast12m\",\n",
    "    \"OP FU CountLast12m\",\n",
    "]\n",
    "cat_features = list(set(X_train_catboost.columns) - set(num_features))\n",
    "\n",
    "gsc = GridSearchCV(\n",
    "    estimator=CatBoostRegressor(verbose=False, cat_features=cat_features),\n",
    "    param_grid=param_grid,\n",
    "    cv=5,\n",
    "    scoring=\"neg_root_mean_squared_error\",\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    refit=True,\n",
    ")\n",
    "\n",
    "# takes ~65 mins to run on a STANDARD_D13_V2\n",
    "grid_result = gsc.fit(X_train_catboost, y_train_catboost)\n",
    "\n",
    "# store performance; neg_root_mean_squared_error = -root_mean_squared_error\n",
    "final_model[model_name][\"train_rmse\"] = -grid_result.best_score_\n",
    "# store model and parameters\n",
    "final_model[model_name][\"model\"] = grid_result.best_estimator_\n",
    "final_model[model_name][\"params\"] = grid_result.best_params_\n",
    "final_model[model_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Model evaluation\n",
    "\n",
    "Now we have tuned the best model given the parameters specified, we will test the model on the test set and also calculate the `mean_absolute_error` which is an additional metric that is more understandable - it is the number of days error in the length of stay prediction across all predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1657740370334
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# generate predictions\n",
    "preds_test = final_model[model_name][\"model\"].predict(X_test_catboost)\n",
    "\n",
    "# append the test metrics to the model\n",
    "final_model[model_name][\"test_rmse\"] = mean_squared_error(\n",
    "    y_test_catboost, preds_test, squared=False\n",
    ")\n",
    "final_model[model_name][\"test_mae\"] = mean_absolute_error(y_test_catboost, preds_test)\n",
    "final_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "How much does hyperparameter tuning on the training set improve the performance on the test set?\n",
    "\n",
    "In our case, the performance actually decreases by an RMSE of 0.1%, indicating that hyperparameter tuning has negligible impact on the performance of the model.\n",
    "\n",
    "Another interpretation is that the model, with an MAE of 4.2 days, fails to adequately capture the nature of length of stay, so hyperparameter tuning is moot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1657784729985
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "models[\"final_model\"] = final_model\n",
    "\n",
    "# save models outside the git tree\n",
    "with open(\"../../models/regression.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(models, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model exploration\n",
    "\n",
    "A single performance metric can be a misleading summary of how a model performs. We will take the \"best performing\" baseline model, and explore in more detail how the model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1657784731045
    }
   },
   "outputs": [],
   "source": [
    "if model_name == \"catboost\":\n",
    "    model_X_test = X_test_catboost\n",
    "    model_y_test = y_test_catboost\n",
    "else:\n",
    "    model_X_test = X_test\n",
    "    model_y_test = y_test\n",
    "\n",
    "# setup a subplot figure\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "fig.set_size_inches(15, 7)\n",
    "\n",
    "# inference - ensure smallest LoS is 0 days (not negative value)\n",
    "preds = np.clip(\n",
    "    models[\"final_model\"][model_name][\"model\"].predict(model_X_test), 0, None\n",
    ")\n",
    "\n",
    "# calculate RMSE and range of LoS\n",
    "rmse = mean_squared_error(model_y_test, preds, squared=False)\n",
    "print(\n",
    "    f\"{model_name} test rmse: {rmse.round(3)} days, range ({preds.min().round(1)} - {preds.max().round(1)} days)\"\n",
    ")\n",
    "\n",
    "# create prediction dataframe\n",
    "predictions_df = pd.DataFrame(data=model_y_test.reset_index(drop=True))\n",
    "predictions_df[\"pred\"] = preds\n",
    "\n",
    "# calculate relative error\n",
    "predictions_df[\"error\"] = predictions_df.pred - predictions_df.LENGTH_OF_STAY\n",
    "\n",
    "# plot predicted vs actual\n",
    "axs[0].scatter(predictions_df.pred, predictions_df.LENGTH_OF_STAY, alpha=0.1)\n",
    "# plot ideal 1:1 prediction line. Max LoS = 30\n",
    "axs[0].plot(np.arange(0, 31), np.arange(0, 31), \"r--\")\n",
    "axs[0].set_xlabel(\"Predicted Length of Stay (days)\")\n",
    "axs[0].set_ylabel(\"Actual Length of Stay (days)\")\n",
    "axs[0].set_xlim([-1, 31])\n",
    "axs[0].set_ylim([-1, 31])\n",
    "axs[0].set_title(f\"{model_name} - RMSE {rmse.round(2)} days\")\n",
    "\n",
    "# plot relative error\n",
    "axs[1].scatter(predictions_df.LENGTH_OF_STAY, predictions_df.error, alpha=0.1)\n",
    "axs[1].set_xlabel(\"Length of Stay (days)\")\n",
    "axs[1].set_ylabel(\"Error (days)\")\n",
    "# plot mean relative error and 95% confidence intervals\n",
    "axs[1].plot(np.arange(0, 31), np.ones(31) * predictions_df.error.mean(), \"r\")\n",
    "axs[1].plot(\n",
    "    np.arange(0, 51),\n",
    "    np.ones(51) * (predictions_df.error.mean() + (2 * predictions_df.error.std())),\n",
    "    \"g--\",\n",
    ")\n",
    "axs[1].plot(\n",
    "    np.arange(0, 51),\n",
    "    np.ones(51) * (predictions_df.error.mean() - (2 * predictions_df.error.std())),\n",
    "    \"g--\",\n",
    ")\n",
    "# scale plot\n",
    "axs[1].set_xlim([-1, 31])\n",
    "# add statistical data in legend. LoA = limits of agreement\n",
    "# flag: errors are not normally distibuted so does 2*std capture 95% interval?\n",
    "\n",
    "axs[1].legend(\n",
    "    [\n",
    "        f\"\\u03bc ({np.round(predictions_df.error.mean(),2)} days)\",\n",
    "        f\"95% LoA (\\u03C3 {np.round(predictions_df.error.std(),2)} days gives {2*np.round(predictions_df.error.std(),2)})\",\n",
    "    ]\n",
    ")\n",
    "axs[1].set_title(f\"{model_name} - RMSE {rmse.round(2)} days\")\n",
    "\n",
    "fig.suptitle(\"Final model\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Feature importance\n",
    "\n",
    "Which features does the model ascribe predictive power to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1657740372079
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Feature names\n",
    "coef = pd.DataFrame(data=list(model_X_test.columns))\n",
    "# Feature importances, sorted\n",
    "coef[\"coef\"] = models[\"final_model\"][model_name][\"model\"].feature_importances_\n",
    "coef.sort_values(\"coef\", ascending=False, inplace=True)\n",
    "coef.set_index(0, inplace=True)\n",
    "# Plot interactive plot\n",
    "# Hover over a feature for full feature name\n",
    "fig = px.bar(coef, x=coef.index, y=\"coef\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Extensions\n",
    "\n",
    "- Build two separate regression models - one for long stayers (21+ days), one for not long-stayers.\n",
    "- Include IS_MINOR data in conjunction with above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6d65a8c07f5b6469e0fc613f182488c0dccce05038bbda39e5ac9075c0454d11"
  },
  "kernel_info": {
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
