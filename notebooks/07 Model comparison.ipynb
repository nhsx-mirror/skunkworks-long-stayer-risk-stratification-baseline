{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Model comparison\n",
    "\n",
    "We have now trained our \"best\" baseline Regression (length of stay) and Classification (risk 1-5) models.\n",
    "\n",
    "We can create an equivalent risk model from the risk categories using the predicted Length of Stay:\n",
    "\n",
    "Risk Category|Day Range for Risk Category\n",
    "-----|------\n",
    "1 - Very low risk|0-6\n",
    "2 - Low risk|7-10\n",
    "3 - Normal risk|11-13\n",
    "4 - Elevated risk|14-15\n",
    "5 - High risk|>15\n",
    "\n",
    "We can now compare the best regression model (**catboost**), cast as a classification, against the best classification model (**catboost**).\n",
    "\n",
    "Inputs|Outputs\n",
    "---|---\n",
    "`processed/features-catboost.parquet`|&nbsp;\n",
    "`models/regression.pickle`|&nbsp;\n",
    "`models/classification.pickle`|&nbsp;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1659600110882
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import f1_score, mean_absolute_error, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sys.path.append(\"../src/\")\n",
    "\n",
    "from utils import risk_score, train_test_validate_split\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = [15, 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1659600462779
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "features_catboost_df = pd.read_parquet(\"../../data/processed/features-catboost.parquet\")\n",
    "\n",
    "# add actual risk scores\n",
    "risk_labels = [\n",
    "    \"1 - Very Low Risk\",\n",
    "    \"2 - Low Risk\",\n",
    "    \"3 - Normal Risk\",\n",
    "    \"4 - Elevated Risk\",\n",
    "    \"5 - High Risk\",\n",
    "]\n",
    "features_catboost_df[\"risk\"] = [\n",
    "    risk_score(los) for los in features_catboost_df.LENGTH_OF_STAY\n",
    "]\n",
    "\n",
    "# non-one-hot encoded data for catboost\n",
    "X_catboost = features_catboost_df.drop(columns=[\"LENGTH_OF_STAY\"])\n",
    "y_catboost_reg = features_catboost_df[\"LENGTH_OF_STAY\"]\n",
    "y_catboost_clf = features_catboost_df[\"risk\"]\n",
    "\n",
    "# Split data for train/validate+test - regression catboost\n",
    "(\n",
    "    X_train_catboost_reg,\n",
    "    X_validate_catboost_reg,\n",
    "    X_test_catboost_reg,\n",
    "    y_train_catboost_reg,\n",
    "    y_validate_catboost_reg,\n",
    "    y_test_catboost_reg,\n",
    ") = train_test_validate_split(\n",
    "    X_catboost,\n",
    "    y_catboost_reg,\n",
    "    train_size=0.70,\n",
    "    validate_size=0.15,\n",
    "    test_size=0.15,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "# Split data for train/validate+test - classification catboost\n",
    "\n",
    "(\n",
    "    X_train_catboost_clf,\n",
    "    X_validate_catboost_clf,\n",
    "    X_test_catboost_clf,\n",
    "    y_train_catboost_clf,\n",
    "    y_validate_catboost_clf,\n",
    "    y_test_catboost_clf,\n",
    ") = train_test_validate_split(\n",
    "    X_catboost,\n",
    "    y_catboost_clf,\n",
    "    train_size=0.70,\n",
    "    validate_size=0.15,\n",
    "    test_size=0.15,\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1659600487146
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# load models from outside the git tree\n",
    "with open(\"../../models/regression.pickle\", \"rb\") as handle:\n",
    "    models_regression = pickle.load(handle)\n",
    "models_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1659600506902
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# load models from outside the git tree\n",
    "with open(\"../../models/classification.pickle\", \"rb\") as handle:\n",
    "    models_classification = pickle.load(handle)\n",
    "models_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Compare predicted risk score (classification) with equivalent-predicted risk score (regression)\n",
    "\n",
    "Classification -> Risk score\n",
    "\n",
    "Regression -> Length Of Stay -> Equivalent risk score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1659600861811
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "best_model = \"catboost\"\n",
    "\n",
    "# setup a subplot figure\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "fig.set_size_inches(15, 7)\n",
    "\n",
    "# perform inference\n",
    "preds_regression = np.clip(\n",
    "    models_regression[\"final_model\"][best_model][\"model\"].predict(X_test_catboost_reg),\n",
    "    0,\n",
    "    None,\n",
    ")\n",
    "preds_classification = models_classification[\"final_model\"][best_model][\n",
    "    \"model\"\n",
    "].predict(X_test_catboost_clf)\n",
    "\n",
    "# calculate performance metrics\n",
    "rmse = mean_squared_error(y_test_catboost_reg, preds_regression, squared=False)\n",
    "mae = mean_absolute_error(y_test_catboost_reg, preds_regression)\n",
    "\n",
    "f1_score_weighted = f1_score(\n",
    "    y_test_catboost_clf, preds_classification, average=\"weighted\"\n",
    ")\n",
    "\n",
    "# create a prediction dataframe\n",
    "predictions_df = pd.DataFrame(data=y_test_catboost_clf.reset_index(drop=True))\n",
    "predictions_df[\"pred_regression_los\"] = preds_regression\n",
    "# calculate equivalent risk score from regression model\n",
    "predictions_df[\"pred_regression\"] = [\n",
    "    risk_score(los) for los in predictions_df.pred_regression_los\n",
    "]\n",
    "predictions_df[\"pred_classification\"] = preds_classification\n",
    "\n",
    "#### Predicted vs Actual ####\n",
    "\n",
    "# plot predicted vs actual CLASSES for classification\n",
    "risks = dict.fromkeys(risk_labels)\n",
    "for proportion in risks:\n",
    "    risks[proportion] = np.array([0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "\n",
    "    for label in risk_labels:\n",
    "        this_risk = int(label[0])\n",
    "\n",
    "        # extract the predicted risk\n",
    "        subset = predictions_df[predictions_df.pred_classification == this_risk]\n",
    "\n",
    "        if proportion == \"1 - Very Low Risk\":\n",
    "            count = (subset.risk == 1).sum()\n",
    "        elif proportion == \"2 - Low Risk\":\n",
    "            count = (subset.risk == 2).sum()\n",
    "        elif proportion == \"3 - Normal Risk\":\n",
    "            count = (subset.risk == 3).sum()\n",
    "        elif proportion == \"4 - Elevated Risk\":\n",
    "            count = (subset.risk == 4).sum()\n",
    "        else:\n",
    "            count = (subset.risk == 5).sum()\n",
    "\n",
    "        prop = 0 if count == 0 else count / subset.shape[0]\n",
    "\n",
    "        risks[proportion][this_risk - 1] = prop\n",
    "\n",
    "bottom = np.array([0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "for proportion in risks:\n",
    "    if proportion == \"1 - Very Low Risk\":\n",
    "        data = risks[proportion]\n",
    "        axs[0].bar(risk_labels, data, label=proportion, width=0.35)\n",
    "    else:\n",
    "        bottom += data\n",
    "        data = risks[proportion]\n",
    "        axs[0].bar(risk_labels, data, label=proportion, bottom=bottom, width=0.35)\n",
    "\n",
    "axs[0].set_xlabel(\"Predicted risk\")\n",
    "axs[0].set_ylabel(\"Actual risk proportion\")\n",
    "axs[0].set_title(\n",
    "    f\"classification: {best_model} - f1 weighted: {f1_score_weighted.round(2)}\"\n",
    ")\n",
    "\n",
    "# plot actual vs predicted CLASSES for regression\n",
    "risks = dict.fromkeys(risk_labels)\n",
    "for proportion in risks:\n",
    "    risks[proportion] = np.array([0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "\n",
    "    for label in risk_labels:\n",
    "        this_risk = int(label[0])\n",
    "\n",
    "        # extract the predicted risk\n",
    "        subset = predictions_df[predictions_df.pred_regression == this_risk]\n",
    "\n",
    "        if proportion == \"1 - Very Low Risk\":\n",
    "            count = (subset.risk == 1).sum()\n",
    "        elif proportion == \"2 - Low Risk\":\n",
    "            count = (subset.risk == 2).sum()\n",
    "        elif proportion == \"3 - Normal Risk\":\n",
    "            count = (subset.risk == 3).sum()\n",
    "        elif proportion == \"4 - Elevated Risk\":\n",
    "            count = (subset.risk == 4).sum()\n",
    "        else:\n",
    "            count = (subset.risk == 5).sum()\n",
    "\n",
    "        prop = 0 if count == 0 else count / subset.shape[0]\n",
    "\n",
    "        risks[proportion][this_risk - 1] = prop\n",
    "\n",
    "bottom = np.array([0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "for proportion in risks:\n",
    "    if proportion == \"1 - Very Low Risk\":\n",
    "        data = risks[proportion]\n",
    "        axs[1].bar(risk_labels, data, label=proportion, width=0.35)\n",
    "    else:\n",
    "        bottom += data\n",
    "        data = risks[proportion]\n",
    "        axs[1].bar(risk_labels, data, label=proportion, bottom=bottom, width=0.35)\n",
    "handles, labels = axs[1].get_legend_handles_labels()\n",
    "axs[1].legend(handles[::-1], labels[::-1], bbox_to_anchor=(1.05, 1))\n",
    "axs[1].set_xlabel(\"Predicted risk\")\n",
    "axs[1].set_ylabel(\"Actual risk proportion\")\n",
    "axs[1].set_title(\n",
    "    f\"regression: {best_model} - RMSE {rmse.round(2)} days, MAE {mae.round(2)} days\"\n",
    ")\n",
    "\n",
    "fig.suptitle(\"Predicted vs Actual risk\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "For our best trained model, the regression approach better captures the overall risk of becoming a long stayer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Extensions\n",
    "\n",
    "* Add number of predictions to bins in plots using e.g. https://stackoverflow.com/questions/30228069/how-to-display-the-value-of-the-bar-on-each-bar-with-pyplot-barh\n",
    "* Refactor visualisation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
