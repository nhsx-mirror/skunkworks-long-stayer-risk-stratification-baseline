{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling - Classification\n",
    "\n",
    "This notebook explores classification models to **predict risk of becoming a long stayer** as a baseline to the [Long Stayer Risk Stratification](https://github.com/nhsx/skunkworks-long-stayer-risk-stratification) model.\n",
    "\n",
    "This notebook is broken down into:\n",
    "\n",
    "1. Converting the length of stay into a relative risk\n",
    "1. Training a range of baseline models using cross validation\n",
    "3. Testing final models on a test dataset\n",
    "4. Exploring in more detail the best performing baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1652704470089
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    balanced_accuracy_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = [15, 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1652705474591
    }
   },
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "\n",
    "def train_model(gsc, X_train, y_train):\n",
    "    \"\"\"Uses a GridSearchCV instance to find a reasonable model, and store\n",
    "    performance and fitted model into a python dict\n",
    "\n",
    "    Parameters:\n",
    "\n",
    "        gsc (sklearn.model_selection.GridSearchCV object): defined model\n",
    "        X_train (pandas dataframe): training dataframe with features\n",
    "        y_train (pandas dataframe): training dataframe with targets\n",
    "\n",
    "    Returns:\n",
    "\n",
    "        (dict): resulting fitted model and performance metrics\n",
    "    \"\"\"\n",
    "\n",
    "    grid_result = gsc.fit(X_train, y_train)\n",
    "\n",
    "    # note model fitted/scored on balanced accuracy\n",
    "    model = {\n",
    "        \"cv_balanced_accuracy_mean\": np.round(\n",
    "            grid_result.cv_results_[\"mean_test_score\"][grid_result.best_index_], 3\n",
    "        ),\n",
    "        \"cv_balanced_accuracy_std\": np.round(\n",
    "            grid_result.cv_results_[\"std_test_score\"][grid_result.best_index_], 2\n",
    "        ),\n",
    "        \"model\": grid_result.best_estimator_,\n",
    "    }\n",
    "\n",
    "    # retrain the best estimator on the full training set - note that refit=True does not appear to do this\n",
    "    # note we calculate balanced accuracy as final metric\n",
    "    model[\"model\"].fit(X_train, y_train)\n",
    "    model[\"balanced_accuracy\"] = np.round(\n",
    "        balanced_accuracy_score(y_train, model[\"model\"].predict(X_train)), 3\n",
    "    )\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def risk_score(los):\n",
    "    \"\"\"Return risk score (1-5) based on LoS\n",
    "\n",
    "    Parameters:\n",
    "        los (float): length of stay in days\n",
    "\n",
    "    Returns:\n",
    "        (int): risk score (1 = Very low risk, 5 = High risk)\n",
    "    \"\"\"\n",
    "\n",
    "    # round los up to whole days\n",
    "    los = math.ceil(los)\n",
    "\n",
    "    if los > 15:\n",
    "        return 5\n",
    "    elif los > 13:\n",
    "        return 4\n",
    "    elif los > 10:\n",
    "        return 3\n",
    "    elif los > 6:\n",
    "        return 2\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1652704470877
    }
   },
   "outputs": [],
   "source": [
    "features_df = pd.read_parquet(\"../../data/features.parquet\")\n",
    "features_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate risk scores\n",
    "\n",
    "We will convert actual Length of Stay (days) into a risk score defined as:\n",
    "\n",
    "Risk Category|Day Range for Risk Category\n",
    "-----|------\n",
    "1 - Very low risk|0-6\n",
    "2 - Low risk|7-10\n",
    "3 - Normal risk|11-13\n",
    "4 - Elevated risk|14-15\n",
    "5 - High risk|>15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1652704471349
    }
   },
   "outputs": [],
   "source": [
    "# actual risk scores\n",
    "risk_labels = [\n",
    "    \"1 - Very Low Risk\",\n",
    "    \"2 - Low Risk\",\n",
    "    \"3 - Normal Risk\",\n",
    "    \"4 - Elevated Risk\",\n",
    "    \"5 - High Risk\",\n",
    "]\n",
    "features_df[\"risk\"] = [risk_score(los) for los in features_df.LENGTH_OF_STAY]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define target and training features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1652704471728
    }
   },
   "outputs": [],
   "source": [
    "X = features_df.drop(columns=[\"LENGTH_OF_STAY\"])\n",
    "y = features_df.risk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/test split\n",
    "\n",
    "For model evaluation, we will hold back a 25% test set, and use cross-validation on the remaining 75% for all models until the final comparison is made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1652704594667
    }
   },
   "outputs": [],
   "source": [
    "# Split data for train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, train_size=0.75, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore class imbalance\n",
    "\n",
    "This is a multi-class classification model, so we need to understand where any class imbalance lies otherwise we will skew to larger classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show how many of each class are present in the training set:\n",
    "X_train.risk.value_counts().sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weight samples\n",
    "\n",
    "Given significant class imbalance, we will weight samples where the smallest class has a weight of 1, and other classes as a (<1) proportion.\n",
    "\n",
    "e.g.\n",
    "\n",
    "class|count|weight\n",
    "---|---|---\n",
    "1|1000|0.1\n",
    "2|100|1\n",
    "3|500|0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = X_train.risk.value_counts().min() / X_train.risk.value_counts().sort_index()\n",
    "\n",
    "## Drop risk from training/test set\n",
    "X_train.drop(columns=\"risk\", inplace=True)\n",
    "X_test.drop(columns=\"risk\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "Strategy is to try a number of classification models with:\n",
    "\n",
    "* GridsearchCV for hyperparameter tuning with cross validation, refitting full training set for best model\n",
    "* Test all final models against the held-out test set.\n",
    "* Explore feature importance of best performing model\n",
    "* Explore fairness (next notebook) of best performing model\n",
    "\n",
    "Logistic regression models are excluded due to significant multi-collinearity (see analysis in Regression notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1652705481582
    }
   },
   "outputs": [],
   "source": [
    "# Initiate empty models dictionary\n",
    "models = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prior model\n",
    "\n",
    "The simplest baseline model takes the most frequent class label as its prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1652705486989
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"prior\"\n",
    "\n",
    "# define gridsearch parameters\n",
    "gsc = GridSearchCV(\n",
    "    estimator=DummyClassifier(strategy=\"prior\"),\n",
    "    param_grid={},\n",
    "    cv=5,\n",
    "    scoring=\"balanced_accuracy\",\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    refit=True,\n",
    ")\n",
    "\n",
    "# takes ~1 second to run on a STANDARD_DS3_V2\n",
    "models[model_name] = train_model(gsc, X_train, y_train)\n",
    "models[model_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree classifier (weighted)\n",
    "\n",
    "Simplest tree classifier using one tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"decisiontree\"\n",
    "\n",
    "# define gridsearch parameters\n",
    "gsc = GridSearchCV(\n",
    "    estimator=DecisionTreeClassifier(class_weight=weights.to_dict()),\n",
    "    param_grid={\"max_depth\": [5, 10, None]},\n",
    "    cv=5,\n",
    "    scoring=\"accuracy\",\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    refit=True,\n",
    ")\n",
    "\n",
    "# takes ~10s to run on a STANDARD_DS3_V2\n",
    "models[model_name] = train_model(gsc, X_train, y_train)\n",
    "models[model_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest (weighted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1652706477018
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"randomforest\"\n",
    "\n",
    "# define gridsearch parameters\n",
    "gsc = GridSearchCV(\n",
    "    estimator=RandomForestClassifier(class_weight=weights.to_dict()),\n",
    "    param_grid={\"n_estimators\": [10, 100, 500], \"max_depth\": [5, 10, None]},\n",
    "    cv=5,\n",
    "    scoring=\"balanced_accuracy\",\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    refit=True,\n",
    ")\n",
    "\n",
    "# takes ~8 mins to run on a STANDARD_DS3_V2\n",
    "models[model_name] = train_model(gsc, X_train, y_train)\n",
    "models[model_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Catboost\n",
    "\n",
    "Boosted tree optimised for categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1652706888015
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"catboost\"\n",
    "\n",
    "# extract categorical features\n",
    "num_features = [\n",
    "    \"AGE_ON_ADMISSION\",\n",
    "    \"EL CountLast12m\",\n",
    "    \"EMCountLast12m\",\n",
    "    \"OP First CountLast12m\",\n",
    "    \"OP FU CountLast12m\",\n",
    "]\n",
    "cat_features = list(set(X_train.columns) - set(num_features))\n",
    "\n",
    "# define gridsearch parameters\n",
    "gsc = GridSearchCV(\n",
    "    estimator=CatBoostClassifier(\n",
    "        verbose=False, class_weights=weights.to_dict(), cat_features=cat_features\n",
    "    ),\n",
    "    param_grid={\n",
    "        \"max_depth\": [5, 10, None],\n",
    "        \"learning_rate\": [0.01, 0.1, 1],\n",
    "        \"iterations\": [10, 100, 500],\n",
    "    },\n",
    "    cv=5,\n",
    "    scoring=\"balanced_accuracy\",\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    refit=True,\n",
    ")\n",
    "\n",
    "# takes ~25 mins to run on a STANDARD_DS3_V2\n",
    "models[model_name] = train_model(gsc, X_train, y_train)\n",
    "models[model_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1652706970050
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "model_name = \"xgboost\"\n",
    "\n",
    "# define gridsearch parameters\n",
    "gsc = GridSearchCV(\n",
    "    estimator=XGBClassifier(weight=weights.to_dict(), random_state=42),\n",
    "    param_grid={\n",
    "        \"n_estimators\": [1, 5],\n",
    "        \"learning_rate\": [0.01, 0.1, 1],\n",
    "        \"max_depth\": [5, 10, None],\n",
    "    },\n",
    "    cv=5,\n",
    "    scoring=\"balanced_accuracy\",\n",
    "    verbose=1,\n",
    "    n_jobs=-1,\n",
    "    refit=True,\n",
    ")\n",
    "\n",
    "# takes ~1 mins to run on a STANDARD_DS3_V2\n",
    "models[model_name] = train_model(gsc, X_train, y_train)\n",
    "models[model_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1652706991554
    }
   },
   "outputs": [],
   "source": [
    "# save models outside the git tree\n",
    "with open(\"../../models/classification.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(models, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1652706998600
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# load models from outside the git tree\n",
    "with open(\"../../models/classification.pickle\", \"rb\") as handle:\n",
    "    models = pickle.load(handle)\n",
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate models\n",
    "\n",
    "Use the held-out test set to evaluate the performance of all the tuned models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1652707415391
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    preds = models[model][\"model\"].predict(X_test)\n",
    "    probs = models[model][\"model\"].predict_proba(X_test)\n",
    "    # calculate performance\n",
    "    balanced_accuracy = balanced_accuracy_score(y_test, preds)\n",
    "    f1_score_weighted = f1_score(y_test, preds, average=\"weighted\")\n",
    "    auc = roc_auc_score(\n",
    "        y_test, probs, multi_class=\"ovr\", average=\"weighted\"\n",
    "    )  # one-vs-rest\n",
    "    print(\n",
    "        f\"{model} test balanced accuracy: {balanced_accuracy.round(3)}, f1 score (weighted): {f1_score_weighted.round(3)}, auc (ovr, weighted): {auc.round(3)}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model exploration\n",
    "\n",
    "A single performance metric can be a misleading summary of how a model performs. We will take the \"best performing\" baseline model, and explore in more detail how the model performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1652707586368
    }
   },
   "outputs": [],
   "source": [
    "model = \"xgboost\"\n",
    "# generate predictions\n",
    "predictions_df = pd.DataFrame(data=y_test.reset_index(drop=True))\n",
    "predictions_df[\"pred\"] = models[model][\"model\"].predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual vs predicted plot\n",
    "\n",
    "Let's visualise model performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1652707586866
    }
   },
   "outputs": [],
   "source": [
    "# plot actual vs predicted\n",
    "predictions_df.risk.hist(alpha=0.5)\n",
    "predictions_df.pred.hist(alpha=0.5)\n",
    "\n",
    "plt.legend([\"Actual risk\", \"Predicted risk\"])\n",
    "plt.xticks([1, 2, 3, 4, 5], risk_labels);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot accuracy by risk category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "risks = dict.fromkeys(risk_labels)\n",
    "for proportion in risks:\n",
    "    risks[proportion] = np.array([0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "\n",
    "    for label in risk_labels:\n",
    "        this_risk = int(label[0])\n",
    "\n",
    "        # extract the real risk\n",
    "        subset = predictions_df[predictions_df.risk == this_risk]\n",
    "\n",
    "        if proportion == \"1 - Very Low Risk\":\n",
    "            prop = (subset.pred == 1).sum() / subset.shape[0]\n",
    "        elif proportion == \"2 - Low Risk\":\n",
    "            prop = (subset.pred == 2).sum() / subset.shape[0]\n",
    "        elif proportion == \"3 - Normal Risk\":\n",
    "            prop = (subset.pred == 3).sum() / subset.shape[0]\n",
    "        elif proportion == \"4 - Elevated Risk\":\n",
    "            prop = (subset.pred == 4).sum() / subset.shape[0]\n",
    "        else:\n",
    "            prop = (subset.pred == 5).sum() / subset.shape[0]\n",
    "\n",
    "        risks[proportion][this_risk - 1] = prop\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1, 1, 1)\n",
    "bottom = np.array([0.0, 0.0, 0.0, 0.0, 0.0])\n",
    "for proportion in risks:\n",
    "    if proportion == \"1 - Very Low Risk\":\n",
    "        data = risks[proportion]\n",
    "        ax.bar(risk_labels, data, label=proportion, width=0.35)\n",
    "    else:\n",
    "        bottom += data\n",
    "        data = risks[proportion]\n",
    "        ax.bar(risk_labels, data, label=proportion, bottom=bottom, width=0.35)\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles[::-1], labels[::-1], bbox_to_anchor=(1.05, 1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Severity of misclassification\n",
    "\n",
    "When the model incorrectly predicts a class, how badly does it do this?\n",
    "\n",
    "Because risk categories are numerical (1-5), we can calculate the difference between them as the number of classes incorrect the prediction was."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df[\"diff\"] = predictions_df.diff().risk\n",
    "fig = px.histogram(predictions_df, x=\"diff\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Feature importance\n",
    "\n",
    "Which features does the model ascribe predictive power to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Feature names\n",
    "coef = pd.DataFrame(data=list(X_train.columns))\n",
    "# Feature importances, sorted\n",
    "coef[\"coef\"] = models[model][\"model\"].feature_importances_\n",
    "coef.sort_values(\"coef\", ascending=False, inplace=True)\n",
    "coef.set_index(0, inplace=True)\n",
    "# Plot interactive plot\n",
    "# Hover over a feature for full feature name\n",
    "fig = px.bar(coef, x=coef.index, y=\"coef\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "- Plot PR curves per class as per https://stackoverflow.com/questions/56090541/how-to-plot-precision-and-recall-of-multiclass-classifier\n",
    "- Explore poor predictive power"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6d65a8c07f5b6469e0fc613f182488c0dccce05038bbda39e5ac9075c0454d11"
  },
  "kernel_info": {
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
