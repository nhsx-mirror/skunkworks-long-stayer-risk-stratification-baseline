{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "# Fairness analysis - regression model\n",
    "\n",
    "Fairness can mean many things.\n",
    "\n",
    "In this analysis, we focus on the accuracy of the predictions in the machine learning model.\n",
    "\n",
    "> Does the machine learning model perform better or worse for any of the following demographics: \"ETHNIC_CATEGORY_CODE_DESCRIPTION\", \"IMD county decile\", \"OAC Group Name\", \"OAC Subgroup Name\", \"OAC Supergroup Name\", \"PATIENT_GENDER_CURRENT_DESCRIPTION\", \"POST_CODE_AT_ADMISSION_DATE_DISTRICT\", \"Rural urban classification\"?\n",
    "\n",
    "This demographic data was **excluded** when training the models, and in this notebook, is reintroduced so we can plot the underlying distribution of count, Length of Stay and the error in the predictions provided by the model.\n",
    "\n",
    "It's important to note that many demographic subgroups will have very small representation (count), so it's important to understand the distribution of your data before jumping to conclusions if model performance is poor for a group of ~100 invidividuals over 200,000 in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1653397752594
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams[\"figure.figsize\"] = [15, 8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1653397753554
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# sensitive columns before one-hot-encoding\n",
    "sensitive_columns = [\n",
    "    \"ETHNIC_CATEGORY_CODE_DESCRIPTION\",\n",
    "    \"IMD county decile\",\n",
    "    \"OAC Group Name\",\n",
    "    \"OAC Subgroup Name\",\n",
    "    \"OAC Supergroup Name\",\n",
    "    \"PATIENT_GENDER_CURRENT_DESCRIPTION\",\n",
    "    \"POST_CODE_AT_ADMISSION_DATE_DISTRICT\",\n",
    "    \"Rural urban classification\",\n",
    "]\n",
    "\n",
    "# one-hot encoded dataframe\n",
    "features_sensitive_df = pd.read_parquet(\n",
    "    \"../../data/processed/features-sensitive.parquet\"\n",
    ")\n",
    "\n",
    "# note catboost requires not one-hot encoding features, as it deals with them during training\n",
    "features_sensitive_catboost_df = pd.read_parquet(\n",
    "    \"../../data/processed/features-sensitive-catboost.parquet\"\n",
    ")\n",
    "\n",
    "# Separate training/test features\n",
    "X = features_sensitive_df.drop(columns=\"LENGTH_OF_STAY\")\n",
    "y = features_sensitive_df.LENGTH_OF_STAY\n",
    "X_catboost = features_sensitive_catboost_df.drop(columns=\"LENGTH_OF_STAY\")\n",
    "y_catboost = features_sensitive_catboost_df.LENGTH_OF_STAY\n",
    "\n",
    "# Split data for train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, train_size=0.75, random_state=42\n",
    ")\n",
    "print(X_train.shape, X_test.shape)\n",
    "\n",
    "X_train_catboost, X_test_catboost, y_train_catboost, y_test_catboost = train_test_split(\n",
    "    X_catboost, y_catboost, train_size=0.75, random_state=42\n",
    ")\n",
    "print(X_train_catboost.shape, X_test_catboost.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Explore underlying distribution of sensitive features\n",
    "\n",
    "Before exploring the models, we need an understanding of actual variation in the data for each subcategory.\n",
    "\n",
    "First we will plot the count of each demographic across the sensitive features, so we can identify and low `n` that may skew further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1653397125701
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "for feature in sensitive_columns:\n",
    "    data = pd.DataFrame(columns=[\"subfeature\", \"count\"])\n",
    "    # iterate over all subcategories for the feature\n",
    "    for subfeature in features_sensitive_df[feature].unique():\n",
    "        subset = features_sensitive_df[features_sensitive_df[feature] == subfeature]\n",
    "        count = subset.LENGTH_OF_STAY.count()\n",
    "        df = pd.DataFrame(data=[{\"subfeature\": subfeature, \"count\": count}])\n",
    "        data = pd.concat([data, df])\n",
    "\n",
    "    fig = px.bar(data, x=\"subfeature\", y=\"count\", title=feature, orientation=\"v\")\n",
    "    # note: horizontal lines don't render correctly in Azure ML Notebooks, but do in VS Code\n",
    "    fig.add_hline(y=data[\"count\"].mean(), line_dash=\"dash\", line_color=\"green\")\n",
    "    fig.update_xaxes(title=\"\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Explore mean Length of Stay by subcategory\n",
    "\n",
    "How does the actual Length of Stay vary by subcategory? This is important to understand if the model is capturing the historical reality or introducing additional errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1653395478707
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "global_mean = features_sensitive_df.LENGTH_OF_STAY.mean()\n",
    "for feature in sensitive_columns:\n",
    "    data = pd.DataFrame(columns=[\"subfeature\", \"mean\"])\n",
    "    # iterate through all subcategories\n",
    "    for subfeature in features_sensitive_df[feature].unique():\n",
    "        subset = features_sensitive_df[features_sensitive_df[feature] == subfeature]\n",
    "        mean_los = subset.LENGTH_OF_STAY.mean()\n",
    "        df = pd.DataFrame(data=[{\"subfeature\": subfeature, \"mean_los\": mean_los}])\n",
    "        data = pd.concat([data, df])\n",
    "\n",
    "    fig = px.bar(\n",
    "        data,\n",
    "        x=\"subfeature\",\n",
    "        y=\"mean_los\",\n",
    "        title=feature,\n",
    "        orientation=\"v\",\n",
    "        labels={\"mean_los\": \"Mean Length of Stay (days)\"},\n",
    "    )\n",
    "    fig.add_hline(y=global_mean, line_dash=\"dash\", line_color=\"green\")\n",
    "    fig.update_xaxes(title=\"\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Load models\n",
    "\n",
    "With an understanding of the underlying distribution of demographics and their length of stay, we'll load the trained models to explore how the performance varies by demographic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1653397776556
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# load models from outside the git tree\n",
    "with open(\"../../models/regression.pickle\", \"rb\") as handle:\n",
    "    models = pickle.load(handle)\n",
    "models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Generate model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1653397776674
    },
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# pick one model to analyse:\n",
    "model = \"xgboost\"\n",
    "\n",
    "if model == \"catboost\":\n",
    "    model_X_test = X_test_catboost\n",
    "    model_y_test = y_test_catboost\n",
    "else:\n",
    "    model_X_test = X_test\n",
    "    model_y_test = y_test\n",
    "\n",
    "# Generate predictions\n",
    "preds = np.clip(\n",
    "    models[model][\"model\"].predict(model_X_test.drop(columns=sensitive_columns)),\n",
    "    0,\n",
    "    None,\n",
    ")\n",
    "\n",
    "# Create a combined data frame\n",
    "fairness_df = model_X_test.copy()\n",
    "fairness_df[\"LENGTH_OF_STAY\"] = model_y_test\n",
    "fairness_df[\"LENGTH_OF_STAY_PREDICTED\"] = preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Explore MAE by feature\n",
    "\n",
    "Do some subcategories of a feature have a higher error than others? Ie. is the model less accurate for certain demographics?\n",
    "\n",
    "Rather than plotting MAE per feature, calculate the difference between the mean MAE and each subcategory, to highlight any subcategories with higher MAE than the mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1653395536728
    }
   },
   "outputs": [],
   "source": [
    "for feature in sensitive_columns:\n",
    "    data = pd.DataFrame(columns=[\"subfeature\", \"mae\"])\n",
    "    for subfeature in fairness_df[feature].unique():\n",
    "        subset = fairness_df[fairness_df[feature] == subfeature]\n",
    "        mae = mean_absolute_error(\n",
    "            subset.LENGTH_OF_STAY, subset.LENGTH_OF_STAY_PREDICTED\n",
    "        )\n",
    "        df = pd.DataFrame(data=[{\"subfeature\": subfeature, \"mae\": mae}])\n",
    "        data = pd.concat([data, df])\n",
    "    mean_mae = data[\"mae\"].mean()  # note this is different to overall MAE of model\n",
    "    data[\"mae_diff\"] = data.mae - mean_mae\n",
    "    fig = px.bar(\n",
    "        data,\n",
    "        x=\"subfeature\",\n",
    "        y=\"mae_diff\",\n",
    "        title=feature,\n",
    "        orientation=\"v\",\n",
    "        labels={\"mae_diff\": \"Error compared to mean error (days)\"},\n",
    "    )\n",
    "    fig.add_hline(y=data.mae_diff.mean(), line_dash=\"dash\", line_color=\"green\")\n",
    "    fig.update_xaxes(title=\"\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore MAE ratio to LoS per feature\n",
    "\n",
    "The models developed here showed a higher MAE for higher LoS, so features with higher LoS to start with, will have a higher MAE. This is not an indication of discrimation, but of the model's limited predictive power at higher LoS.\n",
    "\n",
    "To account for this, we can calculate the MAE difference for each subfeature, scaled to the mean LoS for that subfeature, to check for any subfeatures which stand out.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "gather": {
     "logged": 1653397842591
    }
   },
   "outputs": [],
   "source": [
    "for feature in sensitive_columns:\n",
    "    # capture the mae_mean_ratio for each subfeature\n",
    "    data = pd.DataFrame(columns=[\"subfeature\", \"mae_mean_ratio\"])\n",
    "\n",
    "    for subfeature in fairness_df[feature].unique():\n",
    "        subset = fairness_df[fairness_df[feature] == subfeature]\n",
    "        mean_los = subset.LENGTH_OF_STAY.mean()\n",
    "        mae = mean_absolute_error(\n",
    "            subset.LENGTH_OF_STAY, subset.LENGTH_OF_STAY_PREDICTED\n",
    "        )\n",
    "        mae_mean_los_ratio = mae / mean_los if mean_los > 0 else 0\n",
    "        df = pd.DataFrame(\n",
    "            data=[{\"subfeature\": subfeature, \"mae_mean_los_ratio\": mae_mean_los_ratio}]\n",
    "        )\n",
    "        data = pd.concat([data, df])\n",
    "\n",
    "    # calculate the difference of each ratio to the mean, to highlight any anomalies\n",
    "    data[\"mae_mean_los_ratio_diff\"] = (\n",
    "        data[\"mae_mean_los_ratio\"] - data[\"mae_mean_los_ratio\"].mean()\n",
    "    )\n",
    "    fig = px.bar(\n",
    "        data,\n",
    "        x=\"subfeature\",\n",
    "        y=\"mae_mean_los_ratio_diff\",\n",
    "        title=feature,\n",
    "        orientation=\"v\",\n",
    "        labels={\n",
    "            \"mae_mean_los_ratio_diff\": \"Weighted error compared to mean error (a.u.)\"\n",
    "        },\n",
    "    )\n",
    "    fig.add_hline(\n",
    "        y=data.mae_mean_los_ratio_diff.mean(), line_dash=\"dash\", line_color=\"green\"\n",
    "    )\n",
    "    fig.update_xaxes(title=\"\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Extensions\n",
    "\n",
    "* Establish statistical significance tests for differences in performance between demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python38-azureml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "microsoft": {
   "host": {
    "AzureML": {
     "notebookHasBeenCompleted": true
    }
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
